{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfn8IA7XBmbktlQhOHPD5q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tOpZO5-7fnNU","executionInfo":{"status":"ok","timestamp":1691454370909,"user_tz":-540,"elapsed":4713,"user":{"displayName":"서동관","userId":"12646868893277677565"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["def f(x, y):\n","  return x**2 +2*x*y +1\n","\n","eps = 1e-6\n","dx = (f(1 + eps, 2) -f(1, 2))/eps\n","dy = (f(1,2 + eps)-f(1, 2))/eps\n","dx, dy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vO_iZT0Pf3VJ","executionInfo":{"status":"ok","timestamp":1691454370909,"user_tz":-540,"elapsed":14,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"ebf8afd7-8232-415d-bbb5-17bc29a29b41"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6.0000009991512115, 2.000000000279556)"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# 후진 자동 미분\n","x = torch.tensor(1. , requires_grad=True) # 실수만 입력\n","y = torch.tensor(2. , requires_grad=True)\n","\n","v = f(x, y) # feed forward\n","v.backward() # back prob\n","x.grad, y.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-15by3RHh-jg","executionInfo":{"status":"ok","timestamp":1691454370910,"user_tz":-540,"elapsed":8,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"dcd2bcc0-71ea-4b8c-a65d-6bcc8696d1d7"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(6.), tensor(2.))"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x.grad = None #다시 실행시 초기화해야 같은 값이 나옴\n","y.grad = None\n","v = f(x, y)\n","v.backward()\n","x.grad, y.grad"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8smUYmrYjlFw","executionInfo":{"status":"ok","timestamp":1691454370910,"user_tz":-540,"elapsed":4,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"1fbac00d-9cc4-46fd-dca8-393e5cebc44f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(6.), tensor(2.))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["m=100\n","x = np.random.rand(m, 1)* 2\n","y = 3*x + 4 + np.random.randn(m, 1)\n","plt.plot(x, y, '.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":451},"id":"tNt48ZKbjtp2","executionInfo":{"status":"ok","timestamp":1691454371534,"user_tz":-540,"elapsed":627,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"4be99416-ff99-42c1-9e92-b94fa6dfc236"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7ddc7ebc9d80>]"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["a =torch.randn(1,  requires_grad=True)\n","b = torch.randn(1)\n","b.requires_grad = True\n","a, b\n","eta = 0.01\n","\n","xt = torch.tensor(x, dtype = torch.float32)\n","yt = torch.tensor(y).float()\n"],"metadata":{"id":"H4dxGs2-khtP","executionInfo":{"status":"ok","timestamp":1691454371534,"user_tz":-540,"elapsed":7,"user":{"displayName":"서동관","userId":"12646868893277677565"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","for step in range(5000):\n","  # 1.feed-forward\n","  pred = a*xt +b\n","  # 2.loss 계산\n","  mse = torch.mean((pred -yt)**2)\n","  # 3.graient 계산\n","  a.grad = None\n","  b.grad = None\n","  mse.backward()\n","  # 4. update\n","  a.data= a - eta * a.grad\n","  b.data = b - eta *b.grad\n","  print(mse)"],"metadata":{"id":"Sg22PPmHkvjA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691454375261,"user_tz":-540,"elapsed":3731,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"fa353eb9-322c-4cf8-c03c-c3105b49ca9e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(34.4658, grad_fn=<MeanBackward0>)\n","tensor(31.8133, grad_fn=<MeanBackward0>)\n","tensor(29.3725, grad_fn=<MeanBackward0>)\n","tensor(27.1264, grad_fn=<MeanBackward0>)\n","tensor(25.0595, grad_fn=<MeanBackward0>)\n","tensor(23.1574, grad_fn=<MeanBackward0>)\n","tensor(21.4070, grad_fn=<MeanBackward0>)\n","tensor(19.7962, grad_fn=<MeanBackward0>)\n","tensor(18.3139, grad_fn=<MeanBackward0>)\n","tensor(16.9498, grad_fn=<MeanBackward0>)\n","tensor(15.6944, grad_fn=<MeanBackward0>)\n","tensor(14.5392, grad_fn=<MeanBackward0>)\n","tensor(13.4761, grad_fn=<MeanBackward0>)\n","tensor(12.4977, grad_fn=<MeanBackward0>)\n","tensor(11.5973, grad_fn=<MeanBackward0>)\n","tensor(10.7686, grad_fn=<MeanBackward0>)\n","tensor(10.0060, grad_fn=<MeanBackward0>)\n","tensor(9.3042, grad_fn=<MeanBackward0>)\n","tensor(8.6583, grad_fn=<MeanBackward0>)\n","tensor(8.0638, grad_fn=<MeanBackward0>)\n","tensor(7.5167, grad_fn=<MeanBackward0>)\n","tensor(7.0132, grad_fn=<MeanBackward0>)\n","tensor(6.5498, grad_fn=<MeanBackward0>)\n","tensor(6.1232, grad_fn=<MeanBackward0>)\n","tensor(5.7306, grad_fn=<MeanBackward0>)\n","tensor(5.3693, grad_fn=<MeanBackward0>)\n","tensor(5.0367, grad_fn=<MeanBackward0>)\n","tensor(4.7305, grad_fn=<MeanBackward0>)\n","tensor(4.4487, grad_fn=<MeanBackward0>)\n","tensor(4.1893, grad_fn=<MeanBackward0>)\n","tensor(3.9505, grad_fn=<MeanBackward0>)\n","tensor(3.7307, grad_fn=<MeanBackward0>)\n","tensor(3.5284, grad_fn=<MeanBackward0>)\n","tensor(3.3421, grad_fn=<MeanBackward0>)\n","tensor(3.1706, grad_fn=<MeanBackward0>)\n","tensor(3.0127, grad_fn=<MeanBackward0>)\n","tensor(2.8673, grad_fn=<MeanBackward0>)\n","tensor(2.7334, grad_fn=<MeanBackward0>)\n","tensor(2.6101, grad_fn=<MeanBackward0>)\n","tensor(2.4966, grad_fn=<MeanBackward0>)\n","tensor(2.3921, grad_fn=<MeanBackward0>)\n","tensor(2.2958, grad_fn=<MeanBackward0>)\n","tensor(2.2071, grad_fn=<MeanBackward0>)\n","tensor(2.1254, grad_fn=<MeanBackward0>)\n","tensor(2.0502, grad_fn=<MeanBackward0>)\n","tensor(1.9808, grad_fn=<MeanBackward0>)\n","tensor(1.9170, grad_fn=<MeanBackward0>)\n","tensor(1.8581, grad_fn=<MeanBackward0>)\n","tensor(1.8039, grad_fn=<MeanBackward0>)\n","tensor(1.7539, grad_fn=<MeanBackward0>)\n","tensor(1.7078, grad_fn=<MeanBackward0>)\n","tensor(1.6654, grad_fn=<MeanBackward0>)\n","tensor(1.6262, grad_fn=<MeanBackward0>)\n","tensor(1.5901, grad_fn=<MeanBackward0>)\n","tensor(1.5568, grad_fn=<MeanBackward0>)\n","tensor(1.5261, grad_fn=<MeanBackward0>)\n","tensor(1.4977, grad_fn=<MeanBackward0>)\n","tensor(1.4716, grad_fn=<MeanBackward0>)\n","tensor(1.4474, grad_fn=<MeanBackward0>)\n","tensor(1.4251, grad_fn=<MeanBackward0>)\n","tensor(1.4045, grad_fn=<MeanBackward0>)\n","tensor(1.3855, grad_fn=<MeanBackward0>)\n","tensor(1.3680, grad_fn=<MeanBackward0>)\n","tensor(1.3517, grad_fn=<MeanBackward0>)\n","tensor(1.3367, grad_fn=<MeanBackward0>)\n","tensor(1.3228, grad_fn=<MeanBackward0>)\n","tensor(1.3100, grad_fn=<MeanBackward0>)\n","tensor(1.2981, grad_fn=<MeanBackward0>)\n","tensor(1.2871, grad_fn=<MeanBackward0>)\n","tensor(1.2769, grad_fn=<MeanBackward0>)\n","tensor(1.2674, grad_fn=<MeanBackward0>)\n","tensor(1.2586, grad_fn=<MeanBackward0>)\n","tensor(1.2505, grad_fn=<MeanBackward0>)\n","tensor(1.2429, grad_fn=<MeanBackward0>)\n","tensor(1.2359, grad_fn=<MeanBackward0>)\n","tensor(1.2294, grad_fn=<MeanBackward0>)\n","tensor(1.2233, grad_fn=<MeanBackward0>)\n","tensor(1.2177, grad_fn=<MeanBackward0>)\n","tensor(1.2124, grad_fn=<MeanBackward0>)\n","tensor(1.2075, grad_fn=<MeanBackward0>)\n","tensor(1.2029, grad_fn=<MeanBackward0>)\n","tensor(1.1986, grad_fn=<MeanBackward0>)\n","tensor(1.1946, grad_fn=<MeanBackward0>)\n","tensor(1.1909, grad_fn=<MeanBackward0>)\n","tensor(1.1874, grad_fn=<MeanBackward0>)\n","tensor(1.1841, grad_fn=<MeanBackward0>)\n","tensor(1.1810, grad_fn=<MeanBackward0>)\n","tensor(1.1781, grad_fn=<MeanBackward0>)\n","tensor(1.1753, grad_fn=<MeanBackward0>)\n","tensor(1.1728, grad_fn=<MeanBackward0>)\n","tensor(1.1703, grad_fn=<MeanBackward0>)\n","tensor(1.1680, grad_fn=<MeanBackward0>)\n","tensor(1.1659, grad_fn=<MeanBackward0>)\n","tensor(1.1638, grad_fn=<MeanBackward0>)\n","tensor(1.1619, grad_fn=<MeanBackward0>)\n","tensor(1.1600, grad_fn=<MeanBackward0>)\n","tensor(1.1582, grad_fn=<MeanBackward0>)\n","tensor(1.1566, grad_fn=<MeanBackward0>)\n","tensor(1.1550, grad_fn=<MeanBackward0>)\n","tensor(1.1534, grad_fn=<MeanBackward0>)\n","tensor(1.1520, grad_fn=<MeanBackward0>)\n","tensor(1.1505, grad_fn=<MeanBackward0>)\n","tensor(1.1492, grad_fn=<MeanBackward0>)\n","tensor(1.1479, grad_fn=<MeanBackward0>)\n","tensor(1.1467, grad_fn=<MeanBackward0>)\n","tensor(1.1455, grad_fn=<MeanBackward0>)\n","tensor(1.1443, grad_fn=<MeanBackward0>)\n","tensor(1.1432, grad_fn=<MeanBackward0>)\n","tensor(1.1421, grad_fn=<MeanBackward0>)\n","tensor(1.1410, grad_fn=<MeanBackward0>)\n","tensor(1.1400, grad_fn=<MeanBackward0>)\n","tensor(1.1390, grad_fn=<MeanBackward0>)\n","tensor(1.1381, grad_fn=<MeanBackward0>)\n","tensor(1.1371, grad_fn=<MeanBackward0>)\n","tensor(1.1362, grad_fn=<MeanBackward0>)\n","tensor(1.1353, grad_fn=<MeanBackward0>)\n","tensor(1.1344, grad_fn=<MeanBackward0>)\n","tensor(1.1336, grad_fn=<MeanBackward0>)\n","tensor(1.1327, grad_fn=<MeanBackward0>)\n","tensor(1.1319, grad_fn=<MeanBackward0>)\n","tensor(1.1311, grad_fn=<MeanBackward0>)\n","tensor(1.1303, grad_fn=<MeanBackward0>)\n","tensor(1.1296, grad_fn=<MeanBackward0>)\n","tensor(1.1288, grad_fn=<MeanBackward0>)\n","tensor(1.1281, grad_fn=<MeanBackward0>)\n","tensor(1.1273, grad_fn=<MeanBackward0>)\n","tensor(1.1266, grad_fn=<MeanBackward0>)\n","tensor(1.1259, grad_fn=<MeanBackward0>)\n","tensor(1.1252, grad_fn=<MeanBackward0>)\n","tensor(1.1245, grad_fn=<MeanBackward0>)\n","tensor(1.1238, grad_fn=<MeanBackward0>)\n","tensor(1.1231, grad_fn=<MeanBackward0>)\n","tensor(1.1225, grad_fn=<MeanBackward0>)\n","tensor(1.1218, grad_fn=<MeanBackward0>)\n","tensor(1.1211, grad_fn=<MeanBackward0>)\n","tensor(1.1205, grad_fn=<MeanBackward0>)\n","tensor(1.1199, grad_fn=<MeanBackward0>)\n","tensor(1.1192, grad_fn=<MeanBackward0>)\n","tensor(1.1186, grad_fn=<MeanBackward0>)\n","tensor(1.1180, grad_fn=<MeanBackward0>)\n","tensor(1.1174, grad_fn=<MeanBackward0>)\n","tensor(1.1168, grad_fn=<MeanBackward0>)\n","tensor(1.1162, grad_fn=<MeanBackward0>)\n","tensor(1.1156, grad_fn=<MeanBackward0>)\n","tensor(1.1150, grad_fn=<MeanBackward0>)\n","tensor(1.1144, grad_fn=<MeanBackward0>)\n","tensor(1.1138, grad_fn=<MeanBackward0>)\n","tensor(1.1132, grad_fn=<MeanBackward0>)\n","tensor(1.1127, grad_fn=<MeanBackward0>)\n","tensor(1.1121, grad_fn=<MeanBackward0>)\n","tensor(1.1116, grad_fn=<MeanBackward0>)\n","tensor(1.1110, grad_fn=<MeanBackward0>)\n","tensor(1.1104, grad_fn=<MeanBackward0>)\n","tensor(1.1099, grad_fn=<MeanBackward0>)\n","tensor(1.1094, grad_fn=<MeanBackward0>)\n","tensor(1.1088, grad_fn=<MeanBackward0>)\n","tensor(1.1083, grad_fn=<MeanBackward0>)\n","tensor(1.1078, grad_fn=<MeanBackward0>)\n","tensor(1.1072, grad_fn=<MeanBackward0>)\n","tensor(1.1067, grad_fn=<MeanBackward0>)\n","tensor(1.1062, grad_fn=<MeanBackward0>)\n","tensor(1.1057, grad_fn=<MeanBackward0>)\n","tensor(1.1052, grad_fn=<MeanBackward0>)\n","tensor(1.1046, grad_fn=<MeanBackward0>)\n","tensor(1.1041, grad_fn=<MeanBackward0>)\n","tensor(1.1036, grad_fn=<MeanBackward0>)\n","tensor(1.1031, grad_fn=<MeanBackward0>)\n","tensor(1.1027, grad_fn=<MeanBackward0>)\n","tensor(1.1022, grad_fn=<MeanBackward0>)\n","tensor(1.1017, grad_fn=<MeanBackward0>)\n","tensor(1.1012, grad_fn=<MeanBackward0>)\n","tensor(1.1007, grad_fn=<MeanBackward0>)\n","tensor(1.1002, grad_fn=<MeanBackward0>)\n","tensor(1.0998, grad_fn=<MeanBackward0>)\n","tensor(1.0993, grad_fn=<MeanBackward0>)\n","tensor(1.0988, grad_fn=<MeanBackward0>)\n","tensor(1.0984, grad_fn=<MeanBackward0>)\n","tensor(1.0979, grad_fn=<MeanBackward0>)\n","tensor(1.0974, grad_fn=<MeanBackward0>)\n","tensor(1.0970, grad_fn=<MeanBackward0>)\n","tensor(1.0965, grad_fn=<MeanBackward0>)\n","tensor(1.0961, grad_fn=<MeanBackward0>)\n","tensor(1.0956, grad_fn=<MeanBackward0>)\n","tensor(1.0952, grad_fn=<MeanBackward0>)\n","tensor(1.0948, grad_fn=<MeanBackward0>)\n","tensor(1.0943, grad_fn=<MeanBackward0>)\n","tensor(1.0939, grad_fn=<MeanBackward0>)\n","tensor(1.0935, grad_fn=<MeanBackward0>)\n","tensor(1.0930, grad_fn=<MeanBackward0>)\n","tensor(1.0926, grad_fn=<MeanBackward0>)\n","tensor(1.0922, grad_fn=<MeanBackward0>)\n","tensor(1.0918, grad_fn=<MeanBackward0>)\n","tensor(1.0913, grad_fn=<MeanBackward0>)\n","tensor(1.0909, grad_fn=<MeanBackward0>)\n","tensor(1.0905, grad_fn=<MeanBackward0>)\n","tensor(1.0901, grad_fn=<MeanBackward0>)\n","tensor(1.0897, grad_fn=<MeanBackward0>)\n","tensor(1.0893, grad_fn=<MeanBackward0>)\n","tensor(1.0889, grad_fn=<MeanBackward0>)\n","tensor(1.0885, grad_fn=<MeanBackward0>)\n","tensor(1.0881, grad_fn=<MeanBackward0>)\n","tensor(1.0877, grad_fn=<MeanBackward0>)\n","tensor(1.0873, grad_fn=<MeanBackward0>)\n","tensor(1.0869, grad_fn=<MeanBackward0>)\n","tensor(1.0865, grad_fn=<MeanBackward0>)\n","tensor(1.0862, grad_fn=<MeanBackward0>)\n","tensor(1.0858, grad_fn=<MeanBackward0>)\n","tensor(1.0854, grad_fn=<MeanBackward0>)\n","tensor(1.0850, grad_fn=<MeanBackward0>)\n","tensor(1.0847, grad_fn=<MeanBackward0>)\n","tensor(1.0843, grad_fn=<MeanBackward0>)\n","tensor(1.0839, grad_fn=<MeanBackward0>)\n","tensor(1.0836, grad_fn=<MeanBackward0>)\n","tensor(1.0832, grad_fn=<MeanBackward0>)\n","tensor(1.0828, grad_fn=<MeanBackward0>)\n","tensor(1.0825, grad_fn=<MeanBackward0>)\n","tensor(1.0821, grad_fn=<MeanBackward0>)\n","tensor(1.0818, grad_fn=<MeanBackward0>)\n","tensor(1.0814, grad_fn=<MeanBackward0>)\n","tensor(1.0811, grad_fn=<MeanBackward0>)\n","tensor(1.0807, grad_fn=<MeanBackward0>)\n","tensor(1.0804, grad_fn=<MeanBackward0>)\n","tensor(1.0800, grad_fn=<MeanBackward0>)\n","tensor(1.0797, grad_fn=<MeanBackward0>)\n","tensor(1.0794, grad_fn=<MeanBackward0>)\n","tensor(1.0790, grad_fn=<MeanBackward0>)\n","tensor(1.0787, grad_fn=<MeanBackward0>)\n","tensor(1.0784, grad_fn=<MeanBackward0>)\n","tensor(1.0780, grad_fn=<MeanBackward0>)\n","tensor(1.0777, grad_fn=<MeanBackward0>)\n","tensor(1.0774, grad_fn=<MeanBackward0>)\n","tensor(1.0771, grad_fn=<MeanBackward0>)\n","tensor(1.0767, grad_fn=<MeanBackward0>)\n","tensor(1.0764, grad_fn=<MeanBackward0>)\n","tensor(1.0761, grad_fn=<MeanBackward0>)\n","tensor(1.0758, grad_fn=<MeanBackward0>)\n","tensor(1.0755, grad_fn=<MeanBackward0>)\n","tensor(1.0752, grad_fn=<MeanBackward0>)\n","tensor(1.0749, grad_fn=<MeanBackward0>)\n","tensor(1.0746, grad_fn=<MeanBackward0>)\n","tensor(1.0742, grad_fn=<MeanBackward0>)\n","tensor(1.0739, grad_fn=<MeanBackward0>)\n","tensor(1.0736, grad_fn=<MeanBackward0>)\n","tensor(1.0733, grad_fn=<MeanBackward0>)\n","tensor(1.0731, grad_fn=<MeanBackward0>)\n","tensor(1.0728, grad_fn=<MeanBackward0>)\n","tensor(1.0725, grad_fn=<MeanBackward0>)\n","tensor(1.0722, grad_fn=<MeanBackward0>)\n","tensor(1.0719, grad_fn=<MeanBackward0>)\n","tensor(1.0716, grad_fn=<MeanBackward0>)\n","tensor(1.0713, grad_fn=<MeanBackward0>)\n","tensor(1.0710, grad_fn=<MeanBackward0>)\n","tensor(1.0708, grad_fn=<MeanBackward0>)\n","tensor(1.0705, grad_fn=<MeanBackward0>)\n","tensor(1.0702, grad_fn=<MeanBackward0>)\n","tensor(1.0699, grad_fn=<MeanBackward0>)\n","tensor(1.0696, grad_fn=<MeanBackward0>)\n","tensor(1.0694, grad_fn=<MeanBackward0>)\n","tensor(1.0691, grad_fn=<MeanBackward0>)\n","tensor(1.0688, grad_fn=<MeanBackward0>)\n","tensor(1.0686, grad_fn=<MeanBackward0>)\n","tensor(1.0683, grad_fn=<MeanBackward0>)\n","tensor(1.0680, grad_fn=<MeanBackward0>)\n","tensor(1.0678, grad_fn=<MeanBackward0>)\n","tensor(1.0675, grad_fn=<MeanBackward0>)\n","tensor(1.0673, grad_fn=<MeanBackward0>)\n","tensor(1.0670, grad_fn=<MeanBackward0>)\n","tensor(1.0668, grad_fn=<MeanBackward0>)\n","tensor(1.0665, grad_fn=<MeanBackward0>)\n","tensor(1.0663, grad_fn=<MeanBackward0>)\n","tensor(1.0660, grad_fn=<MeanBackward0>)\n","tensor(1.0658, grad_fn=<MeanBackward0>)\n","tensor(1.0655, grad_fn=<MeanBackward0>)\n","tensor(1.0653, grad_fn=<MeanBackward0>)\n","tensor(1.0650, grad_fn=<MeanBackward0>)\n","tensor(1.0648, grad_fn=<MeanBackward0>)\n","tensor(1.0645, grad_fn=<MeanBackward0>)\n","tensor(1.0643, grad_fn=<MeanBackward0>)\n","tensor(1.0641, grad_fn=<MeanBackward0>)\n","tensor(1.0638, grad_fn=<MeanBackward0>)\n","tensor(1.0636, grad_fn=<MeanBackward0>)\n","tensor(1.0634, grad_fn=<MeanBackward0>)\n","tensor(1.0631, grad_fn=<MeanBackward0>)\n","tensor(1.0629, grad_fn=<MeanBackward0>)\n","tensor(1.0627, grad_fn=<MeanBackward0>)\n","tensor(1.0625, grad_fn=<MeanBackward0>)\n","tensor(1.0622, grad_fn=<MeanBackward0>)\n","tensor(1.0620, grad_fn=<MeanBackward0>)\n","tensor(1.0618, grad_fn=<MeanBackward0>)\n","tensor(1.0616, grad_fn=<MeanBackward0>)\n","tensor(1.0613, grad_fn=<MeanBackward0>)\n","tensor(1.0611, grad_fn=<MeanBackward0>)\n","tensor(1.0609, grad_fn=<MeanBackward0>)\n","tensor(1.0607, grad_fn=<MeanBackward0>)\n","tensor(1.0605, grad_fn=<MeanBackward0>)\n","tensor(1.0603, grad_fn=<MeanBackward0>)\n","tensor(1.0601, grad_fn=<MeanBackward0>)\n","tensor(1.0599, grad_fn=<MeanBackward0>)\n","tensor(1.0596, grad_fn=<MeanBackward0>)\n","tensor(1.0594, grad_fn=<MeanBackward0>)\n","tensor(1.0592, grad_fn=<MeanBackward0>)\n","tensor(1.0590, grad_fn=<MeanBackward0>)\n","tensor(1.0588, grad_fn=<MeanBackward0>)\n","tensor(1.0586, grad_fn=<MeanBackward0>)\n","tensor(1.0584, grad_fn=<MeanBackward0>)\n","tensor(1.0582, grad_fn=<MeanBackward0>)\n","tensor(1.0580, grad_fn=<MeanBackward0>)\n","tensor(1.0578, grad_fn=<MeanBackward0>)\n","tensor(1.0576, grad_fn=<MeanBackward0>)\n","tensor(1.0575, grad_fn=<MeanBackward0>)\n","tensor(1.0573, grad_fn=<MeanBackward0>)\n","tensor(1.0571, grad_fn=<MeanBackward0>)\n","tensor(1.0569, grad_fn=<MeanBackward0>)\n","tensor(1.0567, grad_fn=<MeanBackward0>)\n","tensor(1.0565, grad_fn=<MeanBackward0>)\n","tensor(1.0563, grad_fn=<MeanBackward0>)\n","tensor(1.0561, grad_fn=<MeanBackward0>)\n","tensor(1.0560, grad_fn=<MeanBackward0>)\n","tensor(1.0558, grad_fn=<MeanBackward0>)\n","tensor(1.0556, grad_fn=<MeanBackward0>)\n","tensor(1.0554, grad_fn=<MeanBackward0>)\n","tensor(1.0552, grad_fn=<MeanBackward0>)\n","tensor(1.0551, grad_fn=<MeanBackward0>)\n","tensor(1.0549, grad_fn=<MeanBackward0>)\n","tensor(1.0547, grad_fn=<MeanBackward0>)\n","tensor(1.0545, grad_fn=<MeanBackward0>)\n","tensor(1.0544, grad_fn=<MeanBackward0>)\n","tensor(1.0542, grad_fn=<MeanBackward0>)\n","tensor(1.0540, grad_fn=<MeanBackward0>)\n","tensor(1.0539, grad_fn=<MeanBackward0>)\n","tensor(1.0537, grad_fn=<MeanBackward0>)\n","tensor(1.0535, grad_fn=<MeanBackward0>)\n","tensor(1.0534, grad_fn=<MeanBackward0>)\n","tensor(1.0532, grad_fn=<MeanBackward0>)\n","tensor(1.0530, grad_fn=<MeanBackward0>)\n","tensor(1.0529, grad_fn=<MeanBackward0>)\n","tensor(1.0527, grad_fn=<MeanBackward0>)\n","tensor(1.0525, grad_fn=<MeanBackward0>)\n","tensor(1.0524, grad_fn=<MeanBackward0>)\n","tensor(1.0522, grad_fn=<MeanBackward0>)\n","tensor(1.0521, grad_fn=<MeanBackward0>)\n","tensor(1.0519, grad_fn=<MeanBackward0>)\n","tensor(1.0518, grad_fn=<MeanBackward0>)\n","tensor(1.0516, grad_fn=<MeanBackward0>)\n","tensor(1.0514, grad_fn=<MeanBackward0>)\n","tensor(1.0513, grad_fn=<MeanBackward0>)\n","tensor(1.0511, grad_fn=<MeanBackward0>)\n","tensor(1.0510, grad_fn=<MeanBackward0>)\n","tensor(1.0508, grad_fn=<MeanBackward0>)\n","tensor(1.0507, grad_fn=<MeanBackward0>)\n","tensor(1.0505, grad_fn=<MeanBackward0>)\n","tensor(1.0504, grad_fn=<MeanBackward0>)\n","tensor(1.0503, grad_fn=<MeanBackward0>)\n","tensor(1.0501, grad_fn=<MeanBackward0>)\n","tensor(1.0500, grad_fn=<MeanBackward0>)\n","tensor(1.0498, grad_fn=<MeanBackward0>)\n","tensor(1.0497, grad_fn=<MeanBackward0>)\n","tensor(1.0495, grad_fn=<MeanBackward0>)\n","tensor(1.0494, grad_fn=<MeanBackward0>)\n","tensor(1.0493, grad_fn=<MeanBackward0>)\n","tensor(1.0491, grad_fn=<MeanBackward0>)\n","tensor(1.0490, grad_fn=<MeanBackward0>)\n","tensor(1.0489, grad_fn=<MeanBackward0>)\n","tensor(1.0487, grad_fn=<MeanBackward0>)\n","tensor(1.0486, grad_fn=<MeanBackward0>)\n","tensor(1.0484, grad_fn=<MeanBackward0>)\n","tensor(1.0483, grad_fn=<MeanBackward0>)\n","tensor(1.0482, grad_fn=<MeanBackward0>)\n","tensor(1.0481, grad_fn=<MeanBackward0>)\n","tensor(1.0479, grad_fn=<MeanBackward0>)\n","tensor(1.0478, grad_fn=<MeanBackward0>)\n","tensor(1.0477, grad_fn=<MeanBackward0>)\n","tensor(1.0475, grad_fn=<MeanBackward0>)\n","tensor(1.0474, grad_fn=<MeanBackward0>)\n","tensor(1.0473, grad_fn=<MeanBackward0>)\n","tensor(1.0472, grad_fn=<MeanBackward0>)\n","tensor(1.0470, grad_fn=<MeanBackward0>)\n","tensor(1.0469, grad_fn=<MeanBackward0>)\n","tensor(1.0468, grad_fn=<MeanBackward0>)\n","tensor(1.0467, grad_fn=<MeanBackward0>)\n","tensor(1.0466, grad_fn=<MeanBackward0>)\n","tensor(1.0464, grad_fn=<MeanBackward0>)\n","tensor(1.0463, grad_fn=<MeanBackward0>)\n","tensor(1.0462, grad_fn=<MeanBackward0>)\n","tensor(1.0461, grad_fn=<MeanBackward0>)\n","tensor(1.0460, grad_fn=<MeanBackward0>)\n","tensor(1.0458, grad_fn=<MeanBackward0>)\n","tensor(1.0457, grad_fn=<MeanBackward0>)\n","tensor(1.0456, grad_fn=<MeanBackward0>)\n","tensor(1.0455, grad_fn=<MeanBackward0>)\n","tensor(1.0454, grad_fn=<MeanBackward0>)\n","tensor(1.0453, grad_fn=<MeanBackward0>)\n","tensor(1.0452, grad_fn=<MeanBackward0>)\n","tensor(1.0450, grad_fn=<MeanBackward0>)\n","tensor(1.0449, grad_fn=<MeanBackward0>)\n","tensor(1.0448, grad_fn=<MeanBackward0>)\n","tensor(1.0447, grad_fn=<MeanBackward0>)\n","tensor(1.0446, grad_fn=<MeanBackward0>)\n","tensor(1.0445, grad_fn=<MeanBackward0>)\n","tensor(1.0444, grad_fn=<MeanBackward0>)\n","tensor(1.0443, grad_fn=<MeanBackward0>)\n","tensor(1.0442, grad_fn=<MeanBackward0>)\n","tensor(1.0441, grad_fn=<MeanBackward0>)\n","tensor(1.0440, grad_fn=<MeanBackward0>)\n","tensor(1.0439, grad_fn=<MeanBackward0>)\n","tensor(1.0438, grad_fn=<MeanBackward0>)\n","tensor(1.0437, grad_fn=<MeanBackward0>)\n","tensor(1.0436, grad_fn=<MeanBackward0>)\n","tensor(1.0435, grad_fn=<MeanBackward0>)\n","tensor(1.0434, grad_fn=<MeanBackward0>)\n","tensor(1.0433, grad_fn=<MeanBackward0>)\n","tensor(1.0432, grad_fn=<MeanBackward0>)\n","tensor(1.0431, grad_fn=<MeanBackward0>)\n","tensor(1.0430, grad_fn=<MeanBackward0>)\n","tensor(1.0429, grad_fn=<MeanBackward0>)\n","tensor(1.0428, grad_fn=<MeanBackward0>)\n","tensor(1.0427, grad_fn=<MeanBackward0>)\n","tensor(1.0426, grad_fn=<MeanBackward0>)\n","tensor(1.0425, grad_fn=<MeanBackward0>)\n","tensor(1.0424, grad_fn=<MeanBackward0>)\n","tensor(1.0423, grad_fn=<MeanBackward0>)\n","tensor(1.0422, grad_fn=<MeanBackward0>)\n","tensor(1.0421, grad_fn=<MeanBackward0>)\n","tensor(1.0420, grad_fn=<MeanBackward0>)\n","tensor(1.0419, grad_fn=<MeanBackward0>)\n","tensor(1.0419, grad_fn=<MeanBackward0>)\n","tensor(1.0418, grad_fn=<MeanBackward0>)\n","tensor(1.0417, grad_fn=<MeanBackward0>)\n","tensor(1.0416, grad_fn=<MeanBackward0>)\n","tensor(1.0415, grad_fn=<MeanBackward0>)\n","tensor(1.0414, grad_fn=<MeanBackward0>)\n","tensor(1.0413, grad_fn=<MeanBackward0>)\n","tensor(1.0412, grad_fn=<MeanBackward0>)\n","tensor(1.0412, grad_fn=<MeanBackward0>)\n","tensor(1.0411, grad_fn=<MeanBackward0>)\n","tensor(1.0410, grad_fn=<MeanBackward0>)\n","tensor(1.0409, grad_fn=<MeanBackward0>)\n","tensor(1.0408, grad_fn=<MeanBackward0>)\n","tensor(1.0407, grad_fn=<MeanBackward0>)\n","tensor(1.0407, grad_fn=<MeanBackward0>)\n","tensor(1.0406, grad_fn=<MeanBackward0>)\n","tensor(1.0405, grad_fn=<MeanBackward0>)\n","tensor(1.0404, grad_fn=<MeanBackward0>)\n","tensor(1.0403, grad_fn=<MeanBackward0>)\n","tensor(1.0403, grad_fn=<MeanBackward0>)\n","tensor(1.0402, grad_fn=<MeanBackward0>)\n","tensor(1.0401, grad_fn=<MeanBackward0>)\n","tensor(1.0400, grad_fn=<MeanBackward0>)\n","tensor(1.0399, grad_fn=<MeanBackward0>)\n","tensor(1.0399, grad_fn=<MeanBackward0>)\n","tensor(1.0398, grad_fn=<MeanBackward0>)\n","tensor(1.0397, grad_fn=<MeanBackward0>)\n","tensor(1.0396, grad_fn=<MeanBackward0>)\n","tensor(1.0396, grad_fn=<MeanBackward0>)\n","tensor(1.0395, grad_fn=<MeanBackward0>)\n","tensor(1.0394, grad_fn=<MeanBackward0>)\n","tensor(1.0393, grad_fn=<MeanBackward0>)\n","tensor(1.0393, grad_fn=<MeanBackward0>)\n","tensor(1.0392, grad_fn=<MeanBackward0>)\n","tensor(1.0391, grad_fn=<MeanBackward0>)\n","tensor(1.0391, grad_fn=<MeanBackward0>)\n","tensor(1.0390, grad_fn=<MeanBackward0>)\n","tensor(1.0389, grad_fn=<MeanBackward0>)\n","tensor(1.0388, grad_fn=<MeanBackward0>)\n","tensor(1.0388, grad_fn=<MeanBackward0>)\n","tensor(1.0387, grad_fn=<MeanBackward0>)\n","tensor(1.0386, grad_fn=<MeanBackward0>)\n","tensor(1.0386, grad_fn=<MeanBackward0>)\n","tensor(1.0385, grad_fn=<MeanBackward0>)\n","tensor(1.0384, grad_fn=<MeanBackward0>)\n","tensor(1.0384, grad_fn=<MeanBackward0>)\n","tensor(1.0383, grad_fn=<MeanBackward0>)\n","tensor(1.0382, grad_fn=<MeanBackward0>)\n","tensor(1.0382, grad_fn=<MeanBackward0>)\n","tensor(1.0381, grad_fn=<MeanBackward0>)\n","tensor(1.0380, grad_fn=<MeanBackward0>)\n","tensor(1.0380, grad_fn=<MeanBackward0>)\n","tensor(1.0379, grad_fn=<MeanBackward0>)\n","tensor(1.0378, grad_fn=<MeanBackward0>)\n","tensor(1.0378, grad_fn=<MeanBackward0>)\n","tensor(1.0377, grad_fn=<MeanBackward0>)\n","tensor(1.0377, grad_fn=<MeanBackward0>)\n","tensor(1.0376, grad_fn=<MeanBackward0>)\n","tensor(1.0375, grad_fn=<MeanBackward0>)\n","tensor(1.0375, grad_fn=<MeanBackward0>)\n","tensor(1.0374, grad_fn=<MeanBackward0>)\n","tensor(1.0374, grad_fn=<MeanBackward0>)\n","tensor(1.0373, grad_fn=<MeanBackward0>)\n","tensor(1.0372, grad_fn=<MeanBackward0>)\n","tensor(1.0372, grad_fn=<MeanBackward0>)\n","tensor(1.0371, grad_fn=<MeanBackward0>)\n","tensor(1.0371, grad_fn=<MeanBackward0>)\n","tensor(1.0370, grad_fn=<MeanBackward0>)\n","tensor(1.0369, grad_fn=<MeanBackward0>)\n","tensor(1.0369, grad_fn=<MeanBackward0>)\n","tensor(1.0368, grad_fn=<MeanBackward0>)\n","tensor(1.0368, grad_fn=<MeanBackward0>)\n","tensor(1.0367, grad_fn=<MeanBackward0>)\n","tensor(1.0367, grad_fn=<MeanBackward0>)\n","tensor(1.0366, grad_fn=<MeanBackward0>)\n","tensor(1.0366, grad_fn=<MeanBackward0>)\n","tensor(1.0365, grad_fn=<MeanBackward0>)\n","tensor(1.0364, grad_fn=<MeanBackward0>)\n","tensor(1.0364, grad_fn=<MeanBackward0>)\n","tensor(1.0363, grad_fn=<MeanBackward0>)\n","tensor(1.0363, grad_fn=<MeanBackward0>)\n","tensor(1.0362, grad_fn=<MeanBackward0>)\n","tensor(1.0362, grad_fn=<MeanBackward0>)\n","tensor(1.0361, grad_fn=<MeanBackward0>)\n","tensor(1.0361, grad_fn=<MeanBackward0>)\n","tensor(1.0360, grad_fn=<MeanBackward0>)\n","tensor(1.0360, grad_fn=<MeanBackward0>)\n","tensor(1.0359, grad_fn=<MeanBackward0>)\n","tensor(1.0359, grad_fn=<MeanBackward0>)\n","tensor(1.0358, grad_fn=<MeanBackward0>)\n","tensor(1.0358, grad_fn=<MeanBackward0>)\n","tensor(1.0357, grad_fn=<MeanBackward0>)\n","tensor(1.0357, grad_fn=<MeanBackward0>)\n","tensor(1.0356, grad_fn=<MeanBackward0>)\n","tensor(1.0356, grad_fn=<MeanBackward0>)\n","tensor(1.0355, grad_fn=<MeanBackward0>)\n","tensor(1.0355, grad_fn=<MeanBackward0>)\n","tensor(1.0354, grad_fn=<MeanBackward0>)\n","tensor(1.0354, grad_fn=<MeanBackward0>)\n","tensor(1.0353, grad_fn=<MeanBackward0>)\n","tensor(1.0353, grad_fn=<MeanBackward0>)\n","tensor(1.0352, grad_fn=<MeanBackward0>)\n","tensor(1.0352, grad_fn=<MeanBackward0>)\n","tensor(1.0352, grad_fn=<MeanBackward0>)\n","tensor(1.0351, grad_fn=<MeanBackward0>)\n","tensor(1.0351, grad_fn=<MeanBackward0>)\n","tensor(1.0350, grad_fn=<MeanBackward0>)\n","tensor(1.0350, grad_fn=<MeanBackward0>)\n","tensor(1.0349, grad_fn=<MeanBackward0>)\n","tensor(1.0349, grad_fn=<MeanBackward0>)\n","tensor(1.0348, grad_fn=<MeanBackward0>)\n","tensor(1.0348, grad_fn=<MeanBackward0>)\n","tensor(1.0348, grad_fn=<MeanBackward0>)\n","tensor(1.0347, grad_fn=<MeanBackward0>)\n","tensor(1.0347, grad_fn=<MeanBackward0>)\n","tensor(1.0346, grad_fn=<MeanBackward0>)\n","tensor(1.0346, grad_fn=<MeanBackward0>)\n","tensor(1.0345, grad_fn=<MeanBackward0>)\n","tensor(1.0345, grad_fn=<MeanBackward0>)\n","tensor(1.0345, grad_fn=<MeanBackward0>)\n","tensor(1.0344, grad_fn=<MeanBackward0>)\n","tensor(1.0344, grad_fn=<MeanBackward0>)\n","tensor(1.0343, grad_fn=<MeanBackward0>)\n","tensor(1.0343, grad_fn=<MeanBackward0>)\n","tensor(1.0343, grad_fn=<MeanBackward0>)\n","tensor(1.0342, grad_fn=<MeanBackward0>)\n","tensor(1.0342, grad_fn=<MeanBackward0>)\n","tensor(1.0341, grad_fn=<MeanBackward0>)\n","tensor(1.0341, grad_fn=<MeanBackward0>)\n","tensor(1.0341, grad_fn=<MeanBackward0>)\n","tensor(1.0340, grad_fn=<MeanBackward0>)\n","tensor(1.0340, grad_fn=<MeanBackward0>)\n","tensor(1.0340, grad_fn=<MeanBackward0>)\n","tensor(1.0339, grad_fn=<MeanBackward0>)\n","tensor(1.0339, grad_fn=<MeanBackward0>)\n","tensor(1.0338, grad_fn=<MeanBackward0>)\n","tensor(1.0338, grad_fn=<MeanBackward0>)\n","tensor(1.0338, grad_fn=<MeanBackward0>)\n","tensor(1.0337, grad_fn=<MeanBackward0>)\n","tensor(1.0337, grad_fn=<MeanBackward0>)\n","tensor(1.0337, grad_fn=<MeanBackward0>)\n","tensor(1.0336, grad_fn=<MeanBackward0>)\n","tensor(1.0336, grad_fn=<MeanBackward0>)\n","tensor(1.0336, grad_fn=<MeanBackward0>)\n","tensor(1.0335, grad_fn=<MeanBackward0>)\n","tensor(1.0335, grad_fn=<MeanBackward0>)\n","tensor(1.0335, grad_fn=<MeanBackward0>)\n","tensor(1.0334, grad_fn=<MeanBackward0>)\n","tensor(1.0334, grad_fn=<MeanBackward0>)\n","tensor(1.0334, grad_fn=<MeanBackward0>)\n","tensor(1.0333, grad_fn=<MeanBackward0>)\n","tensor(1.0333, grad_fn=<MeanBackward0>)\n","tensor(1.0333, grad_fn=<MeanBackward0>)\n","tensor(1.0332, grad_fn=<MeanBackward0>)\n","tensor(1.0332, grad_fn=<MeanBackward0>)\n","tensor(1.0332, grad_fn=<MeanBackward0>)\n","tensor(1.0331, grad_fn=<MeanBackward0>)\n","tensor(1.0331, grad_fn=<MeanBackward0>)\n","tensor(1.0331, grad_fn=<MeanBackward0>)\n","tensor(1.0330, grad_fn=<MeanBackward0>)\n","tensor(1.0330, grad_fn=<MeanBackward0>)\n","tensor(1.0330, grad_fn=<MeanBackward0>)\n","tensor(1.0329, grad_fn=<MeanBackward0>)\n","tensor(1.0329, grad_fn=<MeanBackward0>)\n","tensor(1.0329, grad_fn=<MeanBackward0>)\n","tensor(1.0328, grad_fn=<MeanBackward0>)\n","tensor(1.0328, grad_fn=<MeanBackward0>)\n","tensor(1.0328, grad_fn=<MeanBackward0>)\n","tensor(1.0328, grad_fn=<MeanBackward0>)\n","tensor(1.0327, grad_fn=<MeanBackward0>)\n","tensor(1.0327, grad_fn=<MeanBackward0>)\n","tensor(1.0327, grad_fn=<MeanBackward0>)\n","tensor(1.0326, grad_fn=<MeanBackward0>)\n","tensor(1.0326, grad_fn=<MeanBackward0>)\n","tensor(1.0326, grad_fn=<MeanBackward0>)\n","tensor(1.0325, grad_fn=<MeanBackward0>)\n","tensor(1.0325, grad_fn=<MeanBackward0>)\n","tensor(1.0325, grad_fn=<MeanBackward0>)\n","tensor(1.0325, grad_fn=<MeanBackward0>)\n","tensor(1.0324, grad_fn=<MeanBackward0>)\n","tensor(1.0324, grad_fn=<MeanBackward0>)\n","tensor(1.0324, grad_fn=<MeanBackward0>)\n","tensor(1.0324, grad_fn=<MeanBackward0>)\n","tensor(1.0323, grad_fn=<MeanBackward0>)\n","tensor(1.0323, grad_fn=<MeanBackward0>)\n","tensor(1.0323, grad_fn=<MeanBackward0>)\n","tensor(1.0322, grad_fn=<MeanBackward0>)\n","tensor(1.0322, grad_fn=<MeanBackward0>)\n","tensor(1.0322, grad_fn=<MeanBackward0>)\n","tensor(1.0322, grad_fn=<MeanBackward0>)\n","tensor(1.0321, grad_fn=<MeanBackward0>)\n","tensor(1.0321, grad_fn=<MeanBackward0>)\n","tensor(1.0321, grad_fn=<MeanBackward0>)\n","tensor(1.0321, grad_fn=<MeanBackward0>)\n","tensor(1.0320, grad_fn=<MeanBackward0>)\n","tensor(1.0320, grad_fn=<MeanBackward0>)\n","tensor(1.0320, grad_fn=<MeanBackward0>)\n","tensor(1.0320, grad_fn=<MeanBackward0>)\n","tensor(1.0319, grad_fn=<MeanBackward0>)\n","tensor(1.0319, grad_fn=<MeanBackward0>)\n","tensor(1.0319, grad_fn=<MeanBackward0>)\n","tensor(1.0319, grad_fn=<MeanBackward0>)\n","tensor(1.0318, grad_fn=<MeanBackward0>)\n","tensor(1.0318, grad_fn=<MeanBackward0>)\n","tensor(1.0318, grad_fn=<MeanBackward0>)\n","tensor(1.0318, grad_fn=<MeanBackward0>)\n","tensor(1.0318, grad_fn=<MeanBackward0>)\n","tensor(1.0317, grad_fn=<MeanBackward0>)\n","tensor(1.0317, grad_fn=<MeanBackward0>)\n","tensor(1.0317, grad_fn=<MeanBackward0>)\n","tensor(1.0317, grad_fn=<MeanBackward0>)\n","tensor(1.0316, grad_fn=<MeanBackward0>)\n","tensor(1.0316, grad_fn=<MeanBackward0>)\n","tensor(1.0316, grad_fn=<MeanBackward0>)\n","tensor(1.0316, grad_fn=<MeanBackward0>)\n","tensor(1.0316, grad_fn=<MeanBackward0>)\n","tensor(1.0315, grad_fn=<MeanBackward0>)\n","tensor(1.0315, grad_fn=<MeanBackward0>)\n","tensor(1.0315, grad_fn=<MeanBackward0>)\n","tensor(1.0315, grad_fn=<MeanBackward0>)\n","tensor(1.0314, grad_fn=<MeanBackward0>)\n","tensor(1.0314, grad_fn=<MeanBackward0>)\n","tensor(1.0314, grad_fn=<MeanBackward0>)\n","tensor(1.0314, grad_fn=<MeanBackward0>)\n","tensor(1.0314, grad_fn=<MeanBackward0>)\n","tensor(1.0313, grad_fn=<MeanBackward0>)\n","tensor(1.0313, grad_fn=<MeanBackward0>)\n","tensor(1.0313, grad_fn=<MeanBackward0>)\n","tensor(1.0313, grad_fn=<MeanBackward0>)\n","tensor(1.0313, grad_fn=<MeanBackward0>)\n","tensor(1.0312, grad_fn=<MeanBackward0>)\n","tensor(1.0312, grad_fn=<MeanBackward0>)\n","tensor(1.0312, grad_fn=<MeanBackward0>)\n","tensor(1.0312, grad_fn=<MeanBackward0>)\n","tensor(1.0312, grad_fn=<MeanBackward0>)\n","tensor(1.0311, grad_fn=<MeanBackward0>)\n","tensor(1.0311, grad_fn=<MeanBackward0>)\n","tensor(1.0311, grad_fn=<MeanBackward0>)\n","tensor(1.0311, grad_fn=<MeanBackward0>)\n","tensor(1.0311, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0310, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0309, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0308, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0307, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0306, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0305, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0304, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0303, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0302, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0301, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0300, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0299, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0298, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0297, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0296, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0295, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0294, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0293, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0292, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0291, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0290, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0289, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0288, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0287, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0286, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0285, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0284, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0283, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n","tensor(1.0282, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["x_new = torch.tensor([[0],[2]])\n","a*x_new +b"],"metadata":{"id":"tLRbv-WRmHCf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691454375261,"user_tz":-540,"elapsed":29,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"b36d80da-7635-4827-e05c-b364007b86e2"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4.2017],\n","        [9.9550]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["m=100\n","x_train = torch.rand(m, 1) *6 -3\n","y_train = 0.5 * x_train **2 + x_train +2 + torch.randn(m, 1)\n","x_valid = torch.rand(m, 1) *6 -3\n","y_valid = 0.5 * x_valid ** 2 + x_valid + 2 + torch.randn(m, 1)\n","\n","plt.plot(x_train, y_train, '.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"4_fvatE1nszN","executionInfo":{"status":"ok","timestamp":1691454375746,"user_tz":-540,"elapsed":494,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"3782371c-2fd3-4198-e4e8-cdfbb19249df"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7ddc7caee830>]"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["a = torch.randn(1, requires_grad= True)\n","b = torch.randn(1, requires_grad=True)\n","\n","def model(x):  # model = torch.nn.Linear(1(입력값),1(출력값)) 한번에 함수를 만듬\n","  return a*x + b\n","opt = torch.optim.Adam([a, b]) # opt = torch.optim.Adam(model.parameters())\n","loss_fn = torch.nn.MSELoss() # loss_fn= torch.mean((pred - y_train)**2)\n","pred= model(x_train)\n","loss_fn(pred, y_train)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dUGqDMWCnswd","executionInfo":{"status":"ok","timestamp":1691454375746,"user_tz":-540,"elapsed":9,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"5ce813fc-9379-4837-9c89-1e28a3bddf7c"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(24.8019, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["for step in range(5000):\n","  # 1. feed-forward\n","  pred = model(x_train)\n","  # 2. loss\n","  loss = loss_fn(pred, y_train) # loss_fn= torch.mean((pred - y_train)**2)\n","  # 3. grad\n","  opt.zero_grad()  #a.grad = None #b.grad = None 초기화\n","  loss.backward()\n","  # 4. update\n","  opt.step()\n","  print(f'step = {step}   loss = {loss.item()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuWiLBuHnsty","executionInfo":{"status":"ok","timestamp":1691454379821,"user_tz":-540,"elapsed":4079,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"0f447b8e-03b8-4524-fac5-d68ad9015eab"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["step = 0   loss = 24.801864624023438\n","step = 1   loss = 24.784317016601562\n","step = 2   loss = 24.766773223876953\n","step = 3   loss = 24.749238967895508\n","step = 4   loss = 24.731712341308594\n","step = 5   loss = 24.714191436767578\n","step = 6   loss = 24.696674346923828\n","step = 7   loss = 24.67917251586914\n","step = 8   loss = 24.66167640686035\n","step = 9   loss = 24.644189834594727\n","step = 10   loss = 24.6267032623291\n","step = 11   loss = 24.609237670898438\n","step = 12   loss = 24.591772079467773\n","step = 13   loss = 24.574316024780273\n","step = 14   loss = 24.556867599487305\n","step = 15   loss = 24.539430618286133\n","step = 16   loss = 24.522001266479492\n","step = 17   loss = 24.50458526611328\n","step = 18   loss = 24.487171173095703\n","step = 19   loss = 24.469770431518555\n","step = 20   loss = 24.452375411987305\n","step = 21   loss = 24.43499183654785\n","step = 22   loss = 24.417617797851562\n","step = 23   loss = 24.400249481201172\n","step = 24   loss = 24.382892608642578\n","step = 25   loss = 24.365550994873047\n","step = 26   loss = 24.348215103149414\n","step = 27   loss = 24.330886840820312\n","step = 28   loss = 24.313570022583008\n","step = 29   loss = 24.2962589263916\n","step = 30   loss = 24.278959274291992\n","step = 31   loss = 24.261674880981445\n","step = 32   loss = 24.244394302368164\n","step = 33   loss = 24.227127075195312\n","step = 34   loss = 24.20986557006836\n","step = 35   loss = 24.19261932373047\n","step = 36   loss = 24.175376892089844\n","step = 37   loss = 24.15814971923828\n","step = 38   loss = 24.14093017578125\n","step = 39   loss = 24.12372398376465\n","step = 40   loss = 24.106523513793945\n","step = 41   loss = 24.089336395263672\n","step = 42   loss = 24.072158813476562\n","step = 43   loss = 24.054990768432617\n","step = 44   loss = 24.03783416748047\n","step = 45   loss = 24.02068519592285\n","step = 46   loss = 24.00354766845703\n","step = 47   loss = 23.986421585083008\n","step = 48   loss = 23.96930503845215\n","step = 49   loss = 23.952198028564453\n","step = 50   loss = 23.935100555419922\n","step = 51   loss = 23.918012619018555\n","step = 52   loss = 23.900938034057617\n","step = 53   loss = 23.883872985839844\n","step = 54   loss = 23.8668155670166\n","step = 55   loss = 23.849769592285156\n","step = 56   loss = 23.832738876342773\n","step = 57   loss = 23.815710067749023\n","step = 58   loss = 23.798696517944336\n","step = 59   loss = 23.781692504882812\n","step = 60   loss = 23.764699935913086\n","step = 61   loss = 23.74771499633789\n","step = 62   loss = 23.730741500854492\n","step = 63   loss = 23.713777542114258\n","step = 64   loss = 23.696826934814453\n","step = 65   loss = 23.679882049560547\n","step = 66   loss = 23.662952423095703\n","step = 67   loss = 23.646028518676758\n","step = 68   loss = 23.62911605834961\n","step = 69   loss = 23.61221694946289\n","step = 70   loss = 23.595327377319336\n","step = 71   loss = 23.578447341918945\n","step = 72   loss = 23.56157684326172\n","step = 73   loss = 23.544715881347656\n","step = 74   loss = 23.527868270874023\n","step = 75   loss = 23.511030197143555\n","step = 76   loss = 23.49419593811035\n","step = 77   loss = 23.477380752563477\n","step = 78   loss = 23.4605712890625\n","step = 79   loss = 23.443777084350586\n","step = 80   loss = 23.426986694335938\n","step = 81   loss = 23.410207748413086\n","step = 82   loss = 23.39344024658203\n","step = 83   loss = 23.37668228149414\n","step = 84   loss = 23.359935760498047\n","step = 85   loss = 23.343198776245117\n","step = 86   loss = 23.32647132873535\n","step = 87   loss = 23.309755325317383\n","step = 88   loss = 23.293052673339844\n","step = 89   loss = 23.276355743408203\n","step = 90   loss = 23.25967025756836\n","step = 91   loss = 23.242992401123047\n","step = 92   loss = 23.226327896118164\n","step = 93   loss = 23.209672927856445\n","step = 94   loss = 23.19302749633789\n","step = 95   loss = 23.1763916015625\n","step = 96   loss = 23.159770965576172\n","step = 97   loss = 23.14315414428711\n","step = 98   loss = 23.126550674438477\n","step = 99   loss = 23.109956741333008\n","step = 100   loss = 23.093374252319336\n","step = 101   loss = 23.076797485351562\n","step = 102   loss = 23.06023406982422\n","step = 103   loss = 23.043678283691406\n","step = 104   loss = 23.027137756347656\n","step = 105   loss = 23.010608673095703\n","step = 106   loss = 22.99408531188965\n","step = 107   loss = 22.977571487426758\n","step = 108   loss = 22.96106719970703\n","step = 109   loss = 22.944578170776367\n","step = 110   loss = 22.92809295654297\n","step = 111   loss = 22.911623001098633\n","step = 112   loss = 22.895160675048828\n","step = 113   loss = 22.878711700439453\n","step = 114   loss = 22.862268447875977\n","step = 115   loss = 22.845834732055664\n","step = 116   loss = 22.82941436767578\n","step = 117   loss = 22.813003540039062\n","step = 118   loss = 22.796602249145508\n","step = 119   loss = 22.780210494995117\n","step = 120   loss = 22.76382827758789\n","step = 121   loss = 22.747455596923828\n","step = 122   loss = 22.731098175048828\n","step = 123   loss = 22.714746475219727\n","step = 124   loss = 22.698406219482422\n","step = 125   loss = 22.68207550048828\n","step = 126   loss = 22.665754318237305\n","step = 127   loss = 22.649442672729492\n","step = 128   loss = 22.63314437866211\n","step = 129   loss = 22.616853713989258\n","step = 130   loss = 22.600574493408203\n","step = 131   loss = 22.584300994873047\n","step = 132   loss = 22.568044662475586\n","step = 133   loss = 22.55179214477539\n","step = 134   loss = 22.535551071166992\n","step = 135   loss = 22.519319534301758\n","step = 136   loss = 22.503097534179688\n","step = 137   loss = 22.486888885498047\n","step = 138   loss = 22.470687866210938\n","step = 139   loss = 22.454496383666992\n","step = 140   loss = 22.438318252563477\n","step = 141   loss = 22.42214584350586\n","step = 142   loss = 22.405986785888672\n","step = 143   loss = 22.38983726501465\n","step = 144   loss = 22.373695373535156\n","step = 145   loss = 22.357566833496094\n","step = 146   loss = 22.341445922851562\n","step = 147   loss = 22.325336456298828\n","step = 148   loss = 22.309236526489258\n","step = 149   loss = 22.29314422607422\n","step = 150   loss = 22.277063369750977\n","step = 151   loss = 22.260995864868164\n","step = 152   loss = 22.24493408203125\n","step = 153   loss = 22.2288818359375\n","step = 154   loss = 22.212841033935547\n","step = 155   loss = 22.19681167602539\n","step = 156   loss = 22.1807918548584\n","step = 157   loss = 22.164779663085938\n","step = 158   loss = 22.14877700805664\n","step = 159   loss = 22.13278579711914\n","step = 160   loss = 22.116804122924805\n","step = 161   loss = 22.100831985473633\n","step = 162   loss = 22.084871292114258\n","step = 163   loss = 22.068918228149414\n","step = 164   loss = 22.052976608276367\n","step = 165   loss = 22.037046432495117\n","step = 166   loss = 22.0211238861084\n","step = 167   loss = 22.005210876464844\n","step = 168   loss = 21.989307403564453\n","step = 169   loss = 21.97341537475586\n","step = 170   loss = 21.957534790039062\n","step = 171   loss = 21.94165802001953\n","step = 172   loss = 21.925798416137695\n","step = 173   loss = 21.909944534301758\n","step = 174   loss = 21.894102096557617\n","step = 175   loss = 21.878267288208008\n","step = 176   loss = 21.862443923950195\n","step = 177   loss = 21.846630096435547\n","step = 178   loss = 21.830827713012695\n","step = 179   loss = 21.815034866333008\n","step = 180   loss = 21.79924964904785\n","step = 181   loss = 21.78347396850586\n","step = 182   loss = 21.767709732055664\n","step = 183   loss = 21.751955032348633\n","step = 184   loss = 21.73621368408203\n","step = 185   loss = 21.720476150512695\n","step = 186   loss = 21.704750061035156\n","step = 187   loss = 21.68903350830078\n","step = 188   loss = 21.673330307006836\n","step = 189   loss = 21.657630920410156\n","step = 190   loss = 21.641944885253906\n","step = 191   loss = 21.626270294189453\n","step = 192   loss = 21.6106014251709\n","step = 193   loss = 21.59494400024414\n","step = 194   loss = 21.579296112060547\n","step = 195   loss = 21.563661575317383\n","step = 196   loss = 21.548032760620117\n","step = 197   loss = 21.53241539001465\n","step = 198   loss = 21.516803741455078\n","step = 199   loss = 21.501205444335938\n","step = 200   loss = 21.485618591308594\n","step = 201   loss = 21.47003746032715\n","step = 202   loss = 21.4544677734375\n","step = 203   loss = 21.438905715942383\n","step = 204   loss = 21.423357009887695\n","step = 205   loss = 21.407817840576172\n","step = 206   loss = 21.392284393310547\n","step = 207   loss = 21.37676239013672\n","step = 208   loss = 21.361251831054688\n","step = 209   loss = 21.345752716064453\n","step = 210   loss = 21.33025550842285\n","step = 211   loss = 21.314777374267578\n","step = 212   loss = 21.299301147460938\n","step = 213   loss = 21.283842086791992\n","step = 214   loss = 21.268388748168945\n","step = 215   loss = 21.252946853637695\n","step = 216   loss = 21.237512588500977\n","step = 217   loss = 21.222087860107422\n","step = 218   loss = 21.20667266845703\n","step = 219   loss = 21.191267013549805\n","step = 220   loss = 21.175874710083008\n","step = 221   loss = 21.16048812866211\n","step = 222   loss = 21.145109176635742\n","step = 223   loss = 21.129745483398438\n","step = 224   loss = 21.11438751220703\n","step = 225   loss = 21.099042892456055\n","step = 226   loss = 21.083703994750977\n","step = 227   loss = 21.068376541137695\n","step = 228   loss = 21.053058624267578\n","step = 229   loss = 21.037748336791992\n","step = 230   loss = 21.022451400756836\n","step = 231   loss = 21.007160186767578\n","step = 232   loss = 20.991880416870117\n","step = 233   loss = 20.976608276367188\n","step = 234   loss = 20.961347579956055\n","step = 235   loss = 20.94609832763672\n","step = 236   loss = 20.930856704711914\n","step = 237   loss = 20.915624618530273\n","step = 238   loss = 20.900400161743164\n","step = 239   loss = 20.88518714904785\n","step = 240   loss = 20.869985580444336\n","step = 241   loss = 20.85478973388672\n","step = 242   loss = 20.83960723876953\n","step = 243   loss = 20.824430465698242\n","step = 244   loss = 20.80926513671875\n","step = 245   loss = 20.794111251831055\n","step = 246   loss = 20.778963088989258\n","step = 247   loss = 20.763826370239258\n","step = 248   loss = 20.748699188232422\n","step = 249   loss = 20.733583450317383\n","step = 250   loss = 20.71847152709961\n","step = 251   loss = 20.70337677001953\n","step = 252   loss = 20.688283920288086\n","step = 253   loss = 20.673206329345703\n","step = 254   loss = 20.65813446044922\n","step = 255   loss = 20.643075942993164\n","step = 256   loss = 20.628023147583008\n","step = 257   loss = 20.61298179626465\n","step = 258   loss = 20.597949981689453\n","step = 259   loss = 20.582923889160156\n","step = 260   loss = 20.567909240722656\n","step = 261   loss = 20.552907943725586\n","step = 262   loss = 20.537912368774414\n","step = 263   loss = 20.522926330566406\n","step = 264   loss = 20.507953643798828\n","step = 265   loss = 20.49298667907715\n","step = 266   loss = 20.478029251098633\n","step = 267   loss = 20.46308135986328\n","step = 268   loss = 20.448144912719727\n","step = 269   loss = 20.433216094970703\n","step = 270   loss = 20.418296813964844\n","step = 271   loss = 20.40338706970215\n","step = 272   loss = 20.388486862182617\n","step = 273   loss = 20.37359619140625\n","step = 274   loss = 20.358713150024414\n","step = 275   loss = 20.343843460083008\n","step = 276   loss = 20.328977584838867\n","step = 277   loss = 20.31412696838379\n","step = 278   loss = 20.299280166625977\n","step = 279   loss = 20.284446716308594\n","step = 280   loss = 20.269620895385742\n","step = 281   loss = 20.254806518554688\n","step = 282   loss = 20.240001678466797\n","step = 283   loss = 20.225204467773438\n","step = 284   loss = 20.210412979125977\n","step = 285   loss = 20.19563865661621\n","step = 286   loss = 20.18086814880371\n","step = 287   loss = 20.166109085083008\n","step = 288   loss = 20.15135955810547\n","step = 289   loss = 20.136619567871094\n","step = 290   loss = 20.121885299682617\n","step = 291   loss = 20.107162475585938\n","step = 292   loss = 20.092449188232422\n","step = 293   loss = 20.077747344970703\n","step = 294   loss = 20.063051223754883\n","step = 295   loss = 20.04836654663086\n","step = 296   loss = 20.03369140625\n","step = 297   loss = 20.019025802612305\n","step = 298   loss = 20.004365921020508\n","step = 299   loss = 19.989721298217773\n","step = 300   loss = 19.975080490112305\n","step = 301   loss = 19.960451126098633\n","step = 302   loss = 19.945833206176758\n","step = 303   loss = 19.931222915649414\n","step = 304   loss = 19.9166202545166\n","step = 305   loss = 19.90203094482422\n","step = 306   loss = 19.8874454498291\n","step = 307   loss = 19.87287139892578\n","step = 308   loss = 19.858308792114258\n","step = 309   loss = 19.843751907348633\n","step = 310   loss = 19.829204559326172\n","step = 311   loss = 19.814668655395508\n","step = 312   loss = 19.800140380859375\n","step = 313   loss = 19.785621643066406\n","step = 314   loss = 19.7711124420166\n","step = 315   loss = 19.75661277770996\n","step = 316   loss = 19.74212074279785\n","step = 317   loss = 19.727638244628906\n","step = 318   loss = 19.713167190551758\n","step = 319   loss = 19.698705673217773\n","step = 320   loss = 19.684249877929688\n","step = 321   loss = 19.6698055267334\n","step = 322   loss = 19.655370712280273\n","step = 323   loss = 19.640941619873047\n","step = 324   loss = 19.62652587890625\n","step = 325   loss = 19.612117767333984\n","step = 326   loss = 19.59771728515625\n","step = 327   loss = 19.583328247070312\n","step = 328   loss = 19.56894874572754\n","step = 329   loss = 19.554576873779297\n","step = 330   loss = 19.54021453857422\n","step = 331   loss = 19.525863647460938\n","step = 332   loss = 19.511520385742188\n","step = 333   loss = 19.49718475341797\n","step = 334   loss = 19.482858657836914\n","step = 335   loss = 19.468542098999023\n","step = 336   loss = 19.45423698425293\n","step = 337   loss = 19.4399356842041\n","step = 338   loss = 19.425647735595703\n","step = 339   loss = 19.41136932373047\n","step = 340   loss = 19.3971004486084\n","step = 341   loss = 19.382837295532227\n","step = 342   loss = 19.36858367919922\n","step = 343   loss = 19.35434341430664\n","step = 344   loss = 19.34010887145996\n","step = 345   loss = 19.325881958007812\n","step = 346   loss = 19.311668395996094\n","step = 347   loss = 19.297460556030273\n","step = 348   loss = 19.28326416015625\n","step = 349   loss = 19.269075393676758\n","step = 350   loss = 19.254894256591797\n","step = 351   loss = 19.240724563598633\n","step = 352   loss = 19.2265625\n","step = 353   loss = 19.21240997314453\n","step = 354   loss = 19.198265075683594\n","step = 355   loss = 19.184131622314453\n","step = 356   loss = 19.170005798339844\n","step = 357   loss = 19.1558895111084\n","step = 358   loss = 19.141782760620117\n","step = 359   loss = 19.1276798248291\n","step = 360   loss = 19.11359405517578\n","step = 361   loss = 19.09951400756836\n","step = 362   loss = 19.08544158935547\n","step = 363   loss = 19.071382522583008\n","step = 364   loss = 19.057327270507812\n","step = 365   loss = 19.043285369873047\n","step = 366   loss = 19.02924919128418\n","step = 367   loss = 19.015222549438477\n","step = 368   loss = 19.001205444335938\n","step = 369   loss = 18.987197875976562\n","step = 370   loss = 18.97319793701172\n","step = 371   loss = 18.959209442138672\n","step = 372   loss = 18.945228576660156\n","step = 373   loss = 18.931257247924805\n","step = 374   loss = 18.91729164123535\n","step = 375   loss = 18.903339385986328\n","step = 376   loss = 18.889392852783203\n","step = 377   loss = 18.875459671020508\n","step = 378   loss = 18.861530303955078\n","step = 379   loss = 18.847612380981445\n","step = 380   loss = 18.833703994750977\n","step = 381   loss = 18.81980323791504\n","step = 382   loss = 18.8059139251709\n","step = 383   loss = 18.792028427124023\n","step = 384   loss = 18.778156280517578\n","step = 385   loss = 18.76428985595703\n","step = 386   loss = 18.75043487548828\n","step = 387   loss = 18.736587524414062\n","step = 388   loss = 18.722747802734375\n","step = 389   loss = 18.708919525146484\n","step = 390   loss = 18.695100784301758\n","step = 391   loss = 18.681289672851562\n","step = 392   loss = 18.6674861907959\n","step = 393   loss = 18.65369415283203\n","step = 394   loss = 18.639909744262695\n","step = 395   loss = 18.626134872436523\n","step = 396   loss = 18.612367630004883\n","step = 397   loss = 18.598609924316406\n","step = 398   loss = 18.584861755371094\n","step = 399   loss = 18.571123123168945\n","step = 400   loss = 18.557390213012695\n","step = 401   loss = 18.543668746948242\n","step = 402   loss = 18.529956817626953\n","step = 403   loss = 18.516252517700195\n","step = 404   loss = 18.50255584716797\n","step = 405   loss = 18.488868713378906\n","step = 406   loss = 18.475191116333008\n","step = 407   loss = 18.461523056030273\n","step = 408   loss = 18.447860717773438\n","step = 409   loss = 18.4342098236084\n","step = 410   loss = 18.42056655883789\n","step = 411   loss = 18.406930923461914\n","step = 412   loss = 18.393306732177734\n","step = 413   loss = 18.37969207763672\n","step = 414   loss = 18.3660831451416\n","step = 415   loss = 18.35248565673828\n","step = 416   loss = 18.338895797729492\n","step = 417   loss = 18.325315475463867\n","step = 418   loss = 18.311742782592773\n","step = 419   loss = 18.298179626464844\n","step = 420   loss = 18.284626007080078\n","step = 421   loss = 18.271081924438477\n","step = 422   loss = 18.257543563842773\n","step = 423   loss = 18.244016647338867\n","step = 424   loss = 18.23049545288086\n","step = 425   loss = 18.21698570251465\n","step = 426   loss = 18.20348358154297\n","step = 427   loss = 18.189990997314453\n","step = 428   loss = 18.17650604248047\n","step = 429   loss = 18.163026809692383\n","step = 430   loss = 18.149560928344727\n","step = 431   loss = 18.136104583740234\n","step = 432   loss = 18.122655868530273\n","step = 433   loss = 18.109214782714844\n","step = 434   loss = 18.095783233642578\n","step = 435   loss = 18.082359313964844\n","step = 436   loss = 18.068944931030273\n","step = 437   loss = 18.055540084838867\n","step = 438   loss = 18.042142868041992\n","step = 439   loss = 18.02875328063965\n","step = 440   loss = 18.0153751373291\n","step = 441   loss = 18.002004623413086\n","step = 442   loss = 17.988643646240234\n","step = 443   loss = 17.97528839111328\n","step = 444   loss = 17.961942672729492\n","step = 445   loss = 17.9486083984375\n","step = 446   loss = 17.935279846191406\n","step = 447   loss = 17.921960830688477\n","step = 448   loss = 17.90865135192871\n","step = 449   loss = 17.895349502563477\n","step = 450   loss = 17.882055282592773\n","step = 451   loss = 17.868772506713867\n","step = 452   loss = 17.85549545288086\n","step = 453   loss = 17.84222984313965\n","step = 454   loss = 17.82897186279297\n","step = 455   loss = 17.815723419189453\n","step = 456   loss = 17.80248260498047\n","step = 457   loss = 17.78925132751465\n","step = 458   loss = 17.776025772094727\n","step = 459   loss = 17.7628116607666\n","step = 460   loss = 17.74960708618164\n","step = 461   loss = 17.736408233642578\n","step = 462   loss = 17.72321891784668\n","step = 463   loss = 17.710039138793945\n","step = 464   loss = 17.696866989135742\n","step = 465   loss = 17.683704376220703\n","step = 466   loss = 17.670549392700195\n","step = 467   loss = 17.65740394592285\n","step = 468   loss = 17.64426612854004\n","step = 469   loss = 17.631135940551758\n","step = 470   loss = 17.618017196655273\n","step = 471   loss = 17.60490608215332\n","step = 472   loss = 17.5918025970459\n","step = 473   loss = 17.57870864868164\n","step = 474   loss = 17.565624237060547\n","step = 475   loss = 17.55254554748535\n","step = 476   loss = 17.539478302001953\n","step = 477   loss = 17.526418685913086\n","step = 478   loss = 17.513364791870117\n","step = 479   loss = 17.500322341918945\n","step = 480   loss = 17.487287521362305\n","step = 481   loss = 17.474262237548828\n","step = 482   loss = 17.46124267578125\n","step = 483   loss = 17.44823455810547\n","step = 484   loss = 17.43523406982422\n","step = 485   loss = 17.422243118286133\n","step = 486   loss = 17.409259796142578\n","step = 487   loss = 17.396286010742188\n","step = 488   loss = 17.383319854736328\n","step = 489   loss = 17.370363235473633\n","step = 490   loss = 17.35741424560547\n","step = 491   loss = 17.344470977783203\n","step = 492   loss = 17.331539154052734\n","step = 493   loss = 17.318614959716797\n","step = 494   loss = 17.305700302124023\n","step = 495   loss = 17.29279136657715\n","step = 496   loss = 17.27989387512207\n","step = 497   loss = 17.267004013061523\n","step = 498   loss = 17.254125595092773\n","step = 499   loss = 17.24125099182129\n","step = 500   loss = 17.2283878326416\n","step = 501   loss = 17.215530395507812\n","step = 502   loss = 17.202686309814453\n","step = 503   loss = 17.189844131469727\n","step = 504   loss = 17.17701530456543\n","step = 505   loss = 17.16419219970703\n","step = 506   loss = 17.151378631591797\n","step = 507   loss = 17.138574600219727\n","step = 508   loss = 17.125776290893555\n","step = 509   loss = 17.112987518310547\n","step = 510   loss = 17.100208282470703\n","step = 511   loss = 17.087438583374023\n","step = 512   loss = 17.074674606323242\n","step = 513   loss = 17.061920166015625\n","step = 514   loss = 17.04917335510254\n","step = 515   loss = 17.036436080932617\n","step = 516   loss = 17.023706436157227\n","step = 517   loss = 17.010986328125\n","step = 518   loss = 16.998271942138672\n","step = 519   loss = 16.985570907592773\n","step = 520   loss = 16.972871780395508\n","step = 521   loss = 16.96018409729004\n","step = 522   loss = 16.947507858276367\n","step = 523   loss = 16.934837341308594\n","step = 524   loss = 16.92217254638672\n","step = 525   loss = 16.909521102905273\n","step = 526   loss = 16.896873474121094\n","step = 527   loss = 16.884239196777344\n","step = 528   loss = 16.87160873413086\n","step = 529   loss = 16.85898780822754\n","step = 530   loss = 16.846376419067383\n","step = 531   loss = 16.833772659301758\n","step = 532   loss = 16.821178436279297\n","step = 533   loss = 16.8085880279541\n","step = 534   loss = 16.796010971069336\n","step = 535   loss = 16.7834415435791\n","step = 536   loss = 16.770877838134766\n","step = 537   loss = 16.758323669433594\n","step = 538   loss = 16.745779037475586\n","step = 539   loss = 16.73324203491211\n","step = 540   loss = 16.720714569091797\n","step = 541   loss = 16.708192825317383\n","step = 542   loss = 16.695680618286133\n","step = 543   loss = 16.683176040649414\n","step = 544   loss = 16.67068099975586\n","step = 545   loss = 16.658193588256836\n","step = 546   loss = 16.645715713500977\n","step = 547   loss = 16.633243560791016\n","step = 548   loss = 16.62078094482422\n","step = 549   loss = 16.608327865600586\n","step = 550   loss = 16.595882415771484\n","step = 551   loss = 16.583444595336914\n","step = 552   loss = 16.571016311645508\n","step = 553   loss = 16.55859375\n","step = 554   loss = 16.546180725097656\n","step = 555   loss = 16.533777236938477\n","step = 556   loss = 16.521381378173828\n","step = 557   loss = 16.508995056152344\n","step = 558   loss = 16.496612548828125\n","step = 559   loss = 16.484241485595703\n","step = 560   loss = 16.471878051757812\n","step = 561   loss = 16.459522247314453\n","step = 562   loss = 16.447175979614258\n","step = 563   loss = 16.434837341308594\n","step = 564   loss = 16.42250633239746\n","step = 565   loss = 16.410184860229492\n","step = 566   loss = 16.397872924804688\n","step = 567   loss = 16.38556671142578\n","step = 568   loss = 16.373268127441406\n","step = 569   loss = 16.360979080200195\n","step = 570   loss = 16.34869956970215\n","step = 571   loss = 16.336423873901367\n","step = 572   loss = 16.324159622192383\n","step = 573   loss = 16.31190299987793\n","step = 574   loss = 16.29965591430664\n","step = 575   loss = 16.28741455078125\n","step = 576   loss = 16.275182723999023\n","step = 577   loss = 16.262958526611328\n","step = 578   loss = 16.250743865966797\n","step = 579   loss = 16.238536834716797\n","step = 580   loss = 16.226337432861328\n","step = 581   loss = 16.21414566040039\n","step = 582   loss = 16.201961517333984\n","step = 583   loss = 16.189786911010742\n","step = 584   loss = 16.17761993408203\n","step = 585   loss = 16.16546058654785\n","step = 586   loss = 16.153310775756836\n","step = 587   loss = 16.14116859436035\n","step = 588   loss = 16.12903594970703\n","step = 589   loss = 16.11690902709961\n","step = 590   loss = 16.10478973388672\n","step = 591   loss = 16.092679977416992\n","step = 592   loss = 16.080577850341797\n","step = 593   loss = 16.068483352661133\n","step = 594   loss = 16.056398391723633\n","step = 595   loss = 16.044322967529297\n","step = 596   loss = 16.032251358032227\n","step = 597   loss = 16.020191192626953\n","step = 598   loss = 16.008140563964844\n","step = 599   loss = 15.99609375\n","step = 600   loss = 15.984057426452637\n","step = 601   loss = 15.972028732299805\n","step = 602   loss = 15.960005760192871\n","step = 603   loss = 15.947993278503418\n","step = 604   loss = 15.935988426208496\n","step = 605   loss = 15.923993110656738\n","step = 606   loss = 15.912003517150879\n","step = 607   loss = 15.9000244140625\n","step = 608   loss = 15.888049125671387\n","step = 609   loss = 15.876087188720703\n","step = 610   loss = 15.864130973815918\n","step = 611   loss = 15.852182388305664\n","step = 612   loss = 15.840243339538574\n","step = 613   loss = 15.828309059143066\n","step = 614   loss = 15.816387176513672\n","step = 615   loss = 15.804468154907227\n","step = 616   loss = 15.792560577392578\n","step = 617   loss = 15.780661582946777\n","step = 618   loss = 15.768769264221191\n","step = 619   loss = 15.756884574890137\n","step = 620   loss = 15.745007514953613\n","step = 621   loss = 15.73314094543457\n","step = 622   loss = 15.721280097961426\n","step = 623   loss = 15.709428787231445\n","step = 624   loss = 15.69758415222168\n","step = 625   loss = 15.685748100280762\n","step = 626   loss = 15.673919677734375\n","step = 627   loss = 15.662099838256836\n","step = 628   loss = 15.650287628173828\n","step = 629   loss = 15.638484001159668\n","step = 630   loss = 15.626689910888672\n","step = 631   loss = 15.614899635314941\n","step = 632   loss = 15.603118896484375\n","step = 633   loss = 15.591347694396973\n","step = 634   loss = 15.579584121704102\n","step = 635   loss = 15.567827224731445\n","step = 636   loss = 15.55607795715332\n","step = 637   loss = 15.544337272644043\n","step = 638   loss = 15.53260612487793\n","step = 639   loss = 15.520879745483398\n","step = 640   loss = 15.509163856506348\n","step = 641   loss = 15.497457504272461\n","step = 642   loss = 15.485754013061523\n","step = 643   loss = 15.474063873291016\n","step = 644   loss = 15.462376594543457\n","step = 645   loss = 15.450699806213379\n","step = 646   loss = 15.439030647277832\n","step = 647   loss = 15.427369117736816\n","step = 648   loss = 15.415715217590332\n","step = 649   loss = 15.404069900512695\n","step = 650   loss = 15.392433166503906\n","step = 651   loss = 15.380805969238281\n","step = 652   loss = 15.369183540344238\n","step = 653   loss = 15.357568740844727\n","step = 654   loss = 15.345963478088379\n","step = 655   loss = 15.33436393737793\n","step = 656   loss = 15.322772979736328\n","step = 657   loss = 15.311192512512207\n","step = 658   loss = 15.299617767333984\n","step = 659   loss = 15.288050651550293\n","step = 660   loss = 15.276493072509766\n","step = 661   loss = 15.26494026184082\n","step = 662   loss = 15.253398895263672\n","step = 663   loss = 15.241864204406738\n","step = 664   loss = 15.230335235595703\n","step = 665   loss = 15.218816757202148\n","step = 666   loss = 15.207303047180176\n","step = 667   loss = 15.19580078125\n","step = 668   loss = 15.184304237365723\n","step = 669   loss = 15.17281723022461\n","step = 670   loss = 15.161334991455078\n","step = 671   loss = 15.149864196777344\n","step = 672   loss = 15.138400077819824\n","step = 673   loss = 15.126943588256836\n","step = 674   loss = 15.11549186706543\n","step = 675   loss = 15.10405158996582\n","step = 676   loss = 15.092617988586426\n","step = 677   loss = 15.081192970275879\n","step = 678   loss = 15.069775581359863\n","step = 679   loss = 15.058363914489746\n","step = 680   loss = 15.046961784362793\n","step = 681   loss = 15.035566329956055\n","step = 682   loss = 15.024182319641113\n","step = 683   loss = 15.012803077697754\n","step = 684   loss = 15.00143051147461\n","step = 685   loss = 14.990067481994629\n","step = 686   loss = 14.97871208190918\n","step = 687   loss = 14.967364311218262\n","step = 688   loss = 14.956023216247559\n","step = 689   loss = 14.944690704345703\n","step = 690   loss = 14.933367729187012\n","step = 691   loss = 14.922050476074219\n","step = 692   loss = 14.910741806030273\n","step = 693   loss = 14.899438858032227\n","step = 694   loss = 14.888147354125977\n","step = 695   loss = 14.87685775756836\n","step = 696   loss = 14.865582466125488\n","step = 697   loss = 14.8543119430542\n","step = 698   loss = 14.843049049377441\n","step = 699   loss = 14.831792831420898\n","step = 700   loss = 14.820547103881836\n","step = 701   loss = 14.809308052062988\n","step = 702   loss = 14.798074722290039\n","step = 703   loss = 14.786850929260254\n","step = 704   loss = 14.775634765625\n","step = 705   loss = 14.764426231384277\n","step = 706   loss = 14.753225326538086\n","step = 707   loss = 14.74203109741211\n","step = 708   loss = 14.730844497680664\n","step = 709   loss = 14.719666481018066\n","step = 710   loss = 14.70849609375\n","step = 711   loss = 14.697335243225098\n","step = 712   loss = 14.686179161071777\n","step = 713   loss = 14.675031661987305\n","step = 714   loss = 14.663891792297363\n","step = 715   loss = 14.652759552001953\n","step = 716   loss = 14.64163589477539\n","step = 717   loss = 14.630517959594727\n","step = 718   loss = 14.619409561157227\n","step = 719   loss = 14.608307838439941\n","step = 720   loss = 14.597214698791504\n","step = 721   loss = 14.586128234863281\n","step = 722   loss = 14.575050354003906\n","step = 723   loss = 14.563979148864746\n","step = 724   loss = 14.552913665771484\n","step = 725   loss = 14.541858673095703\n","step = 726   loss = 14.530810356140137\n","step = 727   loss = 14.519769668579102\n","step = 728   loss = 14.508737564086914\n","step = 729   loss = 14.497712135314941\n","step = 730   loss = 14.486695289611816\n","step = 731   loss = 14.475685119628906\n","step = 732   loss = 14.464683532714844\n","step = 733   loss = 14.45368766784668\n","step = 734   loss = 14.44270133972168\n","step = 735   loss = 14.431720733642578\n","step = 736   loss = 14.42074966430664\n","step = 737   loss = 14.409784317016602\n","step = 738   loss = 14.398826599121094\n","step = 739   loss = 14.38787841796875\n","step = 740   loss = 14.376936912536621\n","step = 741   loss = 14.36600112915039\n","step = 742   loss = 14.35507583618164\n","step = 743   loss = 14.344155311584473\n","step = 744   loss = 14.333244323730469\n","step = 745   loss = 14.322340965270996\n","step = 746   loss = 14.311445236206055\n","step = 747   loss = 14.300556182861328\n","step = 748   loss = 14.289673805236816\n","step = 749   loss = 14.278800964355469\n","step = 750   loss = 14.267932891845703\n","step = 751   loss = 14.257075309753418\n","step = 752   loss = 14.246223449707031\n","step = 753   loss = 14.235381126403809\n","step = 754   loss = 14.224543571472168\n","step = 755   loss = 14.213715553283691\n","step = 756   loss = 14.20289421081543\n","step = 757   loss = 14.192081451416016\n","step = 758   loss = 14.1812744140625\n","step = 759   loss = 14.170475959777832\n","step = 760   loss = 14.159685134887695\n","step = 761   loss = 14.148900032043457\n","step = 762   loss = 14.138123512268066\n","step = 763   loss = 14.127355575561523\n","step = 764   loss = 14.116595268249512\n","step = 765   loss = 14.105840682983398\n","step = 766   loss = 14.09509563446045\n","step = 767   loss = 14.084355354309082\n","step = 768   loss = 14.073625564575195\n","step = 769   loss = 14.06290054321289\n","step = 770   loss = 14.0521821975708\n","step = 771   loss = 14.041474342346191\n","step = 772   loss = 14.030773162841797\n","step = 773   loss = 14.020079612731934\n","step = 774   loss = 14.009393692016602\n","step = 775   loss = 13.998714447021484\n","step = 776   loss = 13.988041877746582\n","step = 777   loss = 13.977377891540527\n","step = 778   loss = 13.966719627380371\n","step = 779   loss = 13.956071853637695\n","step = 780   loss = 13.945429801940918\n","step = 781   loss = 13.934793472290039\n","step = 782   loss = 13.92416763305664\n","step = 783   loss = 13.91354751586914\n","step = 784   loss = 13.902937889099121\n","step = 785   loss = 13.892330169677734\n","step = 786   loss = 13.881732940673828\n","step = 787   loss = 13.871142387390137\n","step = 788   loss = 13.860559463500977\n","step = 789   loss = 13.849985122680664\n","step = 790   loss = 13.83941650390625\n","step = 791   loss = 13.828857421875\n","step = 792   loss = 13.818303108215332\n","step = 793   loss = 13.807758331298828\n","step = 794   loss = 13.797219276428223\n","step = 795   loss = 13.786686897277832\n","step = 796   loss = 13.776165008544922\n","step = 797   loss = 13.765647888183594\n","step = 798   loss = 13.755139350891113\n","step = 799   loss = 13.744637489318848\n","step = 800   loss = 13.734145164489746\n","step = 801   loss = 13.723657608032227\n","step = 802   loss = 13.713177680969238\n","step = 803   loss = 13.702706336975098\n","step = 804   loss = 13.692241668701172\n","step = 805   loss = 13.681783676147461\n","step = 806   loss = 13.671333312988281\n","step = 807   loss = 13.66089153289795\n","step = 808   loss = 13.6504545211792\n","step = 809   loss = 13.640026092529297\n","step = 810   loss = 13.629607200622559\n","step = 811   loss = 13.619192123413086\n","step = 812   loss = 13.608785629272461\n","step = 813   loss = 13.59838581085205\n","step = 814   loss = 13.587995529174805\n","step = 815   loss = 13.577610969543457\n","step = 816   loss = 13.567234992980957\n","step = 817   loss = 13.556863784790039\n","step = 818   loss = 13.546501159667969\n","step = 819   loss = 13.536147117614746\n","step = 820   loss = 13.525799751281738\n","step = 821   loss = 13.515458106994629\n","step = 822   loss = 13.505125999450684\n","step = 823   loss = 13.494799613952637\n","step = 824   loss = 13.484479904174805\n","step = 825   loss = 13.474169731140137\n","step = 826   loss = 13.463866233825684\n","step = 827   loss = 13.453566551208496\n","step = 828   loss = 13.443279266357422\n","step = 829   loss = 13.432995796203613\n","step = 830   loss = 13.422719955444336\n","step = 831   loss = 13.412453651428223\n","step = 832   loss = 13.402191162109375\n","step = 833   loss = 13.391938209533691\n","step = 834   loss = 13.381691932678223\n","step = 835   loss = 13.371454238891602\n","step = 836   loss = 13.361222267150879\n","step = 837   loss = 13.350996971130371\n","step = 838   loss = 13.340780258178711\n","step = 839   loss = 13.330570220947266\n","step = 840   loss = 13.320368766784668\n","step = 841   loss = 13.310172080993652\n","step = 842   loss = 13.299983978271484\n","step = 843   loss = 13.289800643920898\n","step = 844   loss = 13.27962875366211\n","step = 845   loss = 13.269460678100586\n","step = 846   loss = 13.259302139282227\n","step = 847   loss = 13.249149322509766\n","step = 848   loss = 13.239004135131836\n","step = 849   loss = 13.228865623474121\n","step = 850   loss = 13.218735694885254\n","step = 851   loss = 13.208613395690918\n","step = 852   loss = 13.198494911193848\n","step = 853   loss = 13.188385009765625\n","step = 854   loss = 13.17828369140625\n","step = 855   loss = 13.168187141418457\n","step = 856   loss = 13.158100128173828\n","step = 857   loss = 13.148021697998047\n","step = 858   loss = 13.137947082519531\n","step = 859   loss = 13.127881050109863\n","step = 860   loss = 13.117820739746094\n","step = 861   loss = 13.107769012451172\n","step = 862   loss = 13.097724914550781\n","step = 863   loss = 13.087686538696289\n","step = 864   loss = 13.077655792236328\n","step = 865   loss = 13.067634582519531\n","step = 866   loss = 13.05761432647705\n","step = 867   loss = 13.04760456085205\n","step = 868   loss = 13.037605285644531\n","step = 869   loss = 13.027606964111328\n","step = 870   loss = 13.017620086669922\n","step = 871   loss = 13.007638931274414\n","step = 872   loss = 12.997666358947754\n","step = 873   loss = 12.987700462341309\n","step = 874   loss = 12.977739334106445\n","step = 875   loss = 12.96778678894043\n","step = 876   loss = 12.957840919494629\n","step = 877   loss = 12.947903633117676\n","step = 878   loss = 12.937972068786621\n","step = 879   loss = 12.928047180175781\n","step = 880   loss = 12.918132781982422\n","step = 881   loss = 12.908221244812012\n","step = 882   loss = 12.89831829071045\n","step = 883   loss = 12.888422012329102\n","step = 884   loss = 12.878534317016602\n","step = 885   loss = 12.86865234375\n","step = 886   loss = 12.858777046203613\n","step = 887   loss = 12.848908424377441\n","step = 888   loss = 12.83905029296875\n","step = 889   loss = 12.82919692993164\n","step = 890   loss = 12.819348335266113\n","step = 891   loss = 12.80950927734375\n","step = 892   loss = 12.799676895141602\n","step = 893   loss = 12.789852142333984\n","step = 894   loss = 12.780034065246582\n","step = 895   loss = 12.770221710205078\n","step = 896   loss = 12.760417938232422\n","step = 897   loss = 12.750621795654297\n","step = 898   loss = 12.74083137512207\n","step = 899   loss = 12.731048583984375\n","step = 900   loss = 12.721271514892578\n","step = 901   loss = 12.711501121520996\n","step = 902   loss = 12.701739311218262\n","step = 903   loss = 12.691983222961426\n","step = 904   loss = 12.682236671447754\n","step = 905   loss = 12.672494888305664\n","step = 906   loss = 12.662759780883789\n","step = 907   loss = 12.653034210205078\n","step = 908   loss = 12.64331340789795\n","step = 909   loss = 12.633600234985352\n","step = 910   loss = 12.623895645141602\n","step = 911   loss = 12.6141939163208\n","step = 912   loss = 12.604501724243164\n","step = 913   loss = 12.594817161560059\n","step = 914   loss = 12.585139274597168\n","step = 915   loss = 12.575467109680176\n","step = 916   loss = 12.565803527832031\n","step = 917   loss = 12.556144714355469\n","step = 918   loss = 12.546494483947754\n","step = 919   loss = 12.536850929260254\n","step = 920   loss = 12.527214050292969\n","step = 921   loss = 12.517583847045898\n","step = 922   loss = 12.50796127319336\n","step = 923   loss = 12.498347282409668\n","step = 924   loss = 12.488736152648926\n","step = 925   loss = 12.479134559631348\n","step = 926   loss = 12.469539642333984\n","step = 927   loss = 12.459951400756836\n","step = 928   loss = 12.450369834899902\n","step = 929   loss = 12.440794944763184\n","step = 930   loss = 12.431227684020996\n","step = 931   loss = 12.421667098999023\n","step = 932   loss = 12.412114143371582\n","step = 933   loss = 12.402566909790039\n","step = 934   loss = 12.393027305603027\n","step = 935   loss = 12.383495330810547\n","step = 936   loss = 12.373967170715332\n","step = 937   loss = 12.364448547363281\n","step = 938   loss = 12.354935646057129\n","step = 939   loss = 12.345431327819824\n","step = 940   loss = 12.335933685302734\n","step = 941   loss = 12.326440811157227\n","step = 942   loss = 12.316956520080566\n","step = 943   loss = 12.307477951049805\n","step = 944   loss = 12.298007011413574\n","step = 945   loss = 12.288542747497559\n","step = 946   loss = 12.279084205627441\n","step = 947   loss = 12.269635200500488\n","step = 948   loss = 12.260191917419434\n","step = 949   loss = 12.250754356384277\n","step = 950   loss = 12.241324424743652\n","step = 951   loss = 12.231902122497559\n","step = 952   loss = 12.222485542297363\n","step = 953   loss = 12.213074684143066\n","step = 954   loss = 12.203673362731934\n","step = 955   loss = 12.1942777633667\n","step = 956   loss = 12.184887886047363\n","step = 957   loss = 12.175506591796875\n","step = 958   loss = 12.166129112243652\n","step = 959   loss = 12.156761169433594\n","step = 960   loss = 12.147398948669434\n","step = 961   loss = 12.138044357299805\n","step = 962   loss = 12.12869644165039\n","step = 963   loss = 12.119355201721191\n","step = 964   loss = 12.110020637512207\n","step = 965   loss = 12.100693702697754\n","step = 966   loss = 12.091373443603516\n","step = 967   loss = 12.08205795288086\n","step = 968   loss = 12.072750091552734\n","step = 969   loss = 12.06344985961914\n","step = 970   loss = 12.054156303405762\n","step = 971   loss = 12.044869422912598\n","step = 972   loss = 12.035589218139648\n","step = 973   loss = 12.026315689086914\n","step = 974   loss = 12.017047882080078\n","step = 975   loss = 12.007790565490723\n","step = 976   loss = 11.99853515625\n","step = 977   loss = 11.989289283752441\n","step = 978   loss = 11.980049133300781\n","step = 979   loss = 11.970815658569336\n","step = 980   loss = 11.961589813232422\n","step = 981   loss = 11.952369689941406\n","step = 982   loss = 11.943158149719238\n","step = 983   loss = 11.933952331542969\n","step = 984   loss = 11.924751281738281\n","step = 985   loss = 11.915557861328125\n","step = 986   loss = 11.906373023986816\n","step = 987   loss = 11.897193908691406\n","step = 988   loss = 11.888021469116211\n","step = 989   loss = 11.878857612609863\n","step = 990   loss = 11.869695663452148\n","step = 991   loss = 11.860546112060547\n","step = 992   loss = 11.851398468017578\n","step = 993   loss = 11.842259407043457\n","step = 994   loss = 11.83312702178955\n","step = 995   loss = 11.82400131225586\n","step = 996   loss = 11.814881324768066\n","step = 997   loss = 11.805769920349121\n","step = 998   loss = 11.79666519165039\n","step = 999   loss = 11.787567138671875\n","step = 1000   loss = 11.778475761413574\n","step = 1001   loss = 11.769390106201172\n","step = 1002   loss = 11.760310173034668\n","step = 1003   loss = 11.751236915588379\n","step = 1004   loss = 11.742173194885254\n","step = 1005   loss = 11.733114242553711\n","step = 1006   loss = 11.724061012268066\n","step = 1007   loss = 11.715015411376953\n","step = 1008   loss = 11.705977439880371\n","step = 1009   loss = 11.696944236755371\n","step = 1010   loss = 11.687918663024902\n","step = 1011   loss = 11.678901672363281\n","step = 1012   loss = 11.66988754272461\n","step = 1013   loss = 11.660882949829102\n","step = 1014   loss = 11.651883125305176\n","step = 1015   loss = 11.642890930175781\n","step = 1016   loss = 11.633906364440918\n","step = 1017   loss = 11.624926567077637\n","step = 1018   loss = 11.615954399108887\n","step = 1019   loss = 11.606988906860352\n","step = 1020   loss = 11.598028182983398\n","step = 1021   loss = 11.58907699584961\n","step = 1022   loss = 11.580129623413086\n","step = 1023   loss = 11.571188926696777\n","step = 1024   loss = 11.56225872039795\n","step = 1025   loss = 11.55333137512207\n","step = 1026   loss = 11.544412612915039\n","step = 1027   loss = 11.535497665405273\n","step = 1028   loss = 11.526592254638672\n","step = 1029   loss = 11.517692565917969\n","step = 1030   loss = 11.508798599243164\n","step = 1031   loss = 11.49991226196289\n","step = 1032   loss = 11.491031646728516\n","step = 1033   loss = 11.482159614562988\n","step = 1034   loss = 11.473292350769043\n","step = 1035   loss = 11.464430809020996\n","step = 1036   loss = 11.455575942993164\n","step = 1037   loss = 11.446727752685547\n","step = 1038   loss = 11.437888145446777\n","step = 1039   loss = 11.429054260253906\n","step = 1040   loss = 11.420226097106934\n","step = 1041   loss = 11.411404609680176\n","step = 1042   loss = 11.40259075164795\n","step = 1043   loss = 11.393782615661621\n","step = 1044   loss = 11.384979248046875\n","step = 1045   loss = 11.376185417175293\n","step = 1046   loss = 11.367396354675293\n","step = 1047   loss = 11.358613014221191\n","step = 1048   loss = 11.34984016418457\n","step = 1049   loss = 11.341070175170898\n","step = 1050   loss = 11.332308769226074\n","step = 1051   loss = 11.323552131652832\n","step = 1052   loss = 11.314802169799805\n","step = 1053   loss = 11.306059837341309\n","step = 1054   loss = 11.297321319580078\n","step = 1055   loss = 11.288592338562012\n","step = 1056   loss = 11.279869079589844\n","step = 1057   loss = 11.27115249633789\n","step = 1058   loss = 11.262441635131836\n","step = 1059   loss = 11.253737449645996\n","step = 1060   loss = 11.245039939880371\n","step = 1061   loss = 11.236350059509277\n","step = 1062   loss = 11.227664947509766\n","step = 1063   loss = 11.218986511230469\n","step = 1064   loss = 11.210314750671387\n","step = 1065   loss = 11.201650619506836\n","step = 1066   loss = 11.192992210388184\n","step = 1067   loss = 11.184338569641113\n","step = 1068   loss = 11.17569351196289\n","step = 1069   loss = 11.167054176330566\n","step = 1070   loss = 11.15842056274414\n","step = 1071   loss = 11.149794578552246\n","step = 1072   loss = 11.14117431640625\n","step = 1073   loss = 11.132560729980469\n","step = 1074   loss = 11.123953819274902\n","step = 1075   loss = 11.11535358428955\n","step = 1076   loss = 11.106759071350098\n","step = 1077   loss = 11.09817123413086\n","step = 1078   loss = 11.089590072631836\n","step = 1079   loss = 11.081014633178711\n","step = 1080   loss = 11.0724458694458\n","step = 1081   loss = 11.063885688781738\n","step = 1082   loss = 11.055328369140625\n","step = 1083   loss = 11.046778678894043\n","step = 1084   loss = 11.038235664367676\n","step = 1085   loss = 11.029699325561523\n","step = 1086   loss = 11.021169662475586\n","step = 1087   loss = 11.012646675109863\n","step = 1088   loss = 11.004128456115723\n","step = 1089   loss = 10.995617866516113\n","step = 1090   loss = 10.987112045288086\n","step = 1091   loss = 10.978612899780273\n","step = 1092   loss = 10.970123291015625\n","step = 1093   loss = 10.961636543273926\n","step = 1094   loss = 10.953158378601074\n","step = 1095   loss = 10.944685935974121\n","step = 1096   loss = 10.936219215393066\n","step = 1097   loss = 10.927759170532227\n","step = 1098   loss = 10.919305801391602\n","step = 1099   loss = 10.910858154296875\n","step = 1100   loss = 10.902417182922363\n","step = 1101   loss = 10.893982887268066\n","step = 1102   loss = 10.885554313659668\n","step = 1103   loss = 10.877132415771484\n","step = 1104   loss = 10.868717193603516\n","step = 1105   loss = 10.860306739807129\n","step = 1106   loss = 10.851903915405273\n","step = 1107   loss = 10.84350872039795\n","step = 1108   loss = 10.835118293762207\n","step = 1109   loss = 10.826733589172363\n","step = 1110   loss = 10.81835651397705\n","step = 1111   loss = 10.809986114501953\n","step = 1112   loss = 10.801621437072754\n","step = 1113   loss = 10.793261528015137\n","step = 1114   loss = 10.78490924835205\n","step = 1115   loss = 10.77656364440918\n","step = 1116   loss = 10.768223762512207\n","step = 1117   loss = 10.75989055633545\n","step = 1118   loss = 10.751564025878906\n","step = 1119   loss = 10.743243217468262\n","step = 1120   loss = 10.734928131103516\n","step = 1121   loss = 10.726619720458984\n","step = 1122   loss = 10.718317985534668\n","step = 1123   loss = 10.710022926330566\n","step = 1124   loss = 10.70173454284668\n","step = 1125   loss = 10.693449974060059\n","step = 1126   loss = 10.685173034667969\n","step = 1127   loss = 10.676901817321777\n","step = 1128   loss = 10.668639183044434\n","step = 1129   loss = 10.660379409790039\n","step = 1130   loss = 10.652129173278809\n","step = 1131   loss = 10.643882751464844\n","step = 1132   loss = 10.635643005371094\n","step = 1133   loss = 10.627411842346191\n","step = 1134   loss = 10.619184494018555\n","step = 1135   loss = 10.61096477508545\n","step = 1136   loss = 10.602749824523926\n","step = 1137   loss = 10.594542503356934\n","step = 1138   loss = 10.586339950561523\n","step = 1139   loss = 10.578143119812012\n","step = 1140   loss = 10.569954872131348\n","step = 1141   loss = 10.561772346496582\n","step = 1142   loss = 10.553593635559082\n","step = 1143   loss = 10.545424461364746\n","step = 1144   loss = 10.53725814819336\n","step = 1145   loss = 10.529101371765137\n","step = 1146   loss = 10.520949363708496\n","step = 1147   loss = 10.512803077697754\n","step = 1148   loss = 10.504663467407227\n","step = 1149   loss = 10.496529579162598\n","step = 1150   loss = 10.488402366638184\n","step = 1151   loss = 10.480280876159668\n","step = 1152   loss = 10.472167015075684\n","step = 1153   loss = 10.464057922363281\n","step = 1154   loss = 10.455954551696777\n","step = 1155   loss = 10.447857856750488\n","step = 1156   loss = 10.439769744873047\n","step = 1157   loss = 10.431684494018555\n","step = 1158   loss = 10.423606872558594\n","step = 1159   loss = 10.415534973144531\n","step = 1160   loss = 10.407470703125\n","step = 1161   loss = 10.399410247802734\n","step = 1162   loss = 10.391357421875\n","step = 1163   loss = 10.383309364318848\n","step = 1164   loss = 10.375268936157227\n","step = 1165   loss = 10.36723518371582\n","step = 1166   loss = 10.359206199645996\n","step = 1167   loss = 10.35118293762207\n","step = 1168   loss = 10.343167304992676\n","step = 1169   loss = 10.33515739440918\n","step = 1170   loss = 10.327153205871582\n","step = 1171   loss = 10.3191556930542\n","step = 1172   loss = 10.311164855957031\n","step = 1173   loss = 10.303177833557129\n","step = 1174   loss = 10.295199394226074\n","step = 1175   loss = 10.287223815917969\n","step = 1176   loss = 10.279257774353027\n","step = 1177   loss = 10.271296501159668\n","step = 1178   loss = 10.263340950012207\n","step = 1179   loss = 10.255392074584961\n","step = 1180   loss = 10.247448921203613\n","step = 1181   loss = 10.23951244354248\n","step = 1182   loss = 10.231581687927246\n","step = 1183   loss = 10.223657608032227\n","step = 1184   loss = 10.215738296508789\n","step = 1185   loss = 10.207825660705566\n","step = 1186   loss = 10.199919700622559\n","step = 1187   loss = 10.19201946258545\n","step = 1188   loss = 10.184124946594238\n","step = 1189   loss = 10.176238059997559\n","step = 1190   loss = 10.168354988098145\n","step = 1191   loss = 10.160479545593262\n","step = 1192   loss = 10.152610778808594\n","step = 1193   loss = 10.144745826721191\n","step = 1194   loss = 10.13688850402832\n","step = 1195   loss = 10.129035949707031\n","step = 1196   loss = 10.121191024780273\n","step = 1197   loss = 10.113351821899414\n","step = 1198   loss = 10.105517387390137\n","step = 1199   loss = 10.09769058227539\n","step = 1200   loss = 10.089869499206543\n","step = 1201   loss = 10.082052230834961\n","step = 1202   loss = 10.074243545532227\n","step = 1203   loss = 10.06644058227539\n","step = 1204   loss = 10.058642387390137\n","step = 1205   loss = 10.050851821899414\n","step = 1206   loss = 10.043066024780273\n","step = 1207   loss = 10.035287857055664\n","step = 1208   loss = 10.027514457702637\n","step = 1209   loss = 10.019747734069824\n","step = 1210   loss = 10.011985778808594\n","step = 1211   loss = 10.004230499267578\n","step = 1212   loss = 9.996480941772461\n","step = 1213   loss = 9.988738059997559\n","step = 1214   loss = 9.981000900268555\n","step = 1215   loss = 9.97326946258545\n","step = 1216   loss = 9.965545654296875\n","step = 1217   loss = 9.957825660705566\n","step = 1218   loss = 9.950111389160156\n","step = 1219   loss = 9.942403793334961\n","step = 1220   loss = 9.93470287322998\n","step = 1221   loss = 9.927007675170898\n","step = 1222   loss = 9.919319152832031\n","step = 1223   loss = 9.911635398864746\n","step = 1224   loss = 9.903959274291992\n","step = 1225   loss = 9.89628791809082\n","step = 1226   loss = 9.88862133026123\n","step = 1227   loss = 9.880962371826172\n","step = 1228   loss = 9.873308181762695\n","step = 1229   loss = 9.865659713745117\n","step = 1230   loss = 9.85801887512207\n","step = 1231   loss = 9.850383758544922\n","step = 1232   loss = 9.842754364013672\n","step = 1233   loss = 9.835129737854004\n","step = 1234   loss = 9.82751178741455\n","step = 1235   loss = 9.819899559020996\n","step = 1236   loss = 9.812294006347656\n","step = 1237   loss = 9.804693222045898\n","step = 1238   loss = 9.797100067138672\n","step = 1239   loss = 9.789510726928711\n","step = 1240   loss = 9.781929016113281\n","step = 1241   loss = 9.77435302734375\n","step = 1242   loss = 9.7667818069458\n","step = 1243   loss = 9.759217262268066\n","step = 1244   loss = 9.751657485961914\n","step = 1245   loss = 9.744105339050293\n","step = 1246   loss = 9.73655891418457\n","step = 1247   loss = 9.72901725769043\n","step = 1248   loss = 9.72148323059082\n","step = 1249   loss = 9.713953018188477\n","step = 1250   loss = 9.706428527832031\n","step = 1251   loss = 9.698912620544434\n","step = 1252   loss = 9.691400527954102\n","step = 1253   loss = 9.6838960647583\n","step = 1254   loss = 9.676395416259766\n","step = 1255   loss = 9.668902397155762\n","step = 1256   loss = 9.661412239074707\n","step = 1257   loss = 9.6539306640625\n","step = 1258   loss = 9.646454811096191\n","step = 1259   loss = 9.638984680175781\n","step = 1260   loss = 9.631519317626953\n","step = 1261   loss = 9.624061584472656\n","step = 1262   loss = 9.616606712341309\n","step = 1263   loss = 9.609160423278809\n","step = 1264   loss = 9.60171890258789\n","step = 1265   loss = 9.594284057617188\n","step = 1266   loss = 9.586854934692383\n","step = 1267   loss = 9.579431533813477\n","step = 1268   loss = 9.572012901306152\n","step = 1269   loss = 9.56460189819336\n","step = 1270   loss = 9.557193756103516\n","step = 1271   loss = 9.549793243408203\n","step = 1272   loss = 9.542400360107422\n","step = 1273   loss = 9.535011291503906\n","step = 1274   loss = 9.527627944946289\n","step = 1275   loss = 9.52025032043457\n","step = 1276   loss = 9.512879371643066\n","step = 1277   loss = 9.505514144897461\n","step = 1278   loss = 9.498154640197754\n","step = 1279   loss = 9.490799903869629\n","step = 1280   loss = 9.483451843261719\n","step = 1281   loss = 9.47610855102539\n","step = 1282   loss = 9.468772888183594\n","step = 1283   loss = 9.461442947387695\n","step = 1284   loss = 9.454117774963379\n","step = 1285   loss = 9.446798324584961\n","step = 1286   loss = 9.439484596252441\n","step = 1287   loss = 9.432177543640137\n","step = 1288   loss = 9.424875259399414\n","step = 1289   loss = 9.417579650878906\n","step = 1290   loss = 9.410289764404297\n","step = 1291   loss = 9.403003692626953\n","step = 1292   loss = 9.395726203918457\n","step = 1293   loss = 9.388453483581543\n","step = 1294   loss = 9.381186485290527\n","step = 1295   loss = 9.373924255371094\n","step = 1296   loss = 9.366667747497559\n","step = 1297   loss = 9.359417915344238\n","step = 1298   loss = 9.352173805236816\n","step = 1299   loss = 9.344934463500977\n","step = 1300   loss = 9.337701797485352\n","step = 1301   loss = 9.330474853515625\n","step = 1302   loss = 9.323253631591797\n","step = 1303   loss = 9.316039085388184\n","step = 1304   loss = 9.308828353881836\n","step = 1305   loss = 9.301623344421387\n","step = 1306   loss = 9.294426918029785\n","step = 1307   loss = 9.287233352661133\n","step = 1308   loss = 9.280045509338379\n","step = 1309   loss = 9.272865295410156\n","step = 1310   loss = 9.2656888961792\n","step = 1311   loss = 9.258520126342773\n","step = 1312   loss = 9.251355171203613\n","step = 1313   loss = 9.244195938110352\n","step = 1314   loss = 9.237044334411621\n","step = 1315   loss = 9.229896545410156\n","step = 1316   loss = 9.222755432128906\n","step = 1317   loss = 9.215620040893555\n","step = 1318   loss = 9.208489418029785\n","step = 1319   loss = 9.201366424560547\n","step = 1320   loss = 9.194247245788574\n","step = 1321   loss = 9.187132835388184\n","step = 1322   loss = 9.180026054382324\n","step = 1323   loss = 9.172924041748047\n","step = 1324   loss = 9.165828704833984\n","step = 1325   loss = 9.158737182617188\n","step = 1326   loss = 9.151653289794922\n","step = 1327   loss = 9.144574165344238\n","step = 1328   loss = 9.137500762939453\n","step = 1329   loss = 9.130433082580566\n","step = 1330   loss = 9.123371124267578\n","step = 1331   loss = 9.116314888000488\n","step = 1332   loss = 9.10926342010498\n","step = 1333   loss = 9.102217674255371\n","step = 1334   loss = 9.095178604125977\n","step = 1335   loss = 9.088144302368164\n","step = 1336   loss = 9.081116676330566\n","step = 1337   loss = 9.07409381866455\n","step = 1338   loss = 9.067076683044434\n","step = 1339   loss = 9.060066223144531\n","step = 1340   loss = 9.053059577941895\n","step = 1341   loss = 9.046058654785156\n","step = 1342   loss = 9.03906536102295\n","step = 1343   loss = 9.032076835632324\n","step = 1344   loss = 9.025093078613281\n","step = 1345   loss = 9.018115043640137\n","step = 1346   loss = 9.01114273071289\n","step = 1347   loss = 9.004176139831543\n","step = 1348   loss = 8.997215270996094\n","step = 1349   loss = 8.990260124206543\n","step = 1350   loss = 8.983309745788574\n","step = 1351   loss = 8.976365089416504\n","step = 1352   loss = 8.969427108764648\n","step = 1353   loss = 8.962493896484375\n","step = 1354   loss = 8.955567359924316\n","step = 1355   loss = 8.948644638061523\n","step = 1356   loss = 8.941727638244629\n","step = 1357   loss = 8.93481731414795\n","step = 1358   loss = 8.927911758422852\n","step = 1359   loss = 8.921012878417969\n","step = 1360   loss = 8.914118766784668\n","step = 1361   loss = 8.907230377197266\n","step = 1362   loss = 8.900346755981445\n","step = 1363   loss = 8.89346981048584\n","step = 1364   loss = 8.886598587036133\n","step = 1365   loss = 8.879732131958008\n","step = 1366   loss = 8.872871398925781\n","step = 1367   loss = 8.866015434265137\n","step = 1368   loss = 8.859166145324707\n","step = 1369   loss = 8.85232162475586\n","step = 1370   loss = 8.84548282623291\n","step = 1371   loss = 8.838650703430176\n","step = 1372   loss = 8.83182144165039\n","step = 1373   loss = 8.824999809265137\n","step = 1374   loss = 8.818183898925781\n","step = 1375   loss = 8.811371803283691\n","step = 1376   loss = 8.804567337036133\n","step = 1377   loss = 8.797767639160156\n","step = 1378   loss = 8.790972709655762\n","step = 1379   loss = 8.784183502197266\n","step = 1380   loss = 8.777400970458984\n","step = 1381   loss = 8.770622253417969\n","step = 1382   loss = 8.763850212097168\n","step = 1383   loss = 8.75708293914795\n","step = 1384   loss = 8.750321388244629\n","step = 1385   loss = 8.74356460571289\n","step = 1386   loss = 8.73681354522705\n","step = 1387   loss = 8.730070114135742\n","step = 1388   loss = 8.723329544067383\n","step = 1389   loss = 8.716595649719238\n","step = 1390   loss = 8.709866523742676\n","step = 1391   loss = 8.703143119812012\n","step = 1392   loss = 8.696425437927246\n","step = 1393   loss = 8.689713478088379\n","step = 1394   loss = 8.68300724029541\n","step = 1395   loss = 8.676305770874023\n","step = 1396   loss = 8.669609069824219\n","step = 1397   loss = 8.662919998168945\n","step = 1398   loss = 8.656234741210938\n","step = 1399   loss = 8.649554252624512\n","step = 1400   loss = 8.6428804397583\n","step = 1401   loss = 8.636212348937988\n","step = 1402   loss = 8.629549026489258\n","step = 1403   loss = 8.622891426086426\n","step = 1404   loss = 8.616238594055176\n","step = 1405   loss = 8.609591484069824\n","step = 1406   loss = 8.602950096130371\n","step = 1407   loss = 8.5963134765625\n","step = 1408   loss = 8.589683532714844\n","step = 1409   loss = 8.58305835723877\n","step = 1410   loss = 8.576437950134277\n","step = 1411   loss = 8.569823265075684\n","step = 1412   loss = 8.563215255737305\n","step = 1413   loss = 8.556612014770508\n","step = 1414   loss = 8.550013542175293\n","step = 1415   loss = 8.543420791625977\n","step = 1416   loss = 8.536831855773926\n","step = 1417   loss = 8.530251502990723\n","step = 1418   loss = 8.523674964904785\n","step = 1419   loss = 8.51710319519043\n","step = 1420   loss = 8.510538101196289\n","step = 1421   loss = 8.503976821899414\n","step = 1422   loss = 8.497422218322754\n","step = 1423   loss = 8.49087142944336\n","step = 1424   loss = 8.48432731628418\n","step = 1425   loss = 8.477788925170898\n","step = 1426   loss = 8.4712553024292\n","step = 1427   loss = 8.464726448059082\n","step = 1428   loss = 8.45820426940918\n","step = 1429   loss = 8.45168685913086\n","step = 1430   loss = 8.445173263549805\n","step = 1431   loss = 8.438667297363281\n","step = 1432   loss = 8.432165145874023\n","step = 1433   loss = 8.425670623779297\n","step = 1434   loss = 8.41917896270752\n","step = 1435   loss = 8.412692070007324\n","step = 1436   loss = 8.40621280670166\n","step = 1437   loss = 8.399738311767578\n","step = 1438   loss = 8.393268585205078\n","step = 1439   loss = 8.386804580688477\n","step = 1440   loss = 8.380346298217773\n","step = 1441   loss = 8.373891830444336\n","step = 1442   loss = 8.367444038391113\n","step = 1443   loss = 8.361001968383789\n","step = 1444   loss = 8.354564666748047\n","step = 1445   loss = 8.348132133483887\n","step = 1446   loss = 8.341704368591309\n","step = 1447   loss = 8.335283279418945\n","step = 1448   loss = 8.328866958618164\n","step = 1449   loss = 8.322454452514648\n","step = 1450   loss = 8.316049575805664\n","step = 1451   loss = 8.309649467468262\n","step = 1452   loss = 8.303254127502441\n","step = 1453   loss = 8.296865463256836\n","step = 1454   loss = 8.290480613708496\n","step = 1455   loss = 8.284100532531738\n","step = 1456   loss = 8.277727127075195\n","step = 1457   loss = 8.271357536315918\n","step = 1458   loss = 8.264994621276855\n","step = 1459   loss = 8.258636474609375\n","step = 1460   loss = 8.252284049987793\n","step = 1461   loss = 8.245936393737793\n","step = 1462   loss = 8.239592552185059\n","step = 1463   loss = 8.233256340026855\n","step = 1464   loss = 8.226924896240234\n","step = 1465   loss = 8.220598220825195\n","step = 1466   loss = 8.214277267456055\n","step = 1467   loss = 8.207961082458496\n","step = 1468   loss = 8.20164966583252\n","step = 1469   loss = 8.195343971252441\n","step = 1470   loss = 8.189043998718262\n","step = 1471   loss = 8.182748794555664\n","step = 1472   loss = 8.176459312438965\n","step = 1473   loss = 8.170175552368164\n","step = 1474   loss = 8.163894653320312\n","step = 1475   loss = 8.157620429992676\n","step = 1476   loss = 8.151352882385254\n","step = 1477   loss = 8.145088195800781\n","step = 1478   loss = 8.138830184936523\n","step = 1479   loss = 8.132576942443848\n","step = 1480   loss = 8.126328468322754\n","step = 1481   loss = 8.120085716247559\n","step = 1482   loss = 8.113846778869629\n","step = 1483   loss = 8.10761547088623\n","step = 1484   loss = 8.101387977600098\n","step = 1485   loss = 8.095166206359863\n","step = 1486   loss = 8.088949203491211\n","step = 1487   loss = 8.08273696899414\n","step = 1488   loss = 8.076530456542969\n","step = 1489   loss = 8.070328712463379\n","step = 1490   loss = 8.064132690429688\n","step = 1491   loss = 8.057942390441895\n","step = 1492   loss = 8.051756858825684\n","step = 1493   loss = 8.045577049255371\n","step = 1494   loss = 8.039401054382324\n","step = 1495   loss = 8.033230781555176\n","step = 1496   loss = 8.027066230773926\n","step = 1497   loss = 8.020905494689941\n","step = 1498   loss = 8.014751434326172\n","step = 1499   loss = 8.008601188659668\n","step = 1500   loss = 8.002457618713379\n","step = 1501   loss = 7.9963178634643555\n","step = 1502   loss = 7.9901838302612305\n","step = 1503   loss = 7.984055042266846\n","step = 1504   loss = 7.977931022644043\n","step = 1505   loss = 7.9718122482299805\n","step = 1506   loss = 7.965698719024658\n","step = 1507   loss = 7.959590911865234\n","step = 1508   loss = 7.953486919403076\n","step = 1509   loss = 7.947389125823975\n","step = 1510   loss = 7.941296577453613\n","step = 1511   loss = 7.935208797454834\n","step = 1512   loss = 7.92912483215332\n","step = 1513   loss = 7.9230475425720215\n","step = 1514   loss = 7.916975498199463\n","step = 1515   loss = 7.910907745361328\n","step = 1516   loss = 7.904844760894775\n","step = 1517   loss = 7.898787975311279\n","step = 1518   loss = 7.892736434936523\n","step = 1519   loss = 7.886688709259033\n","step = 1520   loss = 7.880647659301758\n","step = 1521   loss = 7.874610424041748\n","step = 1522   loss = 7.8685784339904785\n","step = 1523   loss = 7.862551212310791\n","step = 1524   loss = 7.85653018951416\n","step = 1525   loss = 7.850513458251953\n","step = 1526   loss = 7.844501972198486\n","step = 1527   loss = 7.838496208190918\n","step = 1528   loss = 7.832495212554932\n","step = 1529   loss = 7.826498508453369\n","step = 1530   loss = 7.820508003234863\n","step = 1531   loss = 7.8145222663879395\n","step = 1532   loss = 7.808541297912598\n","step = 1533   loss = 7.80256462097168\n","step = 1534   loss = 7.796594142913818\n","step = 1535   loss = 7.790627956390381\n","step = 1536   loss = 7.78466796875\n","step = 1537   loss = 7.778712272644043\n","step = 1538   loss = 7.772761344909668\n","step = 1539   loss = 7.766814708709717\n","step = 1540   loss = 7.7608747482299805\n","step = 1541   loss = 7.754939079284668\n","step = 1542   loss = 7.749009609222412\n","step = 1543   loss = 7.743083477020264\n","step = 1544   loss = 7.737163066864014\n","step = 1545   loss = 7.731248378753662\n","step = 1546   loss = 7.725337505340576\n","step = 1547   loss = 7.719432353973389\n","step = 1548   loss = 7.713531970977783\n","step = 1549   loss = 7.707636833190918\n","step = 1550   loss = 7.701746940612793\n","step = 1551   loss = 7.695862293243408\n","step = 1552   loss = 7.689980506896973\n","step = 1553   loss = 7.68410587310791\n","step = 1554   loss = 7.6782355308532715\n","step = 1555   loss = 7.672371864318848\n","step = 1556   loss = 7.666510581970215\n","step = 1557   loss = 7.6606550216674805\n","step = 1558   loss = 7.654804706573486\n","step = 1559   loss = 7.648960590362549\n","step = 1560   loss = 7.643120288848877\n","step = 1561   loss = 7.637285232543945\n","step = 1562   loss = 7.631454944610596\n","step = 1563   loss = 7.625629425048828\n","step = 1564   loss = 7.619809150695801\n","step = 1565   loss = 7.613992691040039\n","step = 1566   loss = 7.608182907104492\n","step = 1567   loss = 7.602377414703369\n","step = 1568   loss = 7.596576690673828\n","step = 1569   loss = 7.590781211853027\n","step = 1570   loss = 7.584991455078125\n","step = 1571   loss = 7.579205513000488\n","step = 1572   loss = 7.57342529296875\n","step = 1573   loss = 7.5676493644714355\n","step = 1574   loss = 7.561878681182861\n","step = 1575   loss = 7.556113243103027\n","step = 1576   loss = 7.550351619720459\n","step = 1577   loss = 7.544595718383789\n","step = 1578   loss = 7.538845062255859\n","step = 1579   loss = 7.5330986976623535\n","step = 1580   loss = 7.527358531951904\n","step = 1581   loss = 7.521622180938721\n","step = 1582   loss = 7.515891075134277\n","step = 1583   loss = 7.510164737701416\n","step = 1584   loss = 7.504444122314453\n","step = 1585   loss = 7.498726844787598\n","step = 1586   loss = 7.493015766143799\n","step = 1587   loss = 7.487308502197266\n","step = 1588   loss = 7.481606960296631\n","step = 1589   loss = 7.475910186767578\n","step = 1590   loss = 7.470218658447266\n","step = 1591   loss = 7.464531421661377\n","step = 1592   loss = 7.4588494300842285\n","step = 1593   loss = 7.45317268371582\n","step = 1594   loss = 7.447500228881836\n","step = 1595   loss = 7.441833019256592\n","step = 1596   loss = 7.436169624328613\n","step = 1597   loss = 7.43051290512085\n","step = 1598   loss = 7.424859523773193\n","step = 1599   loss = 7.419211387634277\n","step = 1600   loss = 7.413568019866943\n","step = 1601   loss = 7.407930374145508\n","step = 1602   loss = 7.402297496795654\n","step = 1603   loss = 7.396669387817383\n","step = 1604   loss = 7.391045570373535\n","step = 1605   loss = 7.3854265213012695\n","step = 1606   loss = 7.379813194274902\n","step = 1607   loss = 7.374204158782959\n","step = 1608   loss = 7.368599891662598\n","step = 1609   loss = 7.363000392913818\n","step = 1610   loss = 7.3574066162109375\n","step = 1611   loss = 7.3518171310424805\n","step = 1612   loss = 7.346231460571289\n","step = 1613   loss = 7.340651988983154\n","step = 1614   loss = 7.335076808929443\n","step = 1615   loss = 7.329506874084473\n","step = 1616   loss = 7.323942184448242\n","step = 1617   loss = 7.318381309509277\n","step = 1618   loss = 7.3128252029418945\n","step = 1619   loss = 7.307275295257568\n","step = 1620   loss = 7.301729202270508\n","step = 1621   loss = 7.296187877655029\n","step = 1622   loss = 7.290650844573975\n","step = 1623   loss = 7.285119533538818\n","step = 1624   loss = 7.279592990875244\n","step = 1625   loss = 7.274071216583252\n","step = 1626   loss = 7.268553256988525\n","step = 1627   loss = 7.263041973114014\n","step = 1628   loss = 7.257534027099609\n","step = 1629   loss = 7.252030849456787\n","step = 1630   loss = 7.246532440185547\n","step = 1631   loss = 7.241040229797363\n","step = 1632   loss = 7.235551834106445\n","step = 1633   loss = 7.230067729949951\n","step = 1634   loss = 7.224588394165039\n","step = 1635   loss = 7.219114780426025\n","step = 1636   loss = 7.213646411895752\n","step = 1637   loss = 7.208181381225586\n","step = 1638   loss = 7.202722072601318\n","step = 1639   loss = 7.197267055511475\n","step = 1640   loss = 7.191816329956055\n","step = 1641   loss = 7.18637228012085\n","step = 1642   loss = 7.180930137634277\n","step = 1643   loss = 7.175494194030762\n","step = 1644   loss = 7.170063495635986\n","step = 1645   loss = 7.164636611938477\n","step = 1646   loss = 7.159216403961182\n","step = 1647   loss = 7.153799057006836\n","step = 1648   loss = 7.1483869552612305\n","step = 1649   loss = 7.142979145050049\n","step = 1650   loss = 7.137576103210449\n","step = 1651   loss = 7.132178783416748\n","step = 1652   loss = 7.126785755157471\n","step = 1653   loss = 7.121397018432617\n","step = 1654   loss = 7.116013050079346\n","step = 1655   loss = 7.110634803771973\n","step = 1656   loss = 7.105259895324707\n","step = 1657   loss = 7.099891185760498\n","step = 1658   loss = 7.0945258140563965\n","step = 1659   loss = 7.089165687561035\n","step = 1660   loss = 7.083809852600098\n","step = 1661   loss = 7.0784592628479\n","step = 1662   loss = 7.073113918304443\n","step = 1663   loss = 7.06777286529541\n","step = 1664   loss = 7.062436580657959\n","step = 1665   loss = 7.057104110717773\n","step = 1666   loss = 7.051776885986328\n","step = 1667   loss = 7.046454429626465\n","step = 1668   loss = 7.041136264801025\n","step = 1669   loss = 7.035823822021484\n","step = 1670   loss = 7.030515670776367\n","step = 1671   loss = 7.025211811065674\n","step = 1672   loss = 7.019912242889404\n","step = 1673   loss = 7.014617919921875\n","step = 1674   loss = 7.009328842163086\n","step = 1675   loss = 7.0040435791015625\n","step = 1676   loss = 6.998763561248779\n","step = 1677   loss = 6.993487358093262\n","step = 1678   loss = 6.988215923309326\n","step = 1679   loss = 6.982950210571289\n","step = 1680   loss = 6.977688789367676\n","step = 1681   loss = 6.972431182861328\n","step = 1682   loss = 6.967178821563721\n","step = 1683   loss = 6.9619317054748535\n","step = 1684   loss = 6.9566874504089355\n","step = 1685   loss = 6.951450347900391\n","step = 1686   loss = 6.946216583251953\n","step = 1687   loss = 6.940986156463623\n","step = 1688   loss = 6.93576192855835\n","step = 1689   loss = 6.9305419921875\n","step = 1690   loss = 6.925327301025391\n","step = 1691   loss = 6.920117378234863\n","step = 1692   loss = 6.914911270141602\n","step = 1693   loss = 6.909709453582764\n","step = 1694   loss = 6.904512405395508\n","step = 1695   loss = 6.89932107925415\n","step = 1696   loss = 6.894132614135742\n","step = 1697   loss = 6.888949394226074\n","step = 1698   loss = 6.883771896362305\n","step = 1699   loss = 6.878598690032959\n","step = 1700   loss = 6.873429775238037\n","step = 1701   loss = 6.868265151977539\n","step = 1702   loss = 6.863104820251465\n","step = 1703   loss = 6.857949733734131\n","step = 1704   loss = 6.8527984619140625\n","step = 1705   loss = 6.847653865814209\n","step = 1706   loss = 6.8425116539001465\n","step = 1707   loss = 6.837374687194824\n","step = 1708   loss = 6.832242488861084\n","step = 1709   loss = 6.827115058898926\n","step = 1710   loss = 6.821991443634033\n","step = 1711   loss = 6.816873073577881\n","step = 1712   loss = 6.8117594718933105\n","step = 1713   loss = 6.806649684906006\n","step = 1714   loss = 6.801544189453125\n","step = 1715   loss = 6.796443939208984\n","step = 1716   loss = 6.791348934173584\n","step = 1717   loss = 6.786257266998291\n","step = 1718   loss = 6.781169891357422\n","step = 1719   loss = 6.776088714599609\n","step = 1720   loss = 6.7710113525390625\n","step = 1721   loss = 6.765937328338623\n","step = 1722   loss = 6.760869979858398\n","step = 1723   loss = 6.7558064460754395\n","step = 1724   loss = 6.7507476806640625\n","step = 1725   loss = 6.745692253112793\n","step = 1726   loss = 6.740642070770264\n","step = 1727   loss = 6.735596179962158\n","step = 1728   loss = 6.730554103851318\n","step = 1729   loss = 6.725518226623535\n","step = 1730   loss = 6.720487117767334\n","step = 1731   loss = 6.715458869934082\n","step = 1732   loss = 6.710435390472412\n","step = 1733   loss = 6.705416679382324\n","step = 1734   loss = 6.700402736663818\n","step = 1735   loss = 6.6953935623168945\n","step = 1736   loss = 6.6903886795043945\n","step = 1737   loss = 6.685388088226318\n","step = 1738   loss = 6.680391788482666\n","step = 1739   loss = 6.675401210784912\n","step = 1740   loss = 6.670413970947266\n","step = 1741   loss = 6.665431976318359\n","step = 1742   loss = 6.660452842712402\n","step = 1743   loss = 6.655479907989502\n","step = 1744   loss = 6.650510787963867\n","step = 1745   loss = 6.6455464363098145\n","step = 1746   loss = 6.640585899353027\n","step = 1747   loss = 6.6356306076049805\n","step = 1748   loss = 6.630678653717041\n","step = 1749   loss = 6.625732421875\n","step = 1750   loss = 6.620790481567383\n","step = 1751   loss = 6.6158528327941895\n","step = 1752   loss = 6.610918998718262\n","step = 1753   loss = 6.605990409851074\n","step = 1754   loss = 6.6010661125183105\n","step = 1755   loss = 6.596147537231445\n","step = 1756   loss = 6.591231822967529\n","step = 1757   loss = 6.586320877075195\n","step = 1758   loss = 6.581414222717285\n","step = 1759   loss = 6.576511859893799\n","step = 1760   loss = 6.571615219116211\n","step = 1761   loss = 6.566720962524414\n","step = 1762   loss = 6.561832904815674\n","step = 1763   loss = 6.556948184967041\n","step = 1764   loss = 6.552068710327148\n","step = 1765   loss = 6.5471930503845215\n","step = 1766   loss = 6.542322158813477\n","step = 1767   loss = 6.537456035614014\n","step = 1768   loss = 6.532593250274658\n","step = 1769   loss = 6.527735710144043\n","step = 1770   loss = 6.522883415222168\n","step = 1771   loss = 6.5180344581604\n","step = 1772   loss = 6.513189792633057\n","step = 1773   loss = 6.508349418640137\n","step = 1774   loss = 6.503514289855957\n","step = 1775   loss = 6.498682975769043\n","step = 1776   loss = 6.493856430053711\n","step = 1777   loss = 6.489034652709961\n","step = 1778   loss = 6.484216213226318\n","step = 1779   loss = 6.4794020652771\n","step = 1780   loss = 6.474592685699463\n","step = 1781   loss = 6.469788074493408\n","step = 1782   loss = 6.464987754821777\n","step = 1783   loss = 6.4601922035217285\n","step = 1784   loss = 6.455400466918945\n","step = 1785   loss = 6.450613021850586\n","step = 1786   loss = 6.445829391479492\n","step = 1787   loss = 6.441051483154297\n","step = 1788   loss = 6.436277389526367\n","step = 1789   loss = 6.431507110595703\n","step = 1790   loss = 6.426741123199463\n","step = 1791   loss = 6.421981334686279\n","step = 1792   loss = 6.417224884033203\n","step = 1793   loss = 6.412472724914551\n","step = 1794   loss = 6.407724380493164\n","step = 1795   loss = 6.402980804443359\n","step = 1796   loss = 6.39824104309082\n","step = 1797   loss = 6.3935065269470215\n","step = 1798   loss = 6.3887763023376465\n","step = 1799   loss = 6.384049892425537\n","step = 1800   loss = 6.379327774047852\n","step = 1801   loss = 6.37460994720459\n","step = 1802   loss = 6.369897365570068\n","step = 1803   loss = 6.3651885986328125\n","step = 1804   loss = 6.3604841232299805\n","step = 1805   loss = 6.355783462524414\n","step = 1806   loss = 6.3510870933532715\n","step = 1807   loss = 6.346395969390869\n","step = 1808   loss = 6.341709136962891\n","step = 1809   loss = 6.337026596069336\n","step = 1810   loss = 6.332347393035889\n","step = 1811   loss = 6.327673435211182\n","step = 1812   loss = 6.323002815246582\n","step = 1813   loss = 6.318337917327881\n","step = 1814   loss = 6.3136773109436035\n","step = 1815   loss = 6.309019565582275\n","step = 1816   loss = 6.3043670654296875\n","step = 1817   loss = 6.299718856811523\n","step = 1818   loss = 6.295074462890625\n","step = 1819   loss = 6.29043436050415\n","step = 1820   loss = 6.285799026489258\n","step = 1821   loss = 6.281167507171631\n","step = 1822   loss = 6.276540756225586\n","step = 1823   loss = 6.271918296813965\n","step = 1824   loss = 6.267299652099609\n","step = 1825   loss = 6.2626848220825195\n","step = 1826   loss = 6.258074760437012\n","step = 1827   loss = 6.253469944000244\n","step = 1828   loss = 6.248868465423584\n","step = 1829   loss = 6.244271278381348\n","step = 1830   loss = 6.239678859710693\n","step = 1831   loss = 6.2350897789001465\n","step = 1832   loss = 6.230505466461182\n","step = 1833   loss = 6.225925922393799\n","step = 1834   loss = 6.221350193023682\n","step = 1835   loss = 6.216778755187988\n","step = 1836   loss = 6.212212085723877\n","step = 1837   loss = 6.207648277282715\n","step = 1838   loss = 6.203089714050293\n","step = 1839   loss = 6.198534965515137\n","step = 1840   loss = 6.1939849853515625\n","step = 1841   loss = 6.189438343048096\n","step = 1842   loss = 6.184896945953369\n","step = 1843   loss = 6.18035888671875\n","step = 1844   loss = 6.175825595855713\n","step = 1845   loss = 6.171297073364258\n","step = 1846   loss = 6.166772365570068\n","step = 1847   loss = 6.162250995635986\n","step = 1848   loss = 6.157734394073486\n","step = 1849   loss = 6.153223037719727\n","step = 1850   loss = 6.148714542388916\n","step = 1851   loss = 6.144209384918213\n","step = 1852   loss = 6.139710903167725\n","step = 1853   loss = 6.1352152824401855\n","step = 1854   loss = 6.130724906921387\n","step = 1855   loss = 6.126237392425537\n","step = 1856   loss = 6.121754169464111\n","step = 1857   loss = 6.117275238037109\n","step = 1858   loss = 6.112801551818848\n","step = 1859   loss = 6.108331203460693\n","step = 1860   loss = 6.103865146636963\n","step = 1861   loss = 6.0994038581848145\n","step = 1862   loss = 6.094944953918457\n","step = 1863   loss = 6.090491771697998\n","step = 1864   loss = 6.086042404174805\n","step = 1865   loss = 6.081597328186035\n","step = 1866   loss = 6.0771565437316895\n","step = 1867   loss = 6.072719573974609\n","step = 1868   loss = 6.068287372589111\n","step = 1869   loss = 6.063858509063721\n","step = 1870   loss = 6.059434413909912\n","step = 1871   loss = 6.055014133453369\n","step = 1872   loss = 6.050597667694092\n","step = 1873   loss = 6.0461859703063965\n","step = 1874   loss = 6.041778087615967\n","step = 1875   loss = 6.037374496459961\n","step = 1876   loss = 6.032974720001221\n","step = 1877   loss = 6.0285797119140625\n","step = 1878   loss = 6.024189472198486\n","step = 1879   loss = 6.019802093505859\n","step = 1880   loss = 6.015418529510498\n","step = 1881   loss = 6.011040210723877\n","step = 1882   loss = 6.0066657066345215\n","step = 1883   loss = 6.00229549407959\n","step = 1884   loss = 5.997928619384766\n","step = 1885   loss = 5.993566513061523\n","step = 1886   loss = 5.989208221435547\n","step = 1887   loss = 5.984854221343994\n","step = 1888   loss = 5.980505466461182\n","step = 1889   loss = 5.97615909576416\n","step = 1890   loss = 5.971817493438721\n","step = 1891   loss = 5.967479228973389\n","step = 1892   loss = 5.963145732879639\n","step = 1893   loss = 5.958817958831787\n","step = 1894   loss = 5.954492092132568\n","step = 1895   loss = 5.950170993804932\n","step = 1896   loss = 5.9458537101745605\n","step = 1897   loss = 5.941540718078613\n","step = 1898   loss = 5.93723201751709\n","step = 1899   loss = 5.932927131652832\n","step = 1900   loss = 5.928626537322998\n","step = 1901   loss = 5.924330234527588\n","step = 1902   loss = 5.920037269592285\n","step = 1903   loss = 5.915749549865723\n","step = 1904   loss = 5.911464691162109\n","step = 1905   loss = 5.907184600830078\n","step = 1906   loss = 5.9029083251953125\n","step = 1907   loss = 5.898636341094971\n","step = 1908   loss = 5.894369125366211\n","step = 1909   loss = 5.890104293823242\n","step = 1910   loss = 5.8858442306518555\n","step = 1911   loss = 5.881587982177734\n","step = 1912   loss = 5.877336502075195\n","step = 1913   loss = 5.873088836669922\n","step = 1914   loss = 5.868844985961914\n","step = 1915   loss = 5.864604949951172\n","step = 1916   loss = 5.860369682312012\n","step = 1917   loss = 5.856138229370117\n","step = 1918   loss = 5.851910591125488\n","step = 1919   loss = 5.847686767578125\n","step = 1920   loss = 5.8434672355651855\n","step = 1921   loss = 5.839252948760986\n","step = 1922   loss = 5.835041522979736\n","step = 1923   loss = 5.830833911895752\n","step = 1924   loss = 5.826629638671875\n","step = 1925   loss = 5.8224310874938965\n","step = 1926   loss = 5.818234920501709\n","step = 1927   loss = 5.8140435218811035\n","step = 1928   loss = 5.809856414794922\n","step = 1929   loss = 5.805672645568848\n","step = 1930   loss = 5.8014936447143555\n","step = 1931   loss = 5.797318935394287\n","step = 1932   loss = 5.793147563934326\n","step = 1933   loss = 5.788980007171631\n","step = 1934   loss = 5.784816741943359\n","step = 1935   loss = 5.7806572914123535\n","step = 1936   loss = 5.7765021324157715\n","step = 1937   loss = 5.772351264953613\n","step = 1938   loss = 5.7682037353515625\n","step = 1939   loss = 5.764060020446777\n","step = 1940   loss = 5.759920597076416\n","step = 1941   loss = 5.7557854652404785\n","step = 1942   loss = 5.751654148101807\n","step = 1943   loss = 5.7475266456604\n","step = 1944   loss = 5.743403911590576\n","step = 1945   loss = 5.739283561706543\n","step = 1946   loss = 5.735168933868408\n","step = 1947   loss = 5.731057167053223\n","step = 1948   loss = 5.726949691772461\n","step = 1949   loss = 5.722846031188965\n","step = 1950   loss = 5.718746185302734\n","step = 1951   loss = 5.714651107788086\n","step = 1952   loss = 5.710558891296387\n","step = 1953   loss = 5.706470966339111\n","step = 1954   loss = 5.702386856079102\n","step = 1955   loss = 5.698307514190674\n","step = 1956   loss = 5.694231986999512\n","step = 1957   loss = 5.690159320831299\n","step = 1958   loss = 5.686091423034668\n","step = 1959   loss = 5.682027816772461\n","step = 1960   loss = 5.677967548370361\n","step = 1961   loss = 5.6739115715026855\n","step = 1962   loss = 5.669858932495117\n","step = 1963   loss = 5.6658101081848145\n","step = 1964   loss = 5.661766529083252\n","step = 1965   loss = 5.657725811004639\n","step = 1966   loss = 5.653688907623291\n","step = 1967   loss = 5.649656295776367\n","step = 1968   loss = 5.645627498626709\n","step = 1969   loss = 5.641602039337158\n","step = 1970   loss = 5.6375813484191895\n","step = 1971   loss = 5.633564472198486\n","step = 1972   loss = 5.629551410675049\n","step = 1973   loss = 5.6255412101745605\n","step = 1974   loss = 5.6215362548828125\n","step = 1975   loss = 5.617535591125488\n","step = 1976   loss = 5.613537788391113\n","step = 1977   loss = 5.609544277191162\n","step = 1978   loss = 5.60555362701416\n","step = 1979   loss = 5.601568222045898\n","step = 1980   loss = 5.597586154937744\n","step = 1981   loss = 5.593608379364014\n","step = 1982   loss = 5.589633941650391\n","step = 1983   loss = 5.585663318634033\n","step = 1984   loss = 5.581697463989258\n","step = 1985   loss = 5.577734470367432\n","step = 1986   loss = 5.5737762451171875\n","step = 1987   loss = 5.569820404052734\n","step = 1988   loss = 5.56587028503418\n","step = 1989   loss = 5.561922550201416\n","step = 1990   loss = 5.557979583740234\n","step = 1991   loss = 5.554040431976318\n","step = 1992   loss = 5.550104141235352\n","step = 1993   loss = 5.546173095703125\n","step = 1994   loss = 5.542244911193848\n","step = 1995   loss = 5.538321018218994\n","step = 1996   loss = 5.534400463104248\n","step = 1997   loss = 5.530484199523926\n","step = 1998   loss = 5.526571750640869\n","step = 1999   loss = 5.522663116455078\n","step = 2000   loss = 5.5187578201293945\n","step = 2001   loss = 5.514856338500977\n","step = 2002   loss = 5.510960102081299\n","step = 2003   loss = 5.50706672668457\n","step = 2004   loss = 5.503176689147949\n","step = 2005   loss = 5.499291896820068\n","step = 2006   loss = 5.4954094886779785\n","step = 2007   loss = 5.4915313720703125\n","step = 2008   loss = 5.48765754699707\n","step = 2009   loss = 5.483786582946777\n","step = 2010   loss = 5.479919910430908\n","step = 2011   loss = 5.476057529449463\n","step = 2012   loss = 5.472198486328125\n","step = 2013   loss = 5.468343734741211\n","step = 2014   loss = 5.464492321014404\n","step = 2015   loss = 5.460644721984863\n","step = 2016   loss = 5.45680046081543\n","step = 2017   loss = 5.452960014343262\n","step = 2018   loss = 5.449123382568359\n","step = 2019   loss = 5.445291519165039\n","step = 2020   loss = 5.441463470458984\n","step = 2021   loss = 5.437637805938721\n","step = 2022   loss = 5.433816909790039\n","step = 2023   loss = 5.4300007820129395\n","step = 2024   loss = 5.426186561584473\n","step = 2025   loss = 5.4223761558532715\n","step = 2026   loss = 5.418570518493652\n","step = 2027   loss = 5.414768695831299\n","step = 2028   loss = 5.410970687866211\n","step = 2029   loss = 5.407175064086914\n","step = 2030   loss = 5.403384208679199\n","step = 2031   loss = 5.39959716796875\n","step = 2032   loss = 5.395813465118408\n","step = 2033   loss = 5.392033576965332\n","step = 2034   loss = 5.38825798034668\n","step = 2035   loss = 5.384486675262451\n","step = 2036   loss = 5.380717754364014\n","step = 2037   loss = 5.376953125\n","step = 2038   loss = 5.373192310333252\n","step = 2039   loss = 5.3694353103637695\n","step = 2040   loss = 5.3656816482543945\n","step = 2041   loss = 5.361931800842285\n","step = 2042   loss = 5.3581862449646\n","step = 2043   loss = 5.3544440269470215\n","step = 2044   loss = 5.350705146789551\n","step = 2045   loss = 5.346970081329346\n","step = 2046   loss = 5.3432393074035645\n","step = 2047   loss = 5.339511871337891\n","step = 2048   loss = 5.335788726806641\n","step = 2049   loss = 5.33206844329834\n","step = 2050   loss = 5.328351974487305\n","step = 2051   loss = 5.324639797210693\n","step = 2052   loss = 5.320930004119873\n","step = 2053   loss = 5.317225933074951\n","step = 2054   loss = 5.31352424621582\n","step = 2055   loss = 5.309826850891113\n","step = 2056   loss = 5.306132793426514\n","step = 2057   loss = 5.30244255065918\n","step = 2058   loss = 5.298755645751953\n","step = 2059   loss = 5.29507303237915\n","step = 2060   loss = 5.291394233703613\n","step = 2061   loss = 5.287717819213867\n","step = 2062   loss = 5.284046649932861\n","step = 2063   loss = 5.280378341674805\n","step = 2064   loss = 5.276713848114014\n","step = 2065   loss = 5.273053169250488\n","step = 2066   loss = 5.26939582824707\n","step = 2067   loss = 5.265742778778076\n","step = 2068   loss = 5.2620930671691895\n","step = 2069   loss = 5.25844669342041\n","step = 2070   loss = 5.2548041343688965\n","step = 2071   loss = 5.251165390014648\n","step = 2072   loss = 5.247530460357666\n","step = 2073   loss = 5.243898391723633\n","step = 2074   loss = 5.24027156829834\n","step = 2075   loss = 5.236647129058838\n","step = 2076   loss = 5.233026027679443\n","step = 2077   loss = 5.229409694671631\n","step = 2078   loss = 5.225796699523926\n","step = 2079   loss = 5.222187042236328\n","step = 2080   loss = 5.218580722808838\n","step = 2081   loss = 5.214978218078613\n","step = 2082   loss = 5.211379528045654\n","step = 2083   loss = 5.207784652709961\n","step = 2084   loss = 5.204192638397217\n","step = 2085   loss = 5.200605392456055\n","step = 2086   loss = 5.197021007537842\n","step = 2087   loss = 5.1934404373168945\n","step = 2088   loss = 5.189863681793213\n","step = 2089   loss = 5.186290740966797\n","step = 2090   loss = 5.182721138000488\n","step = 2091   loss = 5.179153919219971\n","step = 2092   loss = 5.175591945648193\n","step = 2093   loss = 5.172033309936523\n","step = 2094   loss = 5.168478488922119\n","step = 2095   loss = 5.164926052093506\n","step = 2096   loss = 5.161378860473633\n","step = 2097   loss = 5.157834053039551\n","step = 2098   loss = 5.154293060302734\n","step = 2099   loss = 5.150756359100342\n","step = 2100   loss = 5.147222518920898\n","step = 2101   loss = 5.143692493438721\n","step = 2102   loss = 5.14016580581665\n","step = 2103   loss = 5.136642932891846\n","step = 2104   loss = 5.133123874664307\n","step = 2105   loss = 5.129608154296875\n","step = 2106   loss = 5.126096248626709\n","step = 2107   loss = 5.12258768081665\n","step = 2108   loss = 5.119082450866699\n","step = 2109   loss = 5.115581035614014\n","step = 2110   loss = 5.1120829582214355\n","step = 2111   loss = 5.108588695526123\n","step = 2112   loss = 5.105098247528076\n","step = 2113   loss = 5.1016106605529785\n","step = 2114   loss = 5.098127841949463\n","step = 2115   loss = 5.09464693069458\n","step = 2116   loss = 5.091170787811279\n","step = 2117   loss = 5.087697982788086\n","step = 2118   loss = 5.084228515625\n","step = 2119   loss = 5.08076286315918\n","step = 2120   loss = 5.077300071716309\n","step = 2121   loss = 5.073841571807861\n","step = 2122   loss = 5.070385932922363\n","step = 2123   loss = 5.066934108734131\n","step = 2124   loss = 5.063486099243164\n","step = 2125   loss = 5.060041427612305\n","step = 2126   loss = 5.056600570678711\n","step = 2127   loss = 5.053163051605225\n","step = 2128   loss = 5.049728870391846\n","step = 2129   loss = 5.046298980712891\n","step = 2130   loss = 5.042871475219727\n","step = 2131   loss = 5.039447784423828\n","step = 2132   loss = 5.036027908325195\n","step = 2133   loss = 5.03261137008667\n","step = 2134   loss = 5.02919864654541\n","step = 2135   loss = 5.025789260864258\n","step = 2136   loss = 5.022383213043213\n","step = 2137   loss = 5.018980503082275\n","step = 2138   loss = 5.015582084655762\n","step = 2139   loss = 5.012186050415039\n","step = 2140   loss = 5.008793830871582\n","step = 2141   loss = 5.005405426025391\n","step = 2142   loss = 5.002021312713623\n","step = 2143   loss = 4.9986395835876465\n","step = 2144   loss = 4.9952616691589355\n","step = 2145   loss = 4.99188756942749\n","step = 2146   loss = 4.988516330718994\n","step = 2147   loss = 4.9851484298706055\n","step = 2148   loss = 4.981785297393799\n","step = 2149   loss = 4.978423595428467\n","step = 2150   loss = 4.975066661834717\n","step = 2151   loss = 4.971714019775391\n","step = 2152   loss = 4.968362808227539\n","step = 2153   loss = 4.965016841888428\n","step = 2154   loss = 4.961673259735107\n","step = 2155   loss = 4.958333492279053\n","step = 2156   loss = 4.9549970626831055\n","step = 2157   loss = 4.951664447784424\n","step = 2158   loss = 4.948334693908691\n","step = 2159   loss = 4.945009231567383\n","step = 2160   loss = 4.941687107086182\n","step = 2161   loss = 4.93836784362793\n","step = 2162   loss = 4.935052394866943\n","step = 2163   loss = 4.9317402839660645\n","step = 2164   loss = 4.928431034088135\n","step = 2165   loss = 4.925126075744629\n","step = 2166   loss = 4.921824932098389\n","step = 2167   loss = 4.9185261726379395\n","step = 2168   loss = 4.915230751037598\n","step = 2169   loss = 4.91193962097168\n","step = 2170   loss = 4.908651351928711\n","step = 2171   loss = 4.90536642074585\n","step = 2172   loss = 4.902085781097412\n","step = 2173   loss = 4.898807525634766\n","step = 2174   loss = 4.895532608032227\n","step = 2175   loss = 4.8922624588012695\n","step = 2176   loss = 4.888994216918945\n","step = 2177   loss = 4.885730743408203\n","step = 2178   loss = 4.882469654083252\n","step = 2179   loss = 4.879211902618408\n","step = 2180   loss = 4.87595796585083\n","step = 2181   loss = 4.872707366943359\n","step = 2182   loss = 4.869460105895996\n","step = 2183   loss = 4.866216659545898\n","step = 2184   loss = 4.862976551055908\n","step = 2185   loss = 4.859738826751709\n","step = 2186   loss = 4.856505870819092\n","step = 2187   loss = 4.853274822235107\n","step = 2188   loss = 4.850048542022705\n","step = 2189   loss = 4.846824645996094\n","step = 2190   loss = 4.843604564666748\n","step = 2191   loss = 4.84038782119751\n","step = 2192   loss = 4.837174892425537\n","step = 2193   loss = 4.833965301513672\n","step = 2194   loss = 4.8307576179504395\n","step = 2195   loss = 4.827554702758789\n","step = 2196   loss = 4.824354648590088\n","step = 2197   loss = 4.8211588859558105\n","step = 2198   loss = 4.817965507507324\n","step = 2199   loss = 4.814775466918945\n","step = 2200   loss = 4.81158971786499\n","step = 2201   loss = 4.808405876159668\n","step = 2202   loss = 4.8052263259887695\n","step = 2203   loss = 4.80204963684082\n","step = 2204   loss = 4.798876762390137\n","step = 2205   loss = 4.7957072257995605\n","step = 2206   loss = 4.792541027069092\n","step = 2207   loss = 4.789378643035889\n","step = 2208   loss = 4.786218643188477\n","step = 2209   loss = 4.783061981201172\n","step = 2210   loss = 4.779909133911133\n","step = 2211   loss = 4.776759147644043\n","step = 2212   loss = 4.773612976074219\n","step = 2213   loss = 4.770469665527344\n","step = 2214   loss = 4.767330169677734\n","step = 2215   loss = 4.764193534851074\n","step = 2216   loss = 4.761061191558838\n","step = 2217   loss = 4.757931232452393\n","step = 2218   loss = 4.754805088043213\n","step = 2219   loss = 4.751681327819824\n","step = 2220   loss = 4.748561382293701\n","step = 2221   loss = 4.745445728302002\n","step = 2222   loss = 4.7423319816589355\n","step = 2223   loss = 4.739222526550293\n","step = 2224   loss = 4.7361159324646\n","step = 2225   loss = 4.733012676239014\n","step = 2226   loss = 4.729913234710693\n","step = 2227   loss = 4.726816177368164\n","step = 2228   loss = 4.723722457885742\n","step = 2229   loss = 4.720632553100586\n","step = 2230   loss = 4.717545986175537\n","step = 2231   loss = 4.714461803436279\n","step = 2232   loss = 4.711381912231445\n","step = 2233   loss = 4.708304405212402\n","step = 2234   loss = 4.705230712890625\n","step = 2235   loss = 4.702159881591797\n","step = 2236   loss = 4.699093818664551\n","step = 2237   loss = 4.696029186248779\n","step = 2238   loss = 4.692968845367432\n","step = 2239   loss = 4.689911365509033\n","step = 2240   loss = 4.686857223510742\n","step = 2241   loss = 4.683806419372559\n","step = 2242   loss = 4.680758476257324\n","step = 2243   loss = 4.6777143478393555\n","step = 2244   loss = 4.674673557281494\n","step = 2245   loss = 4.671635627746582\n","step = 2246   loss = 4.6686015129089355\n","step = 2247   loss = 4.66556978225708\n","step = 2248   loss = 4.66254186630249\n","step = 2249   loss = 4.659517288208008\n","step = 2250   loss = 4.656495571136475\n","step = 2251   loss = 4.653476715087891\n","step = 2252   loss = 4.6504621505737305\n","step = 2253   loss = 4.647449970245361\n","step = 2254   loss = 4.6444411277771\n","step = 2255   loss = 4.641435623168945\n","step = 2256   loss = 4.638433456420898\n","step = 2257   loss = 4.635434627532959\n","step = 2258   loss = 4.632439136505127\n","step = 2259   loss = 4.629446029663086\n","step = 2260   loss = 4.6264567375183105\n","step = 2261   loss = 4.623470783233643\n","step = 2262   loss = 4.620488166809082\n","step = 2263   loss = 4.6175079345703125\n","step = 2264   loss = 4.61453104019165\n","step = 2265   loss = 4.611558437347412\n","step = 2266   loss = 4.608587741851807\n","step = 2267   loss = 4.605621814727783\n","step = 2268   loss = 4.602657318115234\n","step = 2269   loss = 4.599697113037109\n","step = 2270   loss = 4.596739768981934\n","step = 2271   loss = 4.593785762786865\n","step = 2272   loss = 4.590835094451904\n","step = 2273   loss = 4.587887287139893\n","step = 2274   loss = 4.584942817687988\n","step = 2275   loss = 4.582001209259033\n","step = 2276   loss = 4.5790629386901855\n","step = 2277   loss = 4.5761284828186035\n","step = 2278   loss = 4.573196887969971\n","step = 2279   loss = 4.570268154144287\n","step = 2280   loss = 4.567342281341553\n","step = 2281   loss = 4.564420223236084\n","step = 2282   loss = 4.561501502990723\n","step = 2283   loss = 4.5585856437683105\n","step = 2284   loss = 4.5556721687316895\n","step = 2285   loss = 4.552762985229492\n","step = 2286   loss = 4.549856185913086\n","step = 2287   loss = 4.546952724456787\n","step = 2288   loss = 4.544052600860596\n","step = 2289   loss = 4.541155815124512\n","step = 2290   loss = 4.538261890411377\n","step = 2291   loss = 4.535370826721191\n","step = 2292   loss = 4.53248405456543\n","step = 2293   loss = 4.529599189758301\n","step = 2294   loss = 4.526717662811279\n","step = 2295   loss = 4.523839950561523\n","step = 2296   loss = 4.520964622497559\n","step = 2297   loss = 4.518092632293701\n","step = 2298   loss = 4.515223979949951\n","step = 2299   loss = 4.51235818862915\n","step = 2300   loss = 4.509496212005615\n","step = 2301   loss = 4.506636619567871\n","step = 2302   loss = 4.503780364990234\n","step = 2303   loss = 4.500926971435547\n","step = 2304   loss = 4.498076915740967\n","step = 2305   loss = 4.495230197906494\n","step = 2306   loss = 4.492386341094971\n","step = 2307   loss = 4.489545822143555\n","step = 2308   loss = 4.486708164215088\n","step = 2309   loss = 4.4838738441467285\n","step = 2310   loss = 4.481042385101318\n","step = 2311   loss = 4.478213310241699\n","step = 2312   loss = 4.475389003753662\n","step = 2313   loss = 4.472567081451416\n","step = 2314   loss = 4.469748020172119\n","step = 2315   loss = 4.4669318199157715\n","step = 2316   loss = 4.4641194343566895\n","step = 2317   loss = 4.461309909820557\n","step = 2318   loss = 4.458502769470215\n","step = 2319   loss = 4.4556989669799805\n","step = 2320   loss = 4.452898979187012\n","step = 2321   loss = 4.450101852416992\n","step = 2322   loss = 4.447307586669922\n","step = 2323   loss = 4.444516181945801\n","step = 2324   loss = 4.441728115081787\n","step = 2325   loss = 4.438942909240723\n","step = 2326   loss = 4.436161041259766\n","step = 2327   loss = 4.433382034301758\n","step = 2328   loss = 4.430606365203857\n","step = 2329   loss = 4.427833557128906\n","step = 2330   loss = 4.425063610076904\n","step = 2331   loss = 4.422297477722168\n","step = 2332   loss = 4.4195332527160645\n","step = 2333   loss = 4.416772365570068\n","step = 2334   loss = 4.414015293121338\n","step = 2335   loss = 4.411261081695557\n","step = 2336   loss = 4.408509254455566\n","step = 2337   loss = 4.405760765075684\n","step = 2338   loss = 4.403015613555908\n","step = 2339   loss = 4.400273323059082\n","step = 2340   loss = 4.397534370422363\n","step = 2341   loss = 4.3947978019714355\n","step = 2342   loss = 4.392065048217773\n","step = 2343   loss = 4.389334201812744\n","step = 2344   loss = 4.3866071701049805\n","step = 2345   loss = 4.383882999420166\n","step = 2346   loss = 4.381161689758301\n","step = 2347   loss = 4.378443717956543\n","step = 2348   loss = 4.375728607177734\n","step = 2349   loss = 4.373016834259033\n","step = 2350   loss = 4.370307445526123\n","step = 2351   loss = 4.367600917816162\n","step = 2352   loss = 4.364898681640625\n","step = 2353   loss = 4.362198829650879\n","step = 2354   loss = 4.359501838684082\n","step = 2355   loss = 4.356808185577393\n","step = 2356   loss = 4.354116439819336\n","step = 2357   loss = 4.351428508758545\n","step = 2358   loss = 4.348743438720703\n","step = 2359   loss = 4.346061706542969\n","step = 2360   loss = 4.343382358551025\n","step = 2361   loss = 4.3407063484191895\n","step = 2362   loss = 4.338033676147461\n","step = 2363   loss = 4.335363864898682\n","step = 2364   loss = 4.332696437835693\n","step = 2365   loss = 4.3300323486328125\n","step = 2366   loss = 4.327371120452881\n","step = 2367   loss = 4.324713230133057\n","step = 2368   loss = 4.322057723999023\n","step = 2369   loss = 4.319405555725098\n","step = 2370   loss = 4.316756725311279\n","step = 2371   loss = 4.314110279083252\n","step = 2372   loss = 4.311466693878174\n","step = 2373   loss = 4.308826446533203\n","step = 2374   loss = 4.30618953704834\n","step = 2375   loss = 4.303555011749268\n","step = 2376   loss = 4.3009233474731445\n","step = 2377   loss = 4.298295021057129\n","step = 2378   loss = 4.295669078826904\n","step = 2379   loss = 4.293046474456787\n","step = 2380   loss = 4.290427207946777\n","step = 2381   loss = 4.287810802459717\n","step = 2382   loss = 4.2851972579956055\n","step = 2383   loss = 4.282586097717285\n","step = 2384   loss = 4.279977798461914\n","step = 2385   loss = 4.27737283706665\n","step = 2386   loss = 4.274771213531494\n","step = 2387   loss = 4.272171497344971\n","step = 2388   loss = 4.269575595855713\n","step = 2389   loss = 4.266982555389404\n","step = 2390   loss = 4.264392375946045\n","step = 2391   loss = 4.261804580688477\n","step = 2392   loss = 4.259221076965332\n","step = 2393   loss = 4.256639003753662\n","step = 2394   loss = 4.2540602684021\n","step = 2395   loss = 4.2514848709106445\n","step = 2396   loss = 4.2489118576049805\n","step = 2397   loss = 4.246341705322266\n","step = 2398   loss = 4.243774890899658\n","step = 2399   loss = 4.241211414337158\n","step = 2400   loss = 4.238649845123291\n","step = 2401   loss = 4.2360920906066895\n","step = 2402   loss = 4.233536243438721\n","step = 2403   loss = 4.230983734130859\n","step = 2404   loss = 4.2284345626831055\n","step = 2405   loss = 4.225888252258301\n","step = 2406   loss = 4.223343849182129\n","step = 2407   loss = 4.2208027839660645\n","step = 2408   loss = 4.218264579772949\n","step = 2409   loss = 4.2157301902771\n","step = 2410   loss = 4.213197708129883\n","step = 2411   loss = 4.210668563842773\n","step = 2412   loss = 4.208141803741455\n","step = 2413   loss = 4.205618381500244\n","step = 2414   loss = 4.203097820281982\n","step = 2415   loss = 4.20058012008667\n","step = 2416   loss = 4.198065280914307\n","step = 2417   loss = 4.195552825927734\n","step = 2418   loss = 4.1930437088012695\n","step = 2419   loss = 4.190536975860596\n","step = 2420   loss = 4.188033580780029\n","step = 2421   loss = 4.185533046722412\n","step = 2422   loss = 4.183035373687744\n","step = 2423   loss = 4.180539608001709\n","step = 2424   loss = 4.178048610687256\n","step = 2425   loss = 4.175559043884277\n","step = 2426   loss = 4.173072338104248\n","step = 2427   loss = 4.170588493347168\n","step = 2428   loss = 4.168107986450195\n","step = 2429   loss = 4.165630340576172\n","step = 2430   loss = 4.163155555725098\n","step = 2431   loss = 4.160682678222656\n","step = 2432   loss = 4.1582136154174805\n","step = 2433   loss = 4.155746936798096\n","step = 2434   loss = 4.153283596038818\n","step = 2435   loss = 4.150822639465332\n","step = 2436   loss = 4.148364067077637\n","step = 2437   loss = 4.145909786224365\n","step = 2438   loss = 4.143456935882568\n","step = 2439   loss = 4.141007423400879\n","step = 2440   loss = 4.1385602951049805\n","step = 2441   loss = 4.1361165046691895\n","step = 2442   loss = 4.1336750984191895\n","step = 2443   loss = 4.131237030029297\n","step = 2444   loss = 4.1288018226623535\n","step = 2445   loss = 4.126369476318359\n","step = 2446   loss = 4.123939037322998\n","step = 2447   loss = 4.121511936187744\n","step = 2448   loss = 4.119087219238281\n","step = 2449   loss = 4.116666316986084\n","step = 2450   loss = 4.1142473220825195\n","step = 2451   loss = 4.1118316650390625\n","step = 2452   loss = 4.1094183921813965\n","step = 2453   loss = 4.10700798034668\n","step = 2454   loss = 4.104600429534912\n","step = 2455   loss = 4.102195739746094\n","step = 2456   loss = 4.099793434143066\n","step = 2457   loss = 4.0973944664001465\n","step = 2458   loss = 4.094998359680176\n","step = 2459   loss = 4.092604160308838\n","step = 2460   loss = 4.090213775634766\n","step = 2461   loss = 4.087824821472168\n","step = 2462   loss = 4.085439682006836\n","step = 2463   loss = 4.083057403564453\n","step = 2464   loss = 4.0806779861450195\n","step = 2465   loss = 4.078300952911377\n","step = 2466   loss = 4.075926303863525\n","step = 2467   loss = 4.073554992675781\n","step = 2468   loss = 4.071186542510986\n","step = 2469   loss = 4.068819999694824\n","step = 2470   loss = 4.066457271575928\n","step = 2471   loss = 4.064096450805664\n","step = 2472   loss = 4.06173849105835\n","step = 2473   loss = 4.059383869171143\n","step = 2474   loss = 4.057031154632568\n","step = 2475   loss = 4.054681777954102\n","step = 2476   loss = 4.052335262298584\n","step = 2477   loss = 4.049991130828857\n","step = 2478   loss = 4.04764986038208\n","step = 2479   loss = 4.045311450958252\n","step = 2480   loss = 4.042975425720215\n","step = 2481   loss = 4.040642261505127\n","step = 2482   loss = 4.038311958312988\n","step = 2483   loss = 4.035984039306641\n","step = 2484   loss = 4.033658981323242\n","step = 2485   loss = 4.031336784362793\n","step = 2486   loss = 4.029017448425293\n","step = 2487   loss = 4.026700496673584\n","step = 2488   loss = 4.024386405944824\n","step = 2489   loss = 4.022075653076172\n","step = 2490   loss = 4.019766330718994\n","step = 2491   loss = 4.017460823059082\n","step = 2492   loss = 4.015157699584961\n","step = 2493   loss = 4.012857437133789\n","step = 2494   loss = 4.01055908203125\n","step = 2495   loss = 4.008264064788818\n","step = 2496   loss = 4.005971908569336\n","step = 2497   loss = 4.003681659698486\n","step = 2498   loss = 4.001394748687744\n","step = 2499   loss = 3.999110221862793\n","step = 2500   loss = 3.996828556060791\n","step = 2501   loss = 3.9945502281188965\n","step = 2502   loss = 3.9922738075256348\n","step = 2503   loss = 3.9900002479553223\n","step = 2504   loss = 3.987729072570801\n","step = 2505   loss = 3.9854607582092285\n","step = 2506   loss = 3.9831953048706055\n","step = 2507   loss = 3.9809322357177734\n","step = 2508   loss = 3.9786722660064697\n","step = 2509   loss = 3.976414442062378\n","step = 2510   loss = 3.9741599559783936\n","step = 2511   loss = 3.971907377243042\n","step = 2512   loss = 3.969658136367798\n","step = 2513   loss = 3.967411518096924\n","step = 2514   loss = 3.965167284011841\n","step = 2515   loss = 3.962925434112549\n","step = 2516   loss = 3.960686445236206\n","step = 2517   loss = 3.9584503173828125\n","step = 2518   loss = 3.956216335296631\n","step = 2519   loss = 3.9539859294891357\n","step = 2520   loss = 3.9517574310302734\n","step = 2521   loss = 3.9495317935943604\n","step = 2522   loss = 3.9473085403442383\n","step = 2523   loss = 3.9450883865356445\n","step = 2524   loss = 3.9428703784942627\n","step = 2525   loss = 3.940655469894409\n","step = 2526   loss = 3.9384429454803467\n","step = 2527   loss = 3.936232805252075\n","step = 2528   loss = 3.934025764465332\n","step = 2529   loss = 3.931821346282959\n","step = 2530   loss = 3.929619073867798\n","step = 2531   loss = 3.927419662475586\n","step = 2532   loss = 3.9252233505249023\n","step = 2533   loss = 3.9230291843414307\n","step = 2534   loss = 3.920837640762329\n","step = 2535   loss = 3.918649196624756\n","step = 2536   loss = 3.9164626598358154\n","step = 2537   loss = 3.914278984069824\n","step = 2538   loss = 3.9120981693267822\n","step = 2539   loss = 3.909919500350952\n","step = 2540   loss = 3.9077439308166504\n","step = 2541   loss = 3.9055700302124023\n","step = 2542   loss = 3.90339994430542\n","step = 2543   loss = 3.9012317657470703\n","step = 2544   loss = 3.89906644821167\n","step = 2545   loss = 3.8969037532806396\n","step = 2546   loss = 3.8947434425354004\n","step = 2547   loss = 3.8925857543945312\n","step = 2548   loss = 3.8904309272766113\n","step = 2549   loss = 3.888278007507324\n","step = 2550   loss = 3.8861279487609863\n","step = 2551   loss = 3.883981227874756\n","step = 2552   loss = 3.881836175918579\n","step = 2553   loss = 3.8796942234039307\n","step = 2554   loss = 3.8775548934936523\n","step = 2555   loss = 3.875417470932007\n","step = 2556   loss = 3.8732831478118896\n","step = 2557   loss = 3.8711514472961426\n","step = 2558   loss = 3.8690218925476074\n","step = 2559   loss = 3.8668947219848633\n","step = 2560   loss = 3.8647704124450684\n","step = 2561   loss = 3.862649917602539\n","step = 2562   loss = 3.860530376434326\n","step = 2563   loss = 3.8584139347076416\n","step = 2564   loss = 3.856300115585327\n","step = 2565   loss = 3.854188919067383\n","step = 2566   loss = 3.8520801067352295\n","step = 2567   loss = 3.849973201751709\n","step = 2568   loss = 3.847869873046875\n","step = 2569   loss = 3.845768690109253\n","step = 2570   loss = 3.843670129776001\n","step = 2571   loss = 3.8415744304656982\n","step = 2572   loss = 3.839480400085449\n","step = 2573   loss = 3.8373892307281494\n","step = 2574   loss = 3.835300922393799\n","step = 2575   loss = 3.83321475982666\n","step = 2576   loss = 3.83113169670105\n","step = 2577   loss = 3.829050302505493\n","step = 2578   loss = 3.826972007751465\n","step = 2579   loss = 3.8248963356018066\n","step = 2580   loss = 3.8228232860565186\n","step = 2581   loss = 3.8207521438598633\n","step = 2582   loss = 3.818683385848999\n","step = 2583   loss = 3.816617965698242\n","step = 2584   loss = 3.8145546913146973\n","step = 2585   loss = 3.8124945163726807\n","step = 2586   loss = 3.8104360103607178\n","step = 2587   loss = 3.808380126953125\n","step = 2588   loss = 3.8063266277313232\n","step = 2589   loss = 3.80427622795105\n","step = 2590   loss = 3.802227735519409\n","step = 2591   loss = 3.800182580947876\n","step = 2592   loss = 3.7981393337249756\n","step = 2593   loss = 3.7960987091064453\n","step = 2594   loss = 3.7940597534179688\n","step = 2595   loss = 3.7920241355895996\n","step = 2596   loss = 3.7899911403656006\n","step = 2597   loss = 3.7879607677459717\n","step = 2598   loss = 3.7859320640563965\n","step = 2599   loss = 3.7839062213897705\n","step = 2600   loss = 3.7818830013275146\n","step = 2601   loss = 3.7798616886138916\n","step = 2602   loss = 3.777843713760376\n","step = 2603   loss = 3.7758278846740723\n","step = 2604   loss = 3.7738142013549805\n","step = 2605   loss = 3.771803379058838\n","step = 2606   loss = 3.7697949409484863\n","step = 2607   loss = 3.767788887023926\n","step = 2608   loss = 3.7657859325408936\n","step = 2609   loss = 3.763784885406494\n","step = 2610   loss = 3.7617859840393066\n","step = 2611   loss = 3.7597897052764893\n","step = 2612   loss = 3.757796287536621\n","step = 2613   loss = 3.7558047771453857\n","step = 2614   loss = 3.7538158893585205\n","step = 2615   loss = 3.7518296241760254\n","step = 2616   loss = 3.749845504760742\n","step = 2617   loss = 3.7478644847869873\n","step = 2618   loss = 3.745884895324707\n","step = 2619   loss = 3.743908405303955\n","step = 2620   loss = 3.7419345378875732\n","step = 2621   loss = 3.7399632930755615\n","step = 2622   loss = 3.7379937171936035\n","step = 2623   loss = 3.7360270023345947\n","step = 2624   loss = 3.734062433242798\n","step = 2625   loss = 3.73210072517395\n","step = 2626   loss = 3.7301409244537354\n","step = 2627   loss = 3.7281835079193115\n","step = 2628   loss = 3.726229190826416\n","step = 2629   loss = 3.7242767810821533\n","step = 2630   loss = 3.7223269939422607\n","step = 2631   loss = 3.720379590988159\n","step = 2632   loss = 3.7184343338012695\n","step = 2633   loss = 3.716491937637329\n","step = 2634   loss = 3.7145516872406006\n","step = 2635   loss = 3.712614059448242\n","step = 2636   loss = 3.710678815841675\n","step = 2637   loss = 3.7087459564208984\n","step = 2638   loss = 3.706814765930176\n","step = 2639   loss = 3.7048871517181396\n","step = 2640   loss = 3.7029616832733154\n","step = 2641   loss = 3.701038122177124\n","step = 2642   loss = 3.699117660522461\n","step = 2643   loss = 3.6971993446350098\n","step = 2644   loss = 3.6952831745147705\n","step = 2645   loss = 3.693368911743164\n","step = 2646   loss = 3.691458225250244\n","step = 2647   loss = 3.6895482540130615\n","step = 2648   loss = 3.6876420974731445\n","step = 2649   loss = 3.6857378482818604\n","step = 2650   loss = 3.683835983276367\n","step = 2651   loss = 3.681936740875244\n","step = 2652   loss = 3.680039167404175\n","step = 2653   loss = 3.678144931793213\n","step = 2654   loss = 3.6762523651123047\n","step = 2655   loss = 3.6743621826171875\n","step = 2656   loss = 3.6724743843078613\n","step = 2657   loss = 3.670588970184326\n","step = 2658   loss = 3.6687064170837402\n","step = 2659   loss = 3.666825532913208\n","step = 2660   loss = 3.664947509765625\n","step = 2661   loss = 3.663071870803833\n","step = 2662   loss = 3.661198377609253\n","step = 2663   loss = 3.6593267917633057\n","step = 2664   loss = 3.657458543777466\n","step = 2665   loss = 3.6555919647216797\n","step = 2666   loss = 3.6537280082702637\n","step = 2667   loss = 3.6518661975860596\n","step = 2668   loss = 3.6500070095062256\n","step = 2669   loss = 3.6481497287750244\n","step = 2670   loss = 3.6462950706481934\n","step = 2671   loss = 3.6444430351257324\n","step = 2672   loss = 3.642592668533325\n","step = 2673   loss = 3.640745162963867\n","step = 2674   loss = 3.638899803161621\n","step = 2675   loss = 3.637056589126587\n","step = 2676   loss = 3.635215997695923\n","step = 2677   loss = 3.63337779045105\n","step = 2678   loss = 3.6315414905548096\n","step = 2679   loss = 3.6297075748443604\n","step = 2680   loss = 3.6278765201568604\n","step = 2681   loss = 3.626047372817993\n","step = 2682   loss = 3.624220609664917\n","step = 2683   loss = 3.622396230697632\n","step = 2684   loss = 3.6205739974975586\n","step = 2685   loss = 3.6187543869018555\n","step = 2686   loss = 3.616936683654785\n","step = 2687   loss = 3.615121841430664\n","step = 2688   loss = 3.6133086681365967\n","step = 2689   loss = 3.6114981174468994\n","step = 2690   loss = 3.609689712524414\n","step = 2691   loss = 3.607883930206299\n","step = 2692   loss = 3.6060802936553955\n","step = 2693   loss = 3.6042795181274414\n","step = 2694   loss = 3.602480173110962\n","step = 2695   loss = 3.6006832122802734\n","step = 2696   loss = 3.598888635635376\n","step = 2697   loss = 3.5970964431762695\n","step = 2698   loss = 3.5953071117401123\n","step = 2699   loss = 3.5935192108154297\n","step = 2700   loss = 3.591733694076538\n","step = 2701   loss = 3.5899505615234375\n","step = 2702   loss = 3.588169574737549\n","step = 2703   loss = 3.5863912105560303\n","step = 2704   loss = 3.5846152305603027\n","step = 2705   loss = 3.582840919494629\n","step = 2706   loss = 3.581068992614746\n","step = 2707   loss = 3.579299211502075\n","step = 2708   loss = 3.5775322914123535\n","step = 2709   loss = 3.5757675170898438\n","step = 2710   loss = 3.574004888534546\n","step = 2711   loss = 3.5722439289093018\n","step = 2712   loss = 3.570486068725586\n","step = 2713   loss = 3.568730354309082\n","step = 2714   loss = 3.566976547241211\n","step = 2715   loss = 3.565225124359131\n","step = 2716   loss = 3.5634765625\n","step = 2717   loss = 3.5617294311523438\n","step = 2718   loss = 3.5599844455718994\n","step = 2719   loss = 3.558242082595825\n","step = 2720   loss = 3.556502342224121\n","step = 2721   loss = 3.5547640323638916\n","step = 2722   loss = 3.5530285835266113\n","step = 2723   loss = 3.551295518875122\n","step = 2724   loss = 3.5495638847351074\n","step = 2725   loss = 3.547834873199463\n","step = 2726   loss = 3.546107769012451\n","step = 2727   loss = 3.5443837642669678\n","step = 2728   loss = 3.542661190032959\n","step = 2729   loss = 3.540940761566162\n","step = 2730   loss = 3.5392229557037354\n","step = 2731   loss = 3.5375072956085205\n","step = 2732   loss = 3.5357937812805176\n","step = 2733   loss = 3.534083366394043\n","step = 2734   loss = 3.5323739051818848\n","step = 2735   loss = 3.530667304992676\n","step = 2736   loss = 3.5289623737335205\n","step = 2737   loss = 3.5272605419158936\n","step = 2738   loss = 3.5255603790283203\n","step = 2739   loss = 3.5238616466522217\n","step = 2740   loss = 3.5221664905548096\n","step = 2741   loss = 3.520472764968872\n","step = 2742   loss = 3.5187816619873047\n","step = 2743   loss = 3.517092704772949\n","step = 2744   loss = 3.5154051780700684\n","step = 2745   loss = 3.513720750808716\n","step = 2746   loss = 3.512038230895996\n","step = 2747   loss = 3.51035737991333\n","step = 2748   loss = 3.5086793899536133\n","step = 2749   loss = 3.5070035457611084\n","step = 2750   loss = 3.5053296089172363\n","step = 2751   loss = 3.5036580562591553\n","step = 2752   loss = 3.5019888877868652\n","step = 2753   loss = 3.500321388244629\n","step = 2754   loss = 3.4986562728881836\n","step = 2755   loss = 3.496993064880371\n","step = 2756   loss = 3.4953322410583496\n","step = 2757   loss = 3.493673801422119\n","step = 2758   loss = 3.4920175075531006\n","step = 2759   loss = 3.490363121032715\n","step = 2760   loss = 3.488710880279541\n","step = 2761   loss = 3.4870612621307373\n","step = 2762   loss = 3.4854137897491455\n","step = 2763   loss = 3.4837682247161865\n","step = 2764   loss = 3.4821243286132812\n","step = 2765   loss = 3.480483055114746\n","step = 2766   loss = 3.478843927383423\n","step = 2767   loss = 3.4772069454193115\n","step = 2768   loss = 3.475571870803833\n","step = 2769   loss = 3.4739391803741455\n","step = 2770   loss = 3.47230863571167\n","step = 2771   loss = 3.4706804752349854\n","step = 2772   loss = 3.4690539836883545\n","step = 2773   loss = 3.4674298763275146\n","step = 2774   loss = 3.4658079147338867\n","step = 2775   loss = 3.46418833732605\n","step = 2776   loss = 3.462570905685425\n","step = 2777   loss = 3.4609549045562744\n","step = 2778   loss = 3.459341526031494\n","step = 2779   loss = 3.4577300548553467\n","step = 2780   loss = 3.456120491027832\n","step = 2781   loss = 3.4545137882232666\n","step = 2782   loss = 3.452908515930176\n","step = 2783   loss = 3.451305866241455\n","step = 2784   loss = 3.449705123901367\n","step = 2785   loss = 3.4481067657470703\n","step = 2786   loss = 3.446510076522827\n","step = 2787   loss = 3.444915771484375\n","step = 2788   loss = 3.4433236122131348\n","step = 2789   loss = 3.4417331218719482\n","step = 2790   loss = 3.440145254135132\n","step = 2791   loss = 3.4385592937469482\n","step = 2792   loss = 3.4369754791259766\n","step = 2793   loss = 3.435394048690796\n","step = 2794   loss = 3.4338138103485107\n","step = 2795   loss = 3.432236909866333\n","step = 2796   loss = 3.430661201477051\n","step = 2797   loss = 3.4290881156921387\n","step = 2798   loss = 3.4275166988372803\n","step = 2799   loss = 3.425947666168213\n","step = 2800   loss = 3.4243805408477783\n","step = 2801   loss = 3.4228155612945557\n","step = 2802   loss = 3.421252727508545\n","step = 2803   loss = 3.419691801071167\n","step = 2804   loss = 3.418133020401001\n","step = 2805   loss = 3.416576623916626\n","step = 2806   loss = 3.4150214195251465\n","step = 2807   loss = 3.413468599319458\n","step = 2808   loss = 3.4119184017181396\n","step = 2809   loss = 3.410369873046875\n","step = 2810   loss = 3.4088242053985596\n","step = 2811   loss = 3.4072797298431396\n","step = 2812   loss = 3.4057374000549316\n","step = 2813   loss = 3.4041974544525146\n","step = 2814   loss = 3.4026594161987305\n","step = 2815   loss = 3.4011237621307373\n","step = 2816   loss = 3.3995895385742188\n","step = 2817   loss = 3.3980581760406494\n","step = 2818   loss = 3.3965277671813965\n","step = 2819   loss = 3.3950002193450928\n","step = 2820   loss = 3.393474817276001\n","step = 2821   loss = 3.391951322555542\n","step = 2822   loss = 3.3904294967651367\n","step = 2823   loss = 3.3889095783233643\n","step = 2824   loss = 3.387392282485962\n","step = 2825   loss = 3.3858766555786133\n","step = 2826   loss = 3.3843631744384766\n","step = 2827   loss = 3.3828516006469727\n","step = 2828   loss = 3.3813421726226807\n","step = 2829   loss = 3.3798348903656006\n","step = 2830   loss = 3.3783299922943115\n","step = 2831   loss = 3.376826524734497\n","step = 2832   loss = 3.3753252029418945\n","step = 2833   loss = 3.373825788497925\n","step = 2834   loss = 3.372328996658325\n","step = 2835   loss = 3.370833396911621\n","step = 2836   loss = 3.3693408966064453\n","step = 2837   loss = 3.367849826812744\n","step = 2838   loss = 3.366360664367676\n","step = 2839   loss = 3.364872932434082\n","step = 2840   loss = 3.3633882999420166\n","step = 2841   loss = 3.3619048595428467\n","step = 2842   loss = 3.3604238033294678\n","step = 2843   loss = 3.358944892883301\n","step = 2844   loss = 3.357468366622925\n","step = 2845   loss = 3.3559930324554443\n","step = 2846   loss = 3.3545196056365967\n","step = 2847   loss = 3.353048801422119\n","step = 2848   loss = 3.3515796661376953\n","step = 2849   loss = 3.3501129150390625\n","step = 2850   loss = 3.3486478328704834\n","step = 2851   loss = 3.347184658050537\n","step = 2852   loss = 3.3457236289978027\n","step = 2853   loss = 3.344264507293701\n","step = 2854   loss = 3.3428072929382324\n","step = 2855   loss = 3.3413519859313965\n","step = 2856   loss = 3.3398990631103516\n","step = 2857   loss = 3.3384478092193604\n","step = 2858   loss = 3.336998701095581\n","step = 2859   loss = 3.3355515003204346\n","step = 2860   loss = 3.3341064453125\n","step = 2861   loss = 3.332663059234619\n","step = 2862   loss = 3.33122181892395\n","step = 2863   loss = 3.329782485961914\n","step = 2864   loss = 3.3283450603485107\n","step = 2865   loss = 3.3269097805023193\n","step = 2866   loss = 3.3254764080047607\n","step = 2867   loss = 3.324045181274414\n","step = 2868   loss = 3.322615623474121\n","step = 2869   loss = 3.321188449859619\n","step = 2870   loss = 3.3197624683380127\n","step = 2871   loss = 3.3183393478393555\n","step = 2872   loss = 3.3169174194335938\n","step = 2873   loss = 3.315497636795044\n","step = 2874   loss = 3.314079999923706\n","step = 2875   loss = 3.31266450881958\n","step = 2876   loss = 3.311250686645508\n","step = 2877   loss = 3.3098387718200684\n","step = 2878   loss = 3.308429002761841\n","step = 2879   loss = 3.307020902633667\n","step = 2880   loss = 3.305614948272705\n","step = 2881   loss = 3.304210901260376\n","step = 2882   loss = 3.3028085231781006\n","step = 2883   loss = 3.301408052444458\n","step = 2884   loss = 3.3000104427337646\n","step = 2885   loss = 3.2986135482788086\n","step = 2886   loss = 3.2972192764282227\n","step = 2887   loss = 3.2958266735076904\n","step = 2888   loss = 3.294435739517212\n","step = 2889   loss = 3.2930474281311035\n","step = 2890   loss = 3.2916605472564697\n","step = 2891   loss = 3.290275812149048\n","step = 2892   loss = 3.2888927459716797\n","step = 2893   loss = 3.2875115871429443\n","step = 2894   loss = 3.2861328125\n","step = 2895   loss = 3.2847554683685303\n","step = 2896   loss = 3.2833800315856934\n","step = 2897   loss = 3.2820067405700684\n","step = 2898   loss = 3.280635356903076\n","step = 2899   loss = 3.279266119003296\n","step = 2900   loss = 3.277897834777832\n","step = 2901   loss = 3.2765326499938965\n","step = 2902   loss = 3.2751686573028564\n","step = 2903   loss = 3.2738068103790283\n","step = 2904   loss = 3.272446632385254\n","step = 2905   loss = 3.2710886001586914\n","step = 2906   loss = 3.2697319984436035\n","step = 2907   loss = 3.2683777809143066\n","step = 2908   loss = 3.2670254707336426\n","step = 2909   loss = 3.2656748294830322\n","step = 2910   loss = 3.2643260955810547\n","step = 2911   loss = 3.262979507446289\n","step = 2912   loss = 3.2616348266601562\n","step = 2913   loss = 3.26029109954834\n","step = 2914   loss = 3.2589502334594727\n","step = 2915   loss = 3.257611036300659\n","step = 2916   loss = 3.2562732696533203\n","step = 2917   loss = 3.2549374103546143\n","step = 2918   loss = 3.2536041736602783\n","step = 2919   loss = 3.252272367477417\n","step = 2920   loss = 3.2509424686431885\n","step = 2921   loss = 3.2496142387390137\n","step = 2922   loss = 3.2482879161834717\n","step = 2923   loss = 3.2469632625579834\n","step = 2924   loss = 3.2456412315368652\n","step = 2925   loss = 3.2443206310272217\n","step = 2926   loss = 3.243001699447632\n","step = 2927   loss = 3.241685152053833\n","step = 2928   loss = 3.2403697967529297\n","step = 2929   loss = 3.2390565872192383\n","step = 2930   loss = 3.2377450466156006\n","step = 2931   loss = 3.236435651779175\n","step = 2932   loss = 3.235128164291382\n","step = 2933   loss = 3.2338221073150635\n","step = 2934   loss = 3.232517957687378\n","step = 2935   loss = 3.2312161922454834\n","step = 2936   loss = 3.2299158573150635\n","step = 2937   loss = 3.2286176681518555\n","step = 2938   loss = 3.227321147918701\n","step = 2939   loss = 3.2260260581970215\n","step = 2940   loss = 3.2247328758239746\n","step = 2941   loss = 3.2234420776367188\n","step = 2942   loss = 3.2221529483795166\n","step = 2943   loss = 3.220865249633789\n","step = 2944   loss = 3.2195796966552734\n","step = 2945   loss = 3.2182958126068115\n","step = 2946   loss = 3.2170138359069824\n","step = 2947   loss = 3.215733528137207\n","step = 2948   loss = 3.2144548892974854\n","step = 2949   loss = 3.2131786346435547\n","step = 2950   loss = 3.211904287338257\n","step = 2951   loss = 3.2106311321258545\n","step = 2952   loss = 3.209360122680664\n","step = 2953   loss = 3.2080907821655273\n","step = 2954   loss = 3.2068231105804443\n","step = 2955   loss = 3.2055575847625732\n","step = 2956   loss = 3.204293727874756\n","step = 2957   loss = 3.203031539916992\n","step = 2958   loss = 3.2017714977264404\n","step = 2959   loss = 3.200512409210205\n","step = 2960   loss = 3.19925594329834\n","step = 2961   loss = 3.1980011463165283\n","step = 2962   loss = 3.1967480182647705\n","step = 2963   loss = 3.1954967975616455\n","step = 2964   loss = 3.1942474842071533\n","step = 2965   loss = 3.1929996013641357\n","step = 2966   loss = 3.191753625869751\n","step = 2967   loss = 3.19050931930542\n","step = 2968   loss = 3.1892666816711426\n","step = 2969   loss = 3.188026189804077\n","step = 2970   loss = 3.1867873668670654\n","step = 2971   loss = 3.1855504512786865\n","step = 2972   loss = 3.1843152046203613\n","step = 2973   loss = 3.1830813884735107\n","step = 2974   loss = 3.181849718093872\n","step = 2975   loss = 3.180619716644287\n","step = 2976   loss = 3.179391384124756\n","step = 2977   loss = 3.1781651973724365\n","step = 2978   loss = 3.1769402027130127\n","step = 2979   loss = 3.175717830657959\n","step = 2980   loss = 3.1744964122772217\n","step = 2981   loss = 3.1732773780822754\n","step = 2982   loss = 3.1720595359802246\n","step = 2983   loss = 3.1708436012268066\n","step = 2984   loss = 3.1696295738220215\n","step = 2985   loss = 3.168417453765869\n","step = 2986   loss = 3.1672067642211914\n","step = 2987   loss = 3.1659979820251465\n","step = 2988   loss = 3.164790630340576\n","step = 2989   loss = 3.1635854244232178\n","step = 2990   loss = 3.162381649017334\n","step = 2991   loss = 3.161179780960083\n","step = 2992   loss = 3.1599795818328857\n","step = 2993   loss = 3.1587815284729004\n","step = 2994   loss = 3.1575844287872314\n","step = 2995   loss = 3.1563894748687744\n","step = 2996   loss = 3.15519642829895\n","step = 2997   loss = 3.1540050506591797\n","step = 2998   loss = 3.152815580368042\n","step = 2999   loss = 3.151627540588379\n","step = 3000   loss = 3.1504406929016113\n","step = 3001   loss = 3.1492559909820557\n","step = 3002   loss = 3.148073434829712\n","step = 3003   loss = 3.1468923091888428\n","step = 3004   loss = 3.1457128524780273\n","step = 3005   loss = 3.1445353031158447\n","step = 3006   loss = 3.143359661102295\n","step = 3007   loss = 3.1421854496002197\n","step = 3008   loss = 3.1410129070281982\n","step = 3009   loss = 3.1398422718048096\n","step = 3010   loss = 3.1386725902557373\n","step = 3011   loss = 3.137505531311035\n","step = 3012   loss = 3.1363396644592285\n","step = 3013   loss = 3.135176181793213\n","step = 3014   loss = 3.1340134143829346\n","step = 3015   loss = 3.1328530311584473\n","step = 3016   loss = 3.1316943168640137\n","step = 3017   loss = 3.1305365562438965\n","step = 3018   loss = 3.1293814182281494\n","step = 3019   loss = 3.128227472305298\n","step = 3020   loss = 3.127075433731079\n","step = 3021   loss = 3.125925302505493\n","step = 3022   loss = 3.1247763633728027\n","step = 3023   loss = 3.123629093170166\n","step = 3024   loss = 3.1224842071533203\n","step = 3025   loss = 3.121340751647949\n","step = 3026   loss = 3.1201987266540527\n","step = 3027   loss = 3.119058132171631\n","step = 3028   loss = 3.117919683456421\n","step = 3029   loss = 3.1167829036712646\n","step = 3030   loss = 3.115647554397583\n","step = 3031   loss = 3.114513874053955\n","step = 3032   loss = 3.1133816242218018\n","step = 3033   loss = 3.1122515201568604\n","step = 3034   loss = 3.1111233234405518\n","step = 3035   loss = 3.1099963188171387\n","step = 3036   loss = 3.1088707447052\n","step = 3037   loss = 3.1077475547790527\n","step = 3038   loss = 3.1066253185272217\n","step = 3039   loss = 3.1055054664611816\n","step = 3040   loss = 3.104386568069458\n","step = 3041   loss = 3.1032700538635254\n","step = 3042   loss = 3.102154493331909\n","step = 3043   loss = 3.1010406017303467\n","step = 3044   loss = 3.099928855895996\n","step = 3045   loss = 3.098818302154541\n","step = 3046   loss = 3.097709894180298\n","step = 3047   loss = 3.096602201461792\n","step = 3048   loss = 3.0954971313476562\n","step = 3049   loss = 3.094393253326416\n","step = 3050   loss = 3.0932912826538086\n","step = 3051   loss = 3.092191219329834\n","step = 3052   loss = 3.091092586517334\n","step = 3053   loss = 3.0899949073791504\n","step = 3054   loss = 3.088899612426758\n","step = 3055   loss = 3.0878055095672607\n","step = 3056   loss = 3.0867133140563965\n","step = 3057   loss = 3.0856223106384277\n","step = 3058   loss = 3.08453369140625\n","step = 3059   loss = 3.0834460258483887\n","step = 3060   loss = 3.08236026763916\n","step = 3061   loss = 3.0812761783599854\n","step = 3062   loss = 3.080193519592285\n","step = 3063   loss = 3.0791122913360596\n","step = 3064   loss = 3.078033447265625\n","step = 3065   loss = 3.076955795288086\n","step = 3066   loss = 3.0758795738220215\n","step = 3067   loss = 3.0748050212860107\n","step = 3068   loss = 3.073732376098633\n","step = 3069   loss = 3.0726611614227295\n","step = 3070   loss = 3.071591377258301\n","step = 3071   loss = 3.0705230236053467\n","step = 3072   loss = 3.0694568157196045\n","step = 3073   loss = 3.068392038345337\n","step = 3074   loss = 3.067328691482544\n","step = 3075   loss = 3.066267490386963\n","step = 3076   loss = 3.0652074813842773\n","step = 3077   loss = 3.0641489028930664\n","step = 3078   loss = 3.063091993331909\n","step = 3079   loss = 3.0620367527008057\n","step = 3080   loss = 3.060983180999756\n","step = 3081   loss = 3.0599310398101807\n","step = 3082   loss = 3.0588808059692383\n","step = 3083   loss = 3.0578320026397705\n","step = 3084   loss = 3.0567843914031982\n","step = 3085   loss = 3.055738925933838\n","step = 3086   loss = 3.054694890975952\n","step = 3087   loss = 3.053652048110962\n","step = 3088   loss = 3.0526111125946045\n","step = 3089   loss = 3.051572322845459\n","step = 3090   loss = 3.0505340099334717\n","step = 3091   loss = 3.049497365951538\n","step = 3092   loss = 3.0484628677368164\n","step = 3093   loss = 3.0474298000335693\n","step = 3094   loss = 3.046398639678955\n","step = 3095   loss = 3.045367956161499\n","step = 3096   loss = 3.044339895248413\n","step = 3097   loss = 3.0433132648468018\n","step = 3098   loss = 3.042287826538086\n","step = 3099   loss = 3.041264057159424\n","step = 3100   loss = 3.0402417182922363\n","step = 3101   loss = 3.0392212867736816\n","step = 3102   loss = 3.0382022857666016\n","step = 3103   loss = 3.037184715270996\n","step = 3104   loss = 3.0361688137054443\n","step = 3105   loss = 3.0351548194885254\n","step = 3106   loss = 3.034141778945923\n","step = 3107   loss = 3.033130407333374\n","step = 3108   loss = 3.032120704650879\n","step = 3109   loss = 3.0311126708984375\n","step = 3110   loss = 3.03010630607605\n","step = 3111   loss = 3.0291004180908203\n","step = 3112   loss = 3.028097152709961\n","step = 3113   loss = 3.027095079421997\n","step = 3114   loss = 3.026094436645508\n","step = 3115   loss = 3.025095224380493\n","step = 3116   loss = 3.0240981578826904\n","step = 3117   loss = 3.023102045059204\n","step = 3118   loss = 3.0221078395843506\n","step = 3119   loss = 3.0211150646209717\n","step = 3120   loss = 3.0201234817504883\n","step = 3121   loss = 3.0191338062286377\n","step = 3122   loss = 3.018145799636841\n","step = 3123   loss = 3.0171587467193604\n","step = 3124   loss = 3.0161733627319336\n","step = 3125   loss = 3.0151898860931396\n","step = 3126   loss = 3.014207363128662\n","step = 3127   loss = 3.013226270675659\n","step = 3128   loss = 3.0122475624084473\n","step = 3129   loss = 3.011270046234131\n","step = 3130   loss = 3.010293960571289\n","step = 3131   loss = 3.0093190670013428\n","step = 3132   loss = 3.00834584236145\n","step = 3133   loss = 3.0073742866516113\n","step = 3134   loss = 3.006404399871826\n","step = 3135   loss = 3.0054357051849365\n","step = 3136   loss = 3.0044684410095215\n","step = 3137   loss = 3.00350284576416\n","step = 3138   loss = 3.0025386810302734\n","step = 3139   loss = 3.0015759468078613\n","step = 3140   loss = 3.000614643096924\n","step = 3141   loss = 2.999655246734619\n","step = 3142   loss = 2.998697280883789\n","step = 3143   loss = 2.9977402687072754\n","step = 3144   loss = 2.9967851638793945\n","step = 3145   loss = 2.995831251144409\n","step = 3146   loss = 2.9948787689208984\n","step = 3147   loss = 2.9939281940460205\n","step = 3148   loss = 2.992979049682617\n","step = 3149   loss = 2.9920313358306885\n","step = 3150   loss = 2.9910848140716553\n","step = 3151   loss = 2.990140438079834\n","step = 3152   loss = 2.98919677734375\n","step = 3153   loss = 2.988255023956299\n","step = 3154   loss = 2.987314462661743\n","step = 3155   loss = 2.9863758087158203\n","step = 3156   loss = 2.9854378700256348\n","step = 3157   loss = 2.984501838684082\n","step = 3158   loss = 2.983567476272583\n","step = 3159   loss = 2.9826343059539795\n","step = 3160   loss = 2.981703281402588\n","step = 3161   loss = 2.9807727336883545\n","step = 3162   loss = 2.9798433780670166\n","step = 3163   loss = 2.978916883468628\n","step = 3164   loss = 2.9779906272888184\n","step = 3165   loss = 2.9770660400390625\n","step = 3166   loss = 2.9761435985565186\n","step = 3167   loss = 2.975221872329712\n","step = 3168   loss = 2.974302053451538\n","step = 3169   loss = 2.9733834266662598\n","step = 3170   loss = 2.972466468811035\n","step = 3171   loss = 2.971550941467285\n","step = 3172   loss = 2.9706366062164307\n","step = 3173   loss = 2.969723701477051\n","step = 3174   loss = 2.9688122272491455\n","step = 3175   loss = 2.967902183532715\n","step = 3176   loss = 2.966994047164917\n","step = 3177   loss = 2.9660871028900146\n","step = 3178   loss = 2.965181589126587\n","step = 3179   loss = 2.9642772674560547\n","step = 3180   loss = 2.963374614715576\n","step = 3181   loss = 2.9624733924865723\n","step = 3182   loss = 2.961573600769043\n","step = 3183   loss = 2.96067476272583\n","step = 3184   loss = 2.95977783203125\n","step = 3185   loss = 2.9588820934295654\n","step = 3186   loss = 2.9579880237579346\n","step = 3187   loss = 2.957095146179199\n","step = 3188   loss = 2.9562039375305176\n","step = 3189   loss = 2.9553139209747314\n","step = 3190   loss = 2.9544260501861572\n","step = 3191   loss = 2.953538417816162\n","step = 3192   loss = 2.9526526927948\n","step = 3193   loss = 2.951768398284912\n","step = 3194   loss = 2.950885534286499\n","step = 3195   loss = 2.9500038623809814\n","step = 3196   loss = 2.9491238594055176\n","step = 3197   loss = 2.9482455253601074\n","step = 3198   loss = 2.9473681449890137\n","step = 3199   loss = 2.9464919567108154\n","step = 3200   loss = 2.94561767578125\n","step = 3201   loss = 2.94474458694458\n","step = 3202   loss = 2.9438729286193848\n","step = 3203   loss = 2.943002223968506\n","step = 3204   loss = 2.9421334266662598\n","step = 3205   loss = 2.941265821456909\n","step = 3206   loss = 2.9404001235961914\n","step = 3207   loss = 2.9395358562469482\n","step = 3208   loss = 2.9386720657348633\n","step = 3209   loss = 2.9378104209899902\n","step = 3210   loss = 2.9369494915008545\n","step = 3211   loss = 2.9360907077789307\n","step = 3212   loss = 2.935232639312744\n","step = 3213   loss = 2.9343762397766113\n","step = 3214   loss = 2.933520793914795\n","step = 3215   loss = 2.9326672554016113\n","step = 3216   loss = 2.9318151473999023\n","step = 3217   loss = 2.930964469909668\n","step = 3218   loss = 2.930114507675171\n","step = 3219   loss = 2.9292664527893066\n","step = 3220   loss = 2.928419589996338\n","step = 3221   loss = 2.9275741577148438\n","step = 3222   loss = 2.926730155944824\n","step = 3223   loss = 2.925887107849121\n","step = 3224   loss = 2.9250454902648926\n","step = 3225   loss = 2.9242055416107178\n","step = 3226   loss = 2.9233672618865967\n","step = 3227   loss = 2.922529697418213\n","step = 3228   loss = 2.921694040298462\n","step = 3229   loss = 2.9208590984344482\n","step = 3230   loss = 2.9200263023376465\n","step = 3231   loss = 2.919193983078003\n","step = 3232   loss = 2.918363571166992\n","step = 3233   loss = 2.917534589767456\n","step = 3234   loss = 2.9167065620422363\n","step = 3235   loss = 2.9158804416656494\n","step = 3236   loss = 2.9150550365448\n","step = 3237   loss = 2.914231061935425\n","step = 3238   loss = 2.9134087562561035\n","step = 3239   loss = 2.9125869274139404\n","step = 3240   loss = 2.9117674827575684\n","step = 3241   loss = 2.910949468612671\n","step = 3242   loss = 2.9101321697235107\n","step = 3243   loss = 2.909316301345825\n","step = 3244   loss = 2.9085018634796143\n","step = 3245   loss = 2.907688617706299\n","step = 3246   loss = 2.906876802444458\n","step = 3247   loss = 2.906066656112671\n","step = 3248   loss = 2.905257225036621\n","step = 3249   loss = 2.904449224472046\n","step = 3250   loss = 2.9036428928375244\n","step = 3251   loss = 2.9028375148773193\n","step = 3252   loss = 2.902034044265747\n","step = 3253   loss = 2.901231050491333\n","step = 3254   loss = 2.9004299640655518\n","step = 3255   loss = 2.899630069732666\n","step = 3256   loss = 2.8988311290740967\n","step = 3257   loss = 2.898033857345581\n","step = 3258   loss = 2.897238254547119\n","step = 3259   loss = 2.8964428901672363\n","step = 3260   loss = 2.8956496715545654\n","step = 3261   loss = 2.894857883453369\n","step = 3262   loss = 2.89406681060791\n","step = 3263   loss = 2.893277645111084\n","step = 3264   loss = 2.892489433288574\n","step = 3265   loss = 2.891702175140381\n","step = 3266   loss = 2.8909168243408203\n","step = 3267   loss = 2.890132427215576\n","step = 3268   loss = 2.8893494606018066\n","step = 3269   loss = 2.8885679244995117\n","step = 3270   loss = 2.8877878189086914\n","step = 3271   loss = 2.88700795173645\n","step = 3272   loss = 2.88623046875\n","step = 3273   loss = 2.8854541778564453\n","step = 3274   loss = 2.884678840637207\n","step = 3275   loss = 2.883904457092285\n","step = 3276   loss = 2.883131980895996\n","step = 3277   loss = 2.8823604583740234\n","step = 3278   loss = 2.8815903663635254\n","step = 3279   loss = 2.880821466445923\n","step = 3280   loss = 2.880053997039795\n","step = 3281   loss = 2.8792877197265625\n","step = 3282   loss = 2.8785223960876465\n","step = 3283   loss = 2.877758502960205\n","step = 3284   loss = 2.8769965171813965\n","step = 3285   loss = 2.876235008239746\n","step = 3286   loss = 2.8754751682281494\n","step = 3287   loss = 2.874716281890869\n","step = 3288   loss = 2.8739593029022217\n","step = 3289   loss = 2.8732030391693115\n","step = 3290   loss = 2.872448205947876\n","step = 3291   loss = 2.871694326400757\n","step = 3292   loss = 2.8709421157836914\n","step = 3293   loss = 2.8701906204223633\n","step = 3294   loss = 2.869441270828247\n","step = 3295   loss = 2.868691921234131\n","step = 3296   loss = 2.8679449558258057\n","step = 3297   loss = 2.8671987056732178\n","step = 3298   loss = 2.8664538860321045\n","step = 3299   loss = 2.865710496902466\n","step = 3300   loss = 2.8649680614471436\n","step = 3301   loss = 2.864227056503296\n","step = 3302   loss = 2.8634865283966064\n","step = 3303   loss = 2.862748384475708\n","step = 3304   loss = 2.862011194229126\n","step = 3305   loss = 2.8612747192382812\n","step = 3306   loss = 2.8605399131774902\n","step = 3307   loss = 2.859806537628174\n","step = 3308   loss = 2.8590738773345947\n","step = 3309   loss = 2.858342170715332\n","step = 3310   loss = 2.857612371444702\n","step = 3311   loss = 2.856884241104126\n","step = 3312   loss = 2.856156349182129\n","step = 3313   loss = 2.8554296493530273\n","step = 3314   loss = 2.8547046184539795\n","step = 3315   loss = 2.8539810180664062\n","step = 3316   loss = 2.8532581329345703\n","step = 3317   loss = 2.852536916732788\n","step = 3318   loss = 2.8518171310424805\n","step = 3319   loss = 2.85109806060791\n","step = 3320   loss = 2.8503799438476562\n","step = 3321   loss = 2.849663734436035\n","step = 3322   loss = 2.8489484786987305\n","step = 3323   loss = 2.848234176635742\n","step = 3324   loss = 2.8475213050842285\n","step = 3325   loss = 2.8468096256256104\n","step = 3326   loss = 2.8460991382598877\n","step = 3327   loss = 2.8453896045684814\n","step = 3328   loss = 2.844681978225708\n","step = 3329   loss = 2.8439748287200928\n","step = 3330   loss = 2.8432693481445312\n","step = 3331   loss = 2.8425650596618652\n","step = 3332   loss = 2.8418619632720947\n","step = 3333   loss = 2.8411593437194824\n","step = 3334   loss = 2.840458393096924\n","step = 3335   loss = 2.83975887298584\n","step = 3336   loss = 2.8390603065490723\n","step = 3337   loss = 2.8383634090423584\n","step = 3338   loss = 2.837667226791382\n","step = 3339   loss = 2.8369719982147217\n","step = 3340   loss = 2.8362784385681152\n","step = 3341   loss = 2.835585832595825\n","step = 3342   loss = 2.8348946571350098\n","step = 3343   loss = 2.83420467376709\n","step = 3344   loss = 2.8335154056549072\n","step = 3345   loss = 2.8328280448913574\n","step = 3346   loss = 2.832141160964966\n","step = 3347   loss = 2.831455707550049\n","step = 3348   loss = 2.8307714462280273\n","step = 3349   loss = 2.8300881385803223\n","step = 3350   loss = 2.82940673828125\n","step = 3351   loss = 2.828725814819336\n","step = 3352   loss = 2.8280463218688965\n","step = 3353   loss = 2.8273677825927734\n","step = 3354   loss = 2.826690673828125\n","step = 3355   loss = 2.826014757156372\n","step = 3356   loss = 2.8253395557403564\n","step = 3357   loss = 2.8246660232543945\n","step = 3358   loss = 2.823993444442749\n","step = 3359   loss = 2.823322057723999\n","step = 3360   loss = 2.8226518630981445\n","step = 3361   loss = 2.8219826221466064\n","step = 3362   loss = 2.821315050125122\n","step = 3363   loss = 2.820648193359375\n","step = 3364   loss = 2.8199822902679443\n","step = 3365   loss = 2.8193178176879883\n","step = 3366   loss = 2.818654775619507\n","step = 3367   loss = 2.8179924488067627\n","step = 3368   loss = 2.8173317909240723\n","step = 3369   loss = 2.816671371459961\n","step = 3370   loss = 2.8160130977630615\n","step = 3371   loss = 2.8153553009033203\n","step = 3372   loss = 2.8146986961364746\n","step = 3373   loss = 2.8140435218811035\n","step = 3374   loss = 2.813389301300049\n","step = 3375   loss = 2.812736749649048\n","step = 3376   loss = 2.812084436416626\n","step = 3377   loss = 2.811434030532837\n","step = 3378   loss = 2.8107845783233643\n","step = 3379   loss = 2.810136079788208\n","step = 3380   loss = 2.809488296508789\n","step = 3381   loss = 2.808842420578003\n","step = 3382   loss = 2.808197259902954\n","step = 3383   loss = 2.8075530529022217\n","step = 3384   loss = 2.806910753250122\n","step = 3385   loss = 2.8062689304351807\n","step = 3386   loss = 2.8056282997131348\n","step = 3387   loss = 2.8049886226654053\n","step = 3388   loss = 2.8043506145477295\n","step = 3389   loss = 2.803713083267212\n","step = 3390   loss = 2.803077459335327\n","step = 3391   loss = 2.8024423122406006\n","step = 3392   loss = 2.8018081188201904\n","step = 3393   loss = 2.801175832748413\n","step = 3394   loss = 2.800544023513794\n","step = 3395   loss = 2.7999136447906494\n","step = 3396   loss = 2.799283742904663\n","step = 3397   loss = 2.7986559867858887\n","step = 3398   loss = 2.7980282306671143\n","step = 3399   loss = 2.7974023818969727\n","step = 3400   loss = 2.7967777252197266\n","step = 3401   loss = 2.7961535453796387\n","step = 3402   loss = 2.7955310344696045\n","step = 3403   loss = 2.7949090003967285\n","step = 3404   loss = 2.794288396835327\n","step = 3405   loss = 2.793668746948242\n","step = 3406   loss = 2.793050527572632\n","step = 3407   loss = 2.792433261871338\n","step = 3408   loss = 2.7918174266815186\n","step = 3409   loss = 2.7912020683288574\n","step = 3410   loss = 2.790588140487671\n","step = 3411   loss = 2.7899746894836426\n","step = 3412   loss = 2.789362907409668\n","step = 3413   loss = 2.7887520790100098\n","step = 3414   loss = 2.788142681121826\n","step = 3415   loss = 2.787533760070801\n","step = 3416   loss = 2.78692626953125\n","step = 3417   loss = 2.7863199710845947\n","step = 3418   loss = 2.785714626312256\n","step = 3419   loss = 2.7851104736328125\n","step = 3420   loss = 2.7845070362091064\n","step = 3421   loss = 2.783905029296875\n","step = 3422   loss = 2.783303737640381\n","step = 3423   loss = 2.7827038764953613\n","step = 3424   loss = 2.7821052074432373\n","step = 3425   loss = 2.7815070152282715\n","step = 3426   loss = 2.7809102535247803\n","step = 3427   loss = 2.7803146839141846\n","step = 3428   loss = 2.7797200679779053\n","step = 3429   loss = 2.7791266441345215\n","step = 3430   loss = 2.778533697128296\n","step = 3431   loss = 2.777942180633545\n","step = 3432   loss = 2.7773520946502686\n","step = 3433   loss = 2.7767624855041504\n","step = 3434   loss = 2.776174545288086\n","step = 3435   loss = 2.7755870819091797\n","step = 3436   loss = 2.77500057220459\n","step = 3437   loss = 2.7744154930114746\n","step = 3438   loss = 2.773831367492676\n","step = 3439   loss = 2.7732486724853516\n","step = 3440   loss = 2.7726669311523438\n","step = 3441   loss = 2.7720859050750732\n","step = 3442   loss = 2.7715060710906982\n","step = 3443   loss = 2.7709271907806396\n","step = 3444   loss = 2.7703495025634766\n","step = 3445   loss = 2.7697722911834717\n","step = 3446   loss = 2.7691965103149414\n","step = 3447   loss = 2.768622398376465\n","step = 3448   loss = 2.7680487632751465\n","step = 3449   loss = 2.7674758434295654\n","step = 3450   loss = 2.766904354095459\n","step = 3451   loss = 2.766333818435669\n","step = 3452   loss = 2.7657647132873535\n","step = 3453   loss = 2.765195846557617\n","step = 3454   loss = 2.7646286487579346\n","step = 3455   loss = 2.7640621662139893\n","step = 3456   loss = 2.7634966373443604\n","step = 3457   loss = 2.762932538986206\n","step = 3458   loss = 2.762369155883789\n","step = 3459   loss = 2.7618067264556885\n","step = 3460   loss = 2.7612454891204834\n","step = 3461   loss = 2.7606847286224365\n","step = 3462   loss = 2.7601263523101807\n","step = 3463   loss = 2.7595674991607666\n","step = 3464   loss = 2.7590110301971436\n","step = 3465   loss = 2.7584545612335205\n","step = 3466   loss = 2.757899522781372\n","step = 3467   loss = 2.757345199584961\n","step = 3468   loss = 2.756791591644287\n","step = 3469   loss = 2.756239652633667\n","step = 3470   loss = 2.7556886672973633\n","step = 3471   loss = 2.755138635635376\n","step = 3472   loss = 2.754589319229126\n","step = 3473   loss = 2.7540411949157715\n","step = 3474   loss = 2.7534940242767334\n","step = 3475   loss = 2.752948045730591\n","step = 3476   loss = 2.7524025440216064\n","step = 3477   loss = 2.751858711242676\n","step = 3478   loss = 2.7513158321380615\n","step = 3479   loss = 2.7507736682891846\n","step = 3480   loss = 2.750232458114624\n","step = 3481   loss = 2.749692440032959\n","step = 3482   loss = 2.7491533756256104\n","step = 3483   loss = 2.74861478805542\n","step = 3484   loss = 2.7480781078338623\n","step = 3485   loss = 2.747542142868042\n","step = 3486   loss = 2.747006416320801\n","step = 3487   loss = 2.7464723587036133\n","step = 3488   loss = 2.745939016342163\n","step = 3489   loss = 2.7454068660736084\n","step = 3490   loss = 2.744875431060791\n","step = 3491   loss = 2.7443456649780273\n","step = 3492   loss = 2.743816614151001\n","step = 3493   loss = 2.743288040161133\n","step = 3494   loss = 2.7427608966827393\n","step = 3495   loss = 2.742234230041504\n","step = 3496   loss = 2.741708993911743\n","step = 3497   loss = 2.741184711456299\n","step = 3498   loss = 2.740661382675171\n","step = 3499   loss = 2.7401392459869385\n","step = 3500   loss = 2.739617347717285\n","step = 3501   loss = 2.7390966415405273\n","step = 3502   loss = 2.738577365875244\n","step = 3503   loss = 2.7380588054656982\n","step = 3504   loss = 2.7375409603118896\n","step = 3505   loss = 2.7370243072509766\n","step = 3506   loss = 2.736508846282959\n","step = 3507   loss = 2.7359938621520996\n","step = 3508   loss = 2.7354800701141357\n","step = 3509   loss = 2.734966993331909\n","step = 3510   loss = 2.7344555854797363\n","step = 3511   loss = 2.7339444160461426\n","step = 3512   loss = 2.7334346771240234\n","step = 3513   loss = 2.7329256534576416\n","step = 3514   loss = 2.7324178218841553\n","step = 3515   loss = 2.7319107055664062\n","step = 3516   loss = 2.7314047813415527\n","step = 3517   loss = 2.7308993339538574\n","step = 3518   loss = 2.7303953170776367\n","step = 3519   loss = 2.7298922538757324\n","step = 3520   loss = 2.7293896675109863\n","step = 3521   loss = 2.7288882732391357\n","step = 3522   loss = 2.7283878326416016\n","step = 3523   loss = 2.7278878688812256\n","step = 3524   loss = 2.7273895740509033\n","step = 3525   loss = 2.7268917560577393\n","step = 3526   loss = 2.72639536857605\n","step = 3527   loss = 2.7258989810943604\n","step = 3528   loss = 2.7254042625427246\n","step = 3529   loss = 2.724910259246826\n","step = 3530   loss = 2.7244174480438232\n","step = 3531   loss = 2.7239251136779785\n","step = 3532   loss = 2.7234342098236084\n","step = 3533   loss = 2.7229437828063965\n","step = 3534   loss = 2.722454786300659\n","step = 3535   loss = 2.721966505050659\n","step = 3536   loss = 2.7214789390563965\n","step = 3537   loss = 2.7209928035736084\n","step = 3538   loss = 2.7205069065093994\n","step = 3539   loss = 2.720022201538086\n","step = 3540   loss = 2.719538688659668\n","step = 3541   loss = 2.7190558910369873\n","step = 3542   loss = 2.718573570251465\n","step = 3543   loss = 2.718092918395996\n","step = 3544   loss = 2.7176125049591064\n","step = 3545   loss = 2.7171337604522705\n","step = 3546   loss = 2.7166554927825928\n","step = 3547   loss = 2.7161779403686523\n","step = 3548   loss = 2.7157015800476074\n","step = 3549   loss = 2.715226173400879\n","step = 3550   loss = 2.7147512435913086\n","step = 3551   loss = 2.714277744293213\n","step = 3552   loss = 2.7138047218322754\n","step = 3553   loss = 2.7133331298828125\n","step = 3554   loss = 2.712862014770508\n","step = 3555   loss = 2.7123918533325195\n","step = 3556   loss = 2.7119226455688477\n","step = 3557   loss = 2.711454153060913\n","step = 3558   loss = 2.710986852645874\n","step = 3559   loss = 2.7105207443237305\n","step = 3560   loss = 2.710054874420166\n","step = 3561   loss = 2.709590196609497\n","step = 3562   loss = 2.7091259956359863\n","step = 3563   loss = 2.70866322517395\n","step = 3564   loss = 2.7082014083862305\n","step = 3565   loss = 2.70773983001709\n","step = 3566   loss = 2.707279920578003\n","step = 3567   loss = 2.706820487976074\n","step = 3568   loss = 2.706361770629883\n","step = 3569   loss = 2.705904245376587\n","step = 3570   loss = 2.7054474353790283\n","step = 3571   loss = 2.7049918174743652\n","step = 3572   loss = 2.704536199569702\n","step = 3573   loss = 2.7040822505950928\n","step = 3574   loss = 2.7036292552948\n","step = 3575   loss = 2.703176975250244\n","step = 3576   loss = 2.7027251720428467\n","step = 3577   loss = 2.702274799346924\n","step = 3578   loss = 2.70182466506958\n","step = 3579   loss = 2.701375961303711\n","step = 3580   loss = 2.700927972793579\n","step = 3581   loss = 2.7004806995391846\n","step = 3582   loss = 2.7000343799591064\n","step = 3583   loss = 2.699589252471924\n","step = 3584   loss = 2.6991446018218994\n","step = 3585   loss = 2.6987006664276123\n","step = 3586   loss = 2.6982581615448\n","step = 3587   loss = 2.6978158950805664\n","step = 3588   loss = 2.6973748207092285\n","step = 3589   loss = 2.696934461593628\n","step = 3590   loss = 2.6964950561523438\n","step = 3591   loss = 2.696056842803955\n","step = 3592   loss = 2.6956191062927246\n","step = 3593   loss = 2.6951820850372314\n","step = 3594   loss = 2.6947460174560547\n","step = 3595   loss = 2.6943111419677734\n","step = 3596   loss = 2.6938767433166504\n","step = 3597   loss = 2.6934432983398438\n","step = 3598   loss = 2.6930108070373535\n","step = 3599   loss = 2.6925790309906006\n","step = 3600   loss = 2.692148447036743\n","step = 3601   loss = 2.691718339920044\n","step = 3602   loss = 2.691288948059082\n","step = 3603   loss = 2.6908605098724365\n","step = 3604   loss = 2.6904330253601074\n","step = 3605   loss = 2.6900060176849365\n","step = 3606   loss = 2.6895806789398193\n","step = 3607   loss = 2.6891555786132812\n","step = 3608   loss = 2.6887314319610596\n","step = 3609   loss = 2.688307762145996\n","step = 3610   loss = 2.6878855228424072\n","step = 3611   loss = 2.6874637603759766\n","step = 3612   loss = 2.6870429515838623\n","step = 3613   loss = 2.6866228580474854\n","step = 3614   loss = 2.686203718185425\n","step = 3615   loss = 2.6857848167419434\n","step = 3616   loss = 2.6853678226470947\n","step = 3617   loss = 2.684951066970825\n","step = 3618   loss = 2.684535264968872\n","step = 3619   loss = 2.6841201782226562\n","step = 3620   loss = 2.6837053298950195\n","step = 3621   loss = 2.6832926273345947\n","step = 3622   loss = 2.682879686355591\n","step = 3623   loss = 2.6824681758880615\n","step = 3624   loss = 2.6820569038391113\n","step = 3625   loss = 2.6816470623016357\n","step = 3626   loss = 2.6812374591827393\n","step = 3627   loss = 2.680828809738159\n","step = 3628   loss = 2.6804208755493164\n","step = 3629   loss = 2.6800143718719482\n","step = 3630   loss = 2.679608106613159\n","step = 3631   loss = 2.6792025566101074\n","step = 3632   loss = 2.678798198699951\n","step = 3633   loss = 2.6783945560455322\n","step = 3634   loss = 2.6779918670654297\n","step = 3635   loss = 2.6775896549224854\n","step = 3636   loss = 2.6771881580352783\n","step = 3637   loss = 2.6767876148223877\n","step = 3638   loss = 2.6763882637023926\n","step = 3639   loss = 2.6759893894195557\n","step = 3640   loss = 2.675591468811035\n","step = 3641   loss = 2.675194025039673\n","step = 3642   loss = 2.674797296524048\n","step = 3643   loss = 2.67440128326416\n","step = 3644   loss = 2.674006938934326\n","step = 3645   loss = 2.6736130714416504\n","step = 3646   loss = 2.6732192039489746\n","step = 3647   loss = 2.6728267669677734\n","step = 3648   loss = 2.6724350452423096\n","step = 3649   loss = 2.672044038772583\n","step = 3650   loss = 2.6716537475585938\n","step = 3651   loss = 2.6712646484375\n","step = 3652   loss = 2.6708757877349854\n","step = 3653   loss = 2.6704883575439453\n","step = 3654   loss = 2.670100688934326\n","step = 3655   loss = 2.6697146892547607\n","step = 3656   loss = 2.6693291664123535\n","step = 3657   loss = 2.6689445972442627\n","step = 3658   loss = 2.668560743331909\n","step = 3659   loss = 2.668177604675293\n","step = 3660   loss = 2.667794704437256\n","step = 3661   loss = 2.6674132347106934\n","step = 3662   loss = 2.6670327186584473\n","step = 3663   loss = 2.6666529178619385\n","step = 3664   loss = 2.6662731170654297\n","step = 3665   loss = 2.6658949851989746\n","step = 3666   loss = 2.6655168533325195\n","step = 3667   loss = 2.665140151977539\n","step = 3668   loss = 2.6647636890411377\n","step = 3669   loss = 2.664388418197632\n","step = 3670   loss = 2.664013624191284\n","step = 3671   loss = 2.663639545440674\n","step = 3672   loss = 2.663266181945801\n","step = 3673   loss = 2.662893295288086\n","step = 3674   loss = 2.662522315979004\n","step = 3675   loss = 2.6621510982513428\n","step = 3676   loss = 2.6617813110351562\n","step = 3677   loss = 2.661411762237549\n","step = 3678   loss = 2.661043167114258\n","step = 3679   loss = 2.660675287246704\n","step = 3680   loss = 2.6603078842163086\n","step = 3681   loss = 2.6599416732788086\n","step = 3682   loss = 2.659576177597046\n","step = 3683   loss = 2.6592111587524414\n","step = 3684   loss = 2.6588470935821533\n","step = 3685   loss = 2.6584835052490234\n","step = 3686   loss = 2.658120632171631\n","step = 3687   loss = 2.657759189605713\n","step = 3688   loss = 2.657397985458374\n","step = 3689   loss = 2.6570372581481934\n","step = 3690   loss = 2.656677484512329\n","step = 3691   loss = 2.6563186645507812\n","step = 3692   loss = 2.6559603214263916\n","step = 3693   loss = 2.6556029319763184\n","step = 3694   loss = 2.6552462577819824\n","step = 3695   loss = 2.654890537261963\n","step = 3696   loss = 2.6545352935791016\n","step = 3697   loss = 2.6541802883148193\n","step = 3698   loss = 2.6538267135620117\n","step = 3699   loss = 2.6534736156463623\n","step = 3700   loss = 2.653120994567871\n","step = 3701   loss = 2.6527698040008545\n","step = 3702   loss = 2.652418851852417\n","step = 3703   loss = 2.652068853378296\n","step = 3704   loss = 2.651718854904175\n","step = 3705   loss = 2.6513707637786865\n","step = 3706   loss = 2.6510226726531982\n","step = 3707   loss = 2.650675058364868\n","step = 3708   loss = 2.6503286361694336\n","step = 3709   loss = 2.6499826908111572\n","step = 3710   loss = 2.649637460708618\n","step = 3711   loss = 2.6492931842803955\n","step = 3712   loss = 2.648949384689331\n","step = 3713   loss = 2.648606300354004\n","step = 3714   loss = 2.648263931274414\n","step = 3715   loss = 2.6479227542877197\n","step = 3716   loss = 2.6475820541381836\n","step = 3717   loss = 2.6472415924072266\n","step = 3718   loss = 2.646902084350586\n","step = 3719   loss = 2.646563768386841\n","step = 3720   loss = 2.6462252140045166\n","step = 3721   loss = 2.645888090133667\n","step = 3722   loss = 2.6455512046813965\n","step = 3723   loss = 2.6452155113220215\n","step = 3724   loss = 2.6448802947998047\n","step = 3725   loss = 2.644545793533325\n","step = 3726   loss = 2.644212007522583\n","step = 3727   loss = 2.6438791751861572\n","step = 3728   loss = 2.6435468196868896\n","step = 3729   loss = 2.643214702606201\n","step = 3730   loss = 2.6428840160369873\n","step = 3731   loss = 2.6425533294677734\n","step = 3732   loss = 2.642223834991455\n","step = 3733   loss = 2.641895055770874\n","step = 3734   loss = 2.641566753387451\n","step = 3735   loss = 2.6412389278411865\n","step = 3736   loss = 2.6409125328063965\n","step = 3737   loss = 2.6405866146087646\n","step = 3738   loss = 2.640260934829712\n","step = 3739   loss = 2.6399359703063965\n","step = 3740   loss = 2.6396117210388184\n","step = 3741   loss = 2.6392886638641357\n","step = 3742   loss = 2.6389658451080322\n","step = 3743   loss = 2.638643741607666\n","step = 3744   loss = 2.638322353363037\n","step = 3745   loss = 2.6380016803741455\n","step = 3746   loss = 2.637681484222412\n","step = 3747   loss = 2.637362480163574\n","step = 3748   loss = 2.6370434761047363\n","step = 3749   loss = 2.636725425720215\n","step = 3750   loss = 2.6364083290100098\n","step = 3751   loss = 2.636091947555542\n","step = 3752   loss = 2.6357758045196533\n","step = 3753   loss = 2.635460615158081\n","step = 3754   loss = 2.635145902633667\n","step = 3755   loss = 2.6348319053649902\n","step = 3756   loss = 2.6345183849334717\n","step = 3757   loss = 2.6342058181762695\n","step = 3758   loss = 2.6338939666748047\n","step = 3759   loss = 2.633582353591919\n","step = 3760   loss = 2.6332716941833496\n","step = 3761   loss = 2.6329619884490967\n","step = 3762   loss = 2.632652521133423\n","step = 3763   loss = 2.6323440074920654\n","step = 3764   loss = 2.6320364475250244\n","step = 3765   loss = 2.6317288875579834\n","step = 3766   loss = 2.6314220428466797\n","step = 3767   loss = 2.6311163902282715\n","step = 3768   loss = 2.6308112144470215\n","step = 3769   loss = 2.6305060386657715\n","step = 3770   loss = 2.630202054977417\n","step = 3771   loss = 2.629899024963379\n","step = 3772   loss = 2.629595994949341\n","step = 3773   loss = 2.629293918609619\n","step = 3774   loss = 2.6289925575256348\n","step = 3775   loss = 2.6286916732788086\n","step = 3776   loss = 2.6283915042877197\n","step = 3777   loss = 2.628092050552368\n","step = 3778   loss = 2.627793312072754\n","step = 3779   loss = 2.627495050430298\n","step = 3780   loss = 2.627197504043579\n","step = 3781   loss = 2.6269006729125977\n","step = 3782   loss = 2.6266043186187744\n","step = 3783   loss = 2.6263086795806885\n","step = 3784   loss = 2.6260135173797607\n","step = 3785   loss = 2.6257193088531494\n","step = 3786   loss = 2.625425338745117\n","step = 3787   loss = 2.6251320838928223\n","step = 3788   loss = 2.6248397827148438\n","step = 3789   loss = 2.6245484352111816\n","step = 3790   loss = 2.6242568492889404\n","step = 3791   loss = 2.6239664554595947\n","step = 3792   loss = 2.6236767768859863\n","step = 3793   loss = 2.623387098312378\n","step = 3794   loss = 2.623098373413086\n","step = 3795   loss = 2.6228103637695312\n","step = 3796   loss = 2.622523307800293\n","step = 3797   loss = 2.6222362518310547\n","step = 3798   loss = 2.621950149536133\n","step = 3799   loss = 2.621664524078369\n","step = 3800   loss = 2.6213796138763428\n","step = 3801   loss = 2.6210951805114746\n","step = 3802   loss = 2.6208114624023438\n","step = 3803   loss = 2.6205289363861084\n","step = 3804   loss = 2.620245933532715\n","step = 3805   loss = 2.6199638843536377\n","step = 3806   loss = 2.619683027267456\n","step = 3807   loss = 2.6194024085998535\n","step = 3808   loss = 2.6191225051879883\n","step = 3809   loss = 2.6188433170318604\n","step = 3810   loss = 2.6185643672943115\n","step = 3811   loss = 2.6182861328125\n","step = 3812   loss = 2.618009090423584\n","step = 3813   loss = 2.617732286453247\n","step = 3814   loss = 2.6174557209014893\n","step = 3815   loss = 2.6171798706054688\n","step = 3816   loss = 2.6169049739837646\n","step = 3817   loss = 2.6166298389434814\n","step = 3818   loss = 2.616356611251831\n","step = 3819   loss = 2.6160831451416016\n","step = 3820   loss = 2.6158101558685303\n","step = 3821   loss = 2.6155383586883545\n","step = 3822   loss = 2.615267038345337\n","step = 3823   loss = 2.6149959564208984\n","step = 3824   loss = 2.6147255897521973\n","step = 3825   loss = 2.6144561767578125\n","step = 3826   loss = 2.6141867637634277\n","step = 3827   loss = 2.6139185428619385\n","step = 3828   loss = 2.6136505603790283\n","step = 3829   loss = 2.6133832931518555\n","step = 3830   loss = 2.613116502761841\n","step = 3831   loss = 2.6128504276275635\n","step = 3832   loss = 2.6125850677490234\n","step = 3833   loss = 2.6123199462890625\n","step = 3834   loss = 2.612055778503418\n","step = 3835   loss = 2.6117916107177734\n","step = 3836   loss = 2.6115288734436035\n","step = 3837   loss = 2.6112661361694336\n","step = 3838   loss = 2.611004114151001\n","step = 3839   loss = 2.6107428073883057\n","step = 3840   loss = 2.6104819774627686\n","step = 3841   loss = 2.6102216243743896\n","step = 3842   loss = 2.609961748123169\n","step = 3843   loss = 2.6097028255462646\n","step = 3844   loss = 2.6094443798065186\n","step = 3845   loss = 2.6091864109039307\n","step = 3846   loss = 2.608928918838501\n","step = 3847   loss = 2.6086721420288086\n","step = 3848   loss = 2.6084158420562744\n","step = 3849   loss = 2.6081604957580566\n","step = 3850   loss = 2.607905387878418\n","step = 3851   loss = 2.6076505184173584\n","step = 3852   loss = 2.6073968410491943\n","step = 3853   loss = 2.6071436405181885\n","step = 3854   loss = 2.6068906784057617\n","step = 3855   loss = 2.6066384315490723\n","step = 3856   loss = 2.606387138366699\n","step = 3857   loss = 2.606135845184326\n","step = 3858   loss = 2.6058852672576904\n","step = 3859   loss = 2.605635166168213\n","step = 3860   loss = 2.6053857803344727\n","step = 3861   loss = 2.6051371097564697\n","step = 3862   loss = 2.604888916015625\n","step = 3863   loss = 2.6046407222747803\n","step = 3864   loss = 2.60439395904541\n","step = 3865   loss = 2.604147434234619\n","step = 3866   loss = 2.6039013862609863\n","step = 3867   loss = 2.6036558151245117\n","step = 3868   loss = 2.6034109592437744\n","step = 3869   loss = 2.6031668186187744\n","step = 3870   loss = 2.6029226779937744\n","step = 3871   loss = 2.602679491043091\n","step = 3872   loss = 2.6024365425109863\n","step = 3873   loss = 2.6021947860717773\n","step = 3874   loss = 2.6019530296325684\n","step = 3875   loss = 2.6017119884490967\n","step = 3876   loss = 2.6014716625213623\n","step = 3877   loss = 2.601231575012207\n","step = 3878   loss = 2.600992202758789\n","step = 3879   loss = 2.60075306892395\n","step = 3880   loss = 2.600515127182007\n","step = 3881   loss = 2.6002771854400635\n","step = 3882   loss = 2.6000399589538574\n","step = 3883   loss = 2.5998032093048096\n","step = 3884   loss = 2.599567174911499\n","step = 3885   loss = 2.5993316173553467\n","step = 3886   loss = 2.5990960597991943\n","step = 3887   loss = 2.5988616943359375\n","step = 3888   loss = 2.598628282546997\n","step = 3889   loss = 2.5983943939208984\n","step = 3890   loss = 2.5981616973876953\n","step = 3891   loss = 2.597929000854492\n","step = 3892   loss = 2.5976974964141846\n","step = 3893   loss = 2.597465753555298\n","step = 3894   loss = 2.5972352027893066\n","step = 3895   loss = 2.5970048904418945\n","step = 3896   loss = 2.5967752933502197\n","step = 3897   loss = 2.596545934677124\n","step = 3898   loss = 2.5963170528411865\n","step = 3899   loss = 2.5960888862609863\n","step = 3900   loss = 2.5958614349365234\n","step = 3901   loss = 2.5956344604492188\n","step = 3902   loss = 2.5954079627990723\n","step = 3903   loss = 2.595181465148926\n","step = 3904   loss = 2.594956159591675\n","step = 3905   loss = 2.594731092453003\n","step = 3906   loss = 2.5945067405700684\n","step = 3907   loss = 2.594282865524292\n","step = 3908   loss = 2.594059467315674\n","step = 3909   loss = 2.5938363075256348\n","step = 3910   loss = 2.593613862991333\n","step = 3911   loss = 2.5933923721313477\n","step = 3912   loss = 2.5931708812713623\n","step = 3913   loss = 2.592949867248535\n","step = 3914   loss = 2.5927295684814453\n","step = 3915   loss = 2.5925095081329346\n","step = 3916   loss = 2.592289924621582\n","step = 3917   loss = 2.592071533203125\n","step = 3918   loss = 2.5918526649475098\n","step = 3919   loss = 2.5916354656219482\n","step = 3920   loss = 2.5914182662963867\n","step = 3921   loss = 2.591201066970825\n","step = 3922   loss = 2.59098482131958\n","step = 3923   loss = 2.590769052505493\n","step = 3924   loss = 2.5905535221099854\n","step = 3925   loss = 2.590338945388794\n","step = 3926   loss = 2.5901248455047607\n","step = 3927   loss = 2.5899112224578857\n","step = 3928   loss = 2.58969783782959\n","step = 3929   loss = 2.589484453201294\n","step = 3930   loss = 2.5892727375030518\n","step = 3931   loss = 2.5890607833862305\n","step = 3932   loss = 2.5888490676879883\n","step = 3933   loss = 2.5886385440826416\n","step = 3934   loss = 2.5884287357330322\n","step = 3935   loss = 2.5882186889648438\n","step = 3936   loss = 2.5880093574523926\n","step = 3937   loss = 2.5878005027770996\n","step = 3938   loss = 2.587592124938965\n","step = 3939   loss = 2.5873842239379883\n","step = 3940   loss = 2.587177038192749\n","step = 3941   loss = 2.586970329284668\n","step = 3942   loss = 2.586763858795166\n","step = 3943   loss = 2.5865578651428223\n","step = 3944   loss = 2.586352586746216\n","step = 3945   loss = 2.5861477851867676\n","step = 3946   loss = 2.5859436988830566\n","step = 3947   loss = 2.5857393741607666\n","step = 3948   loss = 2.585536241531372\n","step = 3949   loss = 2.5853326320648193\n","step = 3950   loss = 2.5851306915283203\n","step = 3951   loss = 2.584928512573242\n","step = 3952   loss = 2.5847268104553223\n","step = 3953   loss = 2.5845260620117188\n","step = 3954   loss = 2.5843253135681152\n","step = 3955   loss = 2.58412504196167\n","step = 3956   loss = 2.583925485610962\n","step = 3957   loss = 2.583726406097412\n","step = 3958   loss = 2.5835278034210205\n","step = 3959   loss = 2.583329677581787\n","step = 3960   loss = 2.583132028579712\n","step = 3961   loss = 2.582934617996216\n","step = 3962   loss = 2.582737922668457\n","step = 3963   loss = 2.5825414657592773\n","step = 3964   loss = 2.582345962524414\n","step = 3965   loss = 2.5821502208709717\n","step = 3966   loss = 2.581955671310425\n","step = 3967   loss = 2.581761121749878\n","step = 3968   loss = 2.58156681060791\n","step = 3969   loss = 2.581373691558838\n","step = 3970   loss = 2.5811803340911865\n","step = 3971   loss = 2.5809879302978516\n","step = 3972   loss = 2.580796003341675\n","step = 3973   loss = 2.580603837966919\n","step = 3974   loss = 2.5804128646850586\n","step = 3975   loss = 2.5802221298217773\n","step = 3976   loss = 2.5800321102142334\n","step = 3977   loss = 2.5798423290252686\n","step = 3978   loss = 2.579652786254883\n","step = 3979   loss = 2.5794637203216553\n","step = 3980   loss = 2.579275608062744\n","step = 3981   loss = 2.579087495803833\n","step = 3982   loss = 2.57889986038208\n","step = 3983   loss = 2.5787127017974854\n","step = 3984   loss = 2.578526020050049\n","step = 3985   loss = 2.5783398151397705\n","step = 3986   loss = 2.5781543254852295\n","step = 3987   loss = 2.5779688358306885\n","step = 3988   loss = 2.5777840614318848\n","step = 3989   loss = 2.57759952545166\n","step = 3990   loss = 2.5774154663085938\n","step = 3991   loss = 2.5772321224212646\n","step = 3992   loss = 2.5770490169525146\n","step = 3993   loss = 2.576866865158081\n","step = 3994   loss = 2.5766842365264893\n","step = 3995   loss = 2.576502799987793\n","step = 3996   loss = 2.5763211250305176\n","step = 3997   loss = 2.5761404037475586\n","step = 3998   loss = 2.575960159301758\n","step = 3999   loss = 2.575779914855957\n","step = 4000   loss = 2.5756006240844727\n","step = 4001   loss = 2.5754213333129883\n","step = 4002   loss = 2.5752429962158203\n","step = 4003   loss = 2.5750644207000732\n","step = 4004   loss = 2.5748867988586426\n","step = 4005   loss = 2.574709892272949\n","step = 4006   loss = 2.5745327472686768\n","step = 4007   loss = 2.5743563175201416\n","step = 4008   loss = 2.5741803646087646\n","step = 4009   loss = 2.5740041732788086\n","step = 4010   loss = 2.573828935623169\n","step = 4011   loss = 2.5736544132232666\n","step = 4012   loss = 2.5734801292419434\n","step = 4013   loss = 2.5733063220977783\n","step = 4014   loss = 2.5731329917907715\n","step = 4015   loss = 2.5729596614837646\n","step = 4016   loss = 2.572787284851074\n","step = 4017   loss = 2.572615146636963\n","step = 4018   loss = 2.5724432468414307\n","step = 4019   loss = 2.5722718238830566\n","step = 4020   loss = 2.572100877761841\n","step = 4021   loss = 2.571930170059204\n","step = 4022   loss = 2.5717601776123047\n","step = 4023   loss = 2.5715909004211426\n","step = 4024   loss = 2.5714216232299805\n","step = 4025   loss = 2.5712530612945557\n","step = 4026   loss = 2.5710842609405518\n","step = 4027   loss = 2.5709164142608643\n","step = 4028   loss = 2.5707485675811768\n","step = 4029   loss = 2.5705816745758057\n","step = 4030   loss = 2.5704150199890137\n","step = 4031   loss = 2.5702483654022217\n","step = 4032   loss = 2.570082664489746\n","step = 4033   loss = 2.5699172019958496\n","step = 4034   loss = 2.5697522163391113\n","step = 4035   loss = 2.569586992263794\n","step = 4036   loss = 2.569422960281372\n","step = 4037   loss = 2.5692594051361084\n","step = 4038   loss = 2.5690958499908447\n","step = 4039   loss = 2.5689327716827393\n","step = 4040   loss = 2.568770170211792\n","step = 4041   loss = 2.568607807159424\n","step = 4042   loss = 2.568446159362793\n","step = 4043   loss = 2.568284511566162\n","step = 4044   loss = 2.5681235790252686\n","step = 4045   loss = 2.567962884902954\n","step = 4046   loss = 2.567802667617798\n","step = 4047   loss = 2.5676429271698\n","step = 4048   loss = 2.567483425140381\n","step = 4049   loss = 2.567324638366699\n","step = 4050   loss = 2.5671660900115967\n","step = 4051   loss = 2.5670077800750732\n","step = 4052   loss = 2.566849708557129\n","step = 4053   loss = 2.566692590713501\n","step = 4054   loss = 2.566535711288452\n","step = 4055   loss = 2.5663790702819824\n","step = 4056   loss = 2.5662224292755127\n","step = 4057   loss = 2.5660665035247803\n","step = 4058   loss = 2.565911054611206\n","step = 4059   loss = 2.565756320953369\n","step = 4060   loss = 2.5656015872955322\n","step = 4061   loss = 2.5654473304748535\n","step = 4062   loss = 2.565293073654175\n","step = 4063   loss = 2.5651397705078125\n","step = 4064   loss = 2.5649869441986084\n","step = 4065   loss = 2.564833879470825\n","step = 4066   loss = 2.5646817684173584\n","step = 4067   loss = 2.5645296573638916\n","step = 4068   loss = 2.564378023147583\n","step = 4069   loss = 2.5642266273498535\n","step = 4070   loss = 2.5640759468078613\n","step = 4071   loss = 2.5639259815216064\n","step = 4072   loss = 2.5637755393981934\n","step = 4073   loss = 2.5636260509490967\n","step = 4074   loss = 2.563476800918579\n","step = 4075   loss = 2.5633275508880615\n","step = 4076   loss = 2.5631792545318604\n","step = 4077   loss = 2.563030958175659\n","step = 4078   loss = 2.5628836154937744\n","step = 4079   loss = 2.5627357959747314\n","step = 4080   loss = 2.562589168548584\n","step = 4081   loss = 2.5624425411224365\n","step = 4082   loss = 2.5622963905334473\n","step = 4083   loss = 2.562150239944458\n","step = 4084   loss = 2.562004804611206\n","step = 4085   loss = 2.5618598461151123\n","step = 4086   loss = 2.5617151260375977\n","step = 4087   loss = 2.561570405960083\n","step = 4088   loss = 2.5614264011383057\n","step = 4089   loss = 2.5612823963165283\n","step = 4090   loss = 2.5611395835876465\n","step = 4091   loss = 2.5609967708587646\n","step = 4092   loss = 2.560853958129883\n","step = 4093   loss = 2.5607118606567383\n","step = 4094   loss = 2.560570001602173\n","step = 4095   loss = 2.5604283809661865\n","step = 4096   loss = 2.5602874755859375\n","step = 4097   loss = 2.5601468086242676\n","step = 4098   loss = 2.5600063800811768\n","step = 4099   loss = 2.559866428375244\n","step = 4100   loss = 2.5597267150878906\n","step = 4101   loss = 2.5595874786376953\n","step = 4102   loss = 2.559448719024658\n","step = 4103   loss = 2.559309959411621\n","step = 4104   loss = 2.5591719150543213\n","step = 4105   loss = 2.5590338706970215\n","step = 4106   loss = 2.55889630317688\n","step = 4107   loss = 2.5587592124938965\n","step = 4108   loss = 2.5586228370666504\n","step = 4109   loss = 2.558486223220825\n","step = 4110   loss = 2.558350086212158\n","step = 4111   loss = 2.5582141876220703\n","step = 4112   loss = 2.5580790042877197\n","step = 4113   loss = 2.5579440593719482\n","step = 4114   loss = 2.5578091144561768\n","step = 4115   loss = 2.5576751232147217\n","step = 4116   loss = 2.5575411319732666\n","step = 4117   loss = 2.5574076175689697\n","step = 4118   loss = 2.557274341583252\n","step = 4119   loss = 2.5571413040161133\n","step = 4120   loss = 2.5570085048675537\n","step = 4121   loss = 2.5568764209747314\n","step = 4122   loss = 2.5567445755004883\n","step = 4123   loss = 2.556612968444824\n","step = 4124   loss = 2.5564818382263184\n","step = 4125   loss = 2.5563511848449707\n","step = 4126   loss = 2.556220769882202\n","step = 4127   loss = 2.5560905933380127\n","step = 4128   loss = 2.5559606552124023\n","step = 4129   loss = 2.55583119392395\n","step = 4130   loss = 2.5557022094726562\n","step = 4131   loss = 2.5555734634399414\n","step = 4132   loss = 2.5554447174072266\n","step = 4133   loss = 2.555316686630249\n","step = 4134   loss = 2.5551888942718506\n","step = 4135   loss = 2.5550613403320312\n","step = 4136   loss = 2.55493426322937\n","step = 4137   loss = 2.554807662963867\n","step = 4138   loss = 2.5546810626983643\n","step = 4139   loss = 2.5545549392700195\n","step = 4140   loss = 2.554429054260254\n","step = 4141   loss = 2.5543036460876465\n","step = 4142   loss = 2.554178476333618\n","step = 4143   loss = 2.554054021835327\n","step = 4144   loss = 2.553929328918457\n","step = 4145   loss = 2.553805112838745\n","step = 4146   loss = 2.5536813735961914\n","step = 4147   loss = 2.553557872772217\n","step = 4148   loss = 2.5534348487854004\n","step = 4149   loss = 2.553311824798584\n","step = 4150   loss = 2.553189516067505\n","step = 4151   loss = 2.553067207336426\n","step = 4152   loss = 2.552945613861084\n","step = 4153   loss = 2.552824020385742\n","step = 4154   loss = 2.5527029037475586\n","step = 4155   loss = 2.552582025527954\n","step = 4156   loss = 2.552461624145508\n","step = 4157   loss = 2.5523412227630615\n","step = 4158   loss = 2.5522212982177734\n","step = 4159   loss = 2.5521018505096436\n","step = 4160   loss = 2.5519824028015137\n","step = 4161   loss = 2.551863431930542\n","step = 4162   loss = 2.5517449378967285\n","step = 4163   loss = 2.551626682281494\n","step = 4164   loss = 2.5515084266662598\n","step = 4165   loss = 2.5513908863067627\n","step = 4166   loss = 2.5512735843658447\n","step = 4167   loss = 2.551156520843506\n","step = 4168   loss = 2.551039934158325\n","step = 4169   loss = 2.5509233474731445\n","step = 4170   loss = 2.550807237625122\n","step = 4171   loss = 2.5506911277770996\n","step = 4172   loss = 2.5505759716033936\n","step = 4173   loss = 2.5504605770111084\n","step = 4174   loss = 2.5503461360931396\n","step = 4175   loss = 2.5502312183380127\n","step = 4176   loss = 2.550117015838623\n","step = 4177   loss = 2.5500032901763916\n","step = 4178   loss = 2.5498898029327393\n","step = 4179   loss = 2.549776077270508\n","step = 4180   loss = 2.5496630668640137\n","step = 4181   loss = 2.5495505332946777\n","step = 4182   loss = 2.549437999725342\n","step = 4183   loss = 2.549326181411743\n","step = 4184   loss = 2.5492141246795654\n","step = 4185   loss = 2.549102544784546\n","step = 4186   loss = 2.5489916801452637\n","step = 4187   loss = 2.5488805770874023\n","step = 4188   loss = 2.548769950866699\n","step = 4189   loss = 2.548659563064575\n","step = 4190   loss = 2.5485496520996094\n","step = 4191   loss = 2.5484399795532227\n","step = 4192   loss = 2.548330783843994\n","step = 4193   loss = 2.5482215881347656\n","step = 4194   loss = 2.5481128692626953\n","step = 4195   loss = 2.548004388809204\n","step = 4196   loss = 2.547896146774292\n","step = 4197   loss = 2.54778790473938\n","step = 4198   loss = 2.547680616378784\n","step = 4199   loss = 2.5475730895996094\n","step = 4200   loss = 2.547466278076172\n","step = 4201   loss = 2.5473594665527344\n","step = 4202   loss = 2.547253131866455\n","step = 4203   loss = 2.547146797180176\n","step = 4204   loss = 2.5470409393310547\n","step = 4205   loss = 2.5469353199005127\n","step = 4206   loss = 2.546830177307129\n","step = 4207   loss = 2.546725034713745\n","step = 4208   loss = 2.5466203689575195\n","step = 4209   loss = 2.546516180038452\n","step = 4210   loss = 2.5464119911193848\n","step = 4211   loss = 2.5463080406188965\n","step = 4212   loss = 2.5462043285369873\n","step = 4213   loss = 2.5461013317108154\n","step = 4214   loss = 2.5459980964660645\n","step = 4215   loss = 2.5458953380584717\n","step = 4216   loss = 2.545793056488037\n","step = 4217   loss = 2.5456910133361816\n","step = 4218   loss = 2.5455892086029053\n","step = 4219   loss = 2.545487403869629\n","step = 4220   loss = 2.54538631439209\n","step = 4221   loss = 2.545285224914551\n","step = 4222   loss = 2.545184373855591\n","step = 4223   loss = 2.545083999633789\n","step = 4224   loss = 2.5449836254119873\n","step = 4225   loss = 2.544883966445923\n","step = 4226   loss = 2.5447843074798584\n","step = 4227   loss = 2.544685125350952\n","step = 4228   loss = 2.544585943222046\n","step = 4229   loss = 2.5444867610931396\n","step = 4230   loss = 2.5443880558013916\n","step = 4231   loss = 2.544290065765381\n","step = 4232   loss = 2.544192314147949\n","step = 4233   loss = 2.5440948009490967\n","step = 4234   loss = 2.543996810913086\n","step = 4235   loss = 2.5439000129699707\n","step = 4236   loss = 2.5438032150268555\n","step = 4237   loss = 2.5437064170837402\n","step = 4238   loss = 2.543609857559204\n","step = 4239   loss = 2.543513774871826\n","step = 4240   loss = 2.5434181690216064\n","step = 4241   loss = 2.5433225631713867\n","step = 4242   loss = 2.543227434158325\n","step = 4243   loss = 2.5431325435638428\n","step = 4244   loss = 2.5430376529693604\n","step = 4245   loss = 2.5429434776306152\n","step = 4246   loss = 2.542849063873291\n","step = 4247   loss = 2.542754888534546\n","step = 4248   loss = 2.542661428451538\n","step = 4249   loss = 2.5425679683685303\n","step = 4250   loss = 2.5424747467041016\n","step = 4251   loss = 2.542382001876831\n","step = 4252   loss = 2.5422890186309814\n","step = 4253   loss = 2.542196750640869\n","step = 4254   loss = 2.542104721069336\n","step = 4255   loss = 2.5420126914978027\n","step = 4256   loss = 2.541921377182007\n","step = 4257   loss = 2.541830062866211\n","step = 4258   loss = 2.541738986968994\n","step = 4259   loss = 2.5416481494903564\n","step = 4260   loss = 2.541557550430298\n","step = 4261   loss = 2.5414671897888184\n","step = 4262   loss = 2.541377067565918\n","step = 4263   loss = 2.541287422180176\n","step = 4264   loss = 2.5411977767944336\n","step = 4265   loss = 2.5411083698272705\n","step = 4266   loss = 2.5410196781158447\n","step = 4267   loss = 2.54093074798584\n","step = 4268   loss = 2.540842294692993\n","step = 4269   loss = 2.5407540798187256\n","step = 4270   loss = 2.540665626525879\n","step = 4271   loss = 2.5405783653259277\n","step = 4272   loss = 2.5404906272888184\n","step = 4273   loss = 2.540403366088867\n","step = 4274   loss = 2.540316343307495\n","step = 4275   loss = 2.540229558944702\n","step = 4276   loss = 2.5401430130004883\n","step = 4277   loss = 2.5400564670562744\n","step = 4278   loss = 2.539970636367798\n","step = 4279   loss = 2.5398850440979004\n","step = 4280   loss = 2.5397989749908447\n","step = 4281   loss = 2.5397138595581055\n","step = 4282   loss = 2.5396289825439453\n","step = 4283   loss = 2.539544105529785\n","step = 4284   loss = 2.539459705352783\n","step = 4285   loss = 2.539375066757202\n","step = 4286   loss = 2.5392911434173584\n","step = 4287   loss = 2.5392072200775146\n","step = 4288   loss = 2.53912353515625\n","step = 4289   loss = 2.5390405654907227\n","step = 4290   loss = 2.538957118988037\n","step = 4291   loss = 2.5388741493225098\n","step = 4292   loss = 2.5387918949127197\n","step = 4293   loss = 2.5387096405029297\n","step = 4294   loss = 2.5386269092559814\n","step = 4295   loss = 2.5385451316833496\n","step = 4296   loss = 2.538463592529297\n","step = 4297   loss = 2.538381814956665\n","step = 4298   loss = 2.5383007526397705\n","step = 4299   loss = 2.538219690322876\n","step = 4300   loss = 2.5381391048431396\n","step = 4301   loss = 2.5380585193634033\n","step = 4302   loss = 2.537978172302246\n","step = 4303   loss = 2.537898063659668\n","step = 4304   loss = 2.537818193435669\n","step = 4305   loss = 2.537738561630249\n","step = 4306   loss = 2.5376596450805664\n","step = 4307   loss = 2.5375802516937256\n","step = 4308   loss = 2.537501335144043\n","step = 4309   loss = 2.5374228954315186\n","step = 4310   loss = 2.537344455718994\n","step = 4311   loss = 2.5372660160064697\n","step = 4312   loss = 2.5371882915496826\n","step = 4313   loss = 2.5371108055114746\n","step = 4314   loss = 2.5370328426361084\n","step = 4315   loss = 2.5369553565979004\n","step = 4316   loss = 2.5368785858154297\n","step = 4317   loss = 2.53680157661438\n","step = 4318   loss = 2.536724805831909\n","step = 4319   loss = 2.536648750305176\n","step = 4320   loss = 2.536572217941284\n","step = 4321   loss = 2.536496639251709\n","step = 4322   loss = 2.5364208221435547\n","step = 4323   loss = 2.5363452434539795\n","step = 4324   loss = 2.5362699031829834\n","step = 4325   loss = 2.5361950397491455\n","step = 4326   loss = 2.5361199378967285\n","step = 4327   loss = 2.5360453128814697\n","step = 4328   loss = 2.535971164703369\n","step = 4329   loss = 2.5358967781066895\n","step = 4330   loss = 2.535822868347168\n","step = 4331   loss = 2.5357491970062256\n","step = 4332   loss = 2.5356757640838623\n","step = 4333   loss = 2.53560209274292\n","step = 4334   loss = 2.535529136657715\n","step = 4335   loss = 2.5354561805725098\n","step = 4336   loss = 2.535383462905884\n","step = 4337   loss = 2.535310983657837\n","step = 4338   loss = 2.5352389812469482\n","step = 4339   loss = 2.5351667404174805\n","step = 4340   loss = 2.535094976425171\n","step = 4341   loss = 2.5350234508514404\n","step = 4342   loss = 2.53495192527771\n","step = 4343   loss = 2.5348806381225586\n","step = 4344   loss = 2.5348098278045654\n","step = 4345   loss = 2.5347390174865723\n","step = 4346   loss = 2.534668207168579\n","step = 4347   loss = 2.5345981121063232\n","step = 4348   loss = 2.5345277786254883\n","step = 4349   loss = 2.5344579219818115\n","step = 4350   loss = 2.534388542175293\n","step = 4351   loss = 2.5343189239501953\n","step = 4352   loss = 2.5342493057250977\n","step = 4353   loss = 2.5341806411743164\n","step = 4354   loss = 2.534111738204956\n","step = 4355   loss = 2.534043073654175\n","step = 4356   loss = 2.5339746475219727\n","step = 4357   loss = 2.5339062213897705\n","step = 4358   loss = 2.5338380336761475\n","step = 4359   loss = 2.5337705612182617\n","step = 4360   loss = 2.533702850341797\n","step = 4361   loss = 2.533635139465332\n","step = 4362   loss = 2.5335679054260254\n","step = 4363   loss = 2.533500909805298\n","step = 4364   loss = 2.5334339141845703\n","step = 4365   loss = 2.533367395401001\n","step = 4366   loss = 2.5333008766174316\n","step = 4367   loss = 2.5332345962524414\n","step = 4368   loss = 2.5331685543060303\n","step = 4369   loss = 2.5331029891967773\n","step = 4370   loss = 2.5330371856689453\n","step = 4371   loss = 2.5329713821411133\n","step = 4372   loss = 2.5329065322875977\n","step = 4373   loss = 2.532841444015503\n","step = 4374   loss = 2.5327765941619873\n","step = 4375   loss = 2.5327117443084717\n","step = 4376   loss = 2.5326476097106934\n","step = 4377   loss = 2.532583236694336\n","step = 4378   loss = 2.5325188636779785\n","step = 4379   loss = 2.5324552059173584\n","step = 4380   loss = 2.532391309738159\n","step = 4381   loss = 2.5323281288146973\n","step = 4382   loss = 2.532264471054077\n","step = 4383   loss = 2.5322012901306152\n","step = 4384   loss = 2.5321385860443115\n","step = 4385   loss = 2.532075881958008\n","step = 4386   loss = 2.532013177871704\n","step = 4387   loss = 2.5319509506225586\n","step = 4388   loss = 2.531888723373413\n","step = 4389   loss = 2.531826972961426\n","step = 4390   loss = 2.5317649841308594\n","step = 4391   loss = 2.531703472137451\n","step = 4392   loss = 2.531641960144043\n","step = 4393   loss = 2.531580686569214\n","step = 4394   loss = 2.5315194129943848\n","step = 4395   loss = 2.531458854675293\n","step = 4396   loss = 2.531398296356201\n","step = 4397   loss = 2.5313374996185303\n","step = 4398   loss = 2.5312774181365967\n","step = 4399   loss = 2.531217575073242\n","step = 4400   loss = 2.5311574935913086\n","step = 4401   loss = 2.531097650527954\n","step = 4402   loss = 2.531038284301758\n","step = 4403   loss = 2.5309789180755615\n","step = 4404   loss = 2.5309200286865234\n","step = 4405   loss = 2.530860662460327\n","step = 4406   loss = 2.53080153465271\n","step = 4407   loss = 2.53074312210083\n","step = 4408   loss = 2.530684471130371\n","step = 4409   loss = 2.5306265354156494\n","step = 4410   loss = 2.5305683612823486\n","step = 4411   loss = 2.530510187149048\n","step = 4412   loss = 2.5304527282714844\n","step = 4413   loss = 2.530395269393921\n","step = 4414   loss = 2.5303375720977783\n","step = 4415   loss = 2.530280351638794\n","step = 4416   loss = 2.5302233695983887\n","step = 4417   loss = 2.5301668643951416\n","step = 4418   loss = 2.5301101207733154\n","step = 4419   loss = 2.5300536155700684\n","step = 4420   loss = 2.5299971103668213\n","step = 4421   loss = 2.5299408435821533\n","step = 4422   loss = 2.5298850536346436\n","step = 4423   loss = 2.5298290252685547\n","step = 4424   loss = 2.529773473739624\n","step = 4425   loss = 2.5297179222106934\n","step = 4426   loss = 2.529662609100342\n","step = 4427   loss = 2.5296075344085693\n","step = 4428   loss = 2.529552698135376\n","step = 4429   loss = 2.5294981002807617\n","step = 4430   loss = 2.5294432640075684\n","step = 4431   loss = 2.529388666152954\n","step = 4432   loss = 2.529334306716919\n","step = 4433   loss = 2.529280424118042\n","step = 4434   loss = 2.529226779937744\n","step = 4435   loss = 2.529172897338867\n","step = 4436   loss = 2.5291192531585693\n","step = 4437   loss = 2.5290656089782715\n","step = 4438   loss = 2.529012441635132\n","step = 4439   loss = 2.528959274291992\n","step = 4440   loss = 2.5289065837860107\n","step = 4441   loss = 2.5288538932800293\n","step = 4442   loss = 2.528801202774048\n","step = 4443   loss = 2.5287487506866455\n","step = 4444   loss = 2.5286965370178223\n","step = 4445   loss = 2.528644323348999\n","step = 4446   loss = 2.528592348098755\n","step = 4447   loss = 2.528540849685669\n","step = 4448   loss = 2.528488874435425\n","step = 4449   loss = 2.528437614440918\n","step = 4450   loss = 2.528386354446411\n","step = 4451   loss = 2.5283353328704834\n","step = 4452   loss = 2.5282840728759766\n","step = 4453   loss = 2.528233289718628\n","step = 4454   loss = 2.5281827449798584\n","step = 4455   loss = 2.528132438659668\n","step = 4456   loss = 2.5280821323394775\n","step = 4457   loss = 2.528031826019287\n","step = 4458   loss = 2.5279815196990967\n","step = 4459   loss = 2.5279321670532227\n","step = 4460   loss = 2.5278825759887695\n","step = 4461   loss = 2.5278329849243164\n","step = 4462   loss = 2.5277838706970215\n","step = 4463   loss = 2.5277342796325684\n","step = 4464   loss = 2.5276851654052734\n","step = 4465   loss = 2.5276365280151367\n","step = 4466   loss = 2.527587652206421\n","step = 4467   loss = 2.527539014816284\n","step = 4468   loss = 2.5274906158447266\n","step = 4469   loss = 2.527442455291748\n","step = 4470   loss = 2.5273940563201904\n","step = 4471   loss = 2.527346134185791\n","step = 4472   loss = 2.52729868888855\n","step = 4473   loss = 2.5272507667541504\n","step = 4474   loss = 2.527203321456909\n","step = 4475   loss = 2.527155876159668\n","step = 4476   loss = 2.527108907699585\n","step = 4477   loss = 2.527061939239502\n","step = 4478   loss = 2.52701473236084\n","step = 4479   loss = 2.526968002319336\n","step = 4480   loss = 2.526921272277832\n","step = 4481   loss = 2.5268750190734863\n","step = 4482   loss = 2.5268290042877197\n","step = 4483   loss = 2.526782751083374\n","step = 4484   loss = 2.5267364978790283\n","step = 4485   loss = 2.52669095993042\n","step = 4486   loss = 2.5266449451446533\n","step = 4487   loss = 2.526599645614624\n","step = 4488   loss = 2.5265538692474365\n","step = 4489   loss = 2.5265085697174072\n","step = 4490   loss = 2.526463747024536\n","step = 4491   loss = 2.526418447494507\n","step = 4492   loss = 2.526373863220215\n","step = 4493   loss = 2.526329278945923\n","step = 4494   loss = 2.526284694671631\n","step = 4495   loss = 2.526240348815918\n","step = 4496   loss = 2.526196241378784\n","step = 4497   loss = 2.5261518955230713\n","step = 4498   loss = 2.5261075496673584\n","step = 4499   loss = 2.526064157485962\n","step = 4500   loss = 2.5260205268859863\n","step = 4501   loss = 2.5259766578674316\n","step = 4502   loss = 2.525933265686035\n","step = 4503   loss = 2.525890350341797\n","step = 4504   loss = 2.5258474349975586\n","step = 4505   loss = 2.525804281234741\n","step = 4506   loss = 2.525761365890503\n","step = 4507   loss = 2.5257184505462646\n","step = 4508   loss = 2.5256762504577637\n","step = 4509   loss = 2.5256338119506836\n","step = 4510   loss = 2.5255913734436035\n","step = 4511   loss = 2.5255494117736816\n","step = 4512   loss = 2.5255074501037598\n","step = 4513   loss = 2.525465488433838\n","step = 4514   loss = 2.525424003601074\n","step = 4515   loss = 2.5253822803497314\n","step = 4516   loss = 2.5253405570983887\n","step = 4517   loss = 2.525299310684204\n","step = 4518   loss = 2.5252580642700195\n","step = 4519   loss = 2.525217056274414\n","step = 4520   loss = 2.5251762866973877\n","step = 4521   loss = 2.5251357555389404\n","step = 4522   loss = 2.525094747543335\n","step = 4523   loss = 2.5250542163848877\n","step = 4524   loss = 2.5250139236450195\n","step = 4525   loss = 2.5249736309051514\n","step = 4526   loss = 2.5249335765838623\n","step = 4527   loss = 2.5248935222625732\n","step = 4528   loss = 2.5248537063598633\n","step = 4529   loss = 2.5248138904571533\n","step = 4530   loss = 2.5247745513916016\n","step = 4531   loss = 2.5247347354888916\n","step = 4532   loss = 2.524695634841919\n","step = 4533   loss = 2.5246565341949463\n","step = 4534   loss = 2.5246171951293945\n","step = 4535   loss = 2.52457857131958\n","step = 4536   loss = 2.5245397090911865\n","step = 4537   loss = 2.524500846862793\n","step = 4538   loss = 2.5244626998901367\n","step = 4539   loss = 2.5244240760803223\n","step = 4540   loss = 2.524385929107666\n","step = 4541   loss = 2.5243475437164307\n","step = 4542   loss = 2.5243096351623535\n","step = 4543   loss = 2.5242719650268555\n","step = 4544   loss = 2.5242340564727783\n","step = 4545   loss = 2.524196147918701\n","step = 4546   loss = 2.5241587162017822\n","step = 4547   loss = 2.5241212844848633\n","step = 4548   loss = 2.5240840911865234\n","step = 4549   loss = 2.5240471363067627\n","step = 4550   loss = 2.5240097045898438\n","step = 4551   loss = 2.523972988128662\n","step = 4552   loss = 2.5239362716674805\n","step = 4553   loss = 2.523899793624878\n","step = 4554   loss = 2.5238630771636963\n","step = 4555   loss = 2.523826837539673\n","step = 4556   loss = 2.5237903594970703\n","step = 4557   loss = 2.523754119873047\n","step = 4558   loss = 2.5237178802490234\n","step = 4559   loss = 2.523682117462158\n","step = 4560   loss = 2.523646354675293\n","step = 4561   loss = 2.5236103534698486\n","step = 4562   loss = 2.5235750675201416\n","step = 4563   loss = 2.5235395431518555\n","step = 4564   loss = 2.5235042572021484\n","step = 4565   loss = 2.5234689712524414\n","step = 4566   loss = 2.5234339237213135\n","step = 4567   loss = 2.5233991146087646\n","step = 4568   loss = 2.523364305496216\n","step = 4569   loss = 2.523329496383667\n","step = 4570   loss = 2.5232949256896973\n","step = 4571   loss = 2.5232603549957275\n","step = 4572   loss = 2.523226261138916\n","step = 4573   loss = 2.5231919288635254\n","step = 4574   loss = 2.5231575965881348\n","step = 4575   loss = 2.5231237411499023\n","step = 4576   loss = 2.5230894088745117\n","step = 4577   loss = 2.5230557918548584\n","step = 4578   loss = 2.523022174835205\n","step = 4579   loss = 2.5229883193969727\n","step = 4580   loss = 2.5229549407958984\n","step = 4581   loss = 2.5229218006134033\n","step = 4582   loss = 2.522888660430908\n","step = 4583   loss = 2.522855281829834\n","step = 4584   loss = 2.522822380065918\n","step = 4585   loss = 2.522789478302002\n","step = 4586   loss = 2.522756576538086\n","step = 4587   loss = 2.52272367477417\n","step = 4588   loss = 2.5226917266845703\n","step = 4589   loss = 2.5226590633392334\n","step = 4590   loss = 2.5226263999938965\n","step = 4591   loss = 2.522594451904297\n","step = 4592   loss = 2.5225625038146973\n","step = 4593   loss = 2.5225303173065186\n","step = 4594   loss = 2.522498369216919\n","step = 4595   loss = 2.5224666595458984\n","step = 4596   loss = 2.522434949874878\n","step = 4597   loss = 2.5224032402038574\n","step = 4598   loss = 2.522372007369995\n","step = 4599   loss = 2.5223402976989746\n","step = 4600   loss = 2.5223093032836914\n","step = 4601   loss = 2.522278070449829\n","step = 4602   loss = 2.522247552871704\n","step = 4603   loss = 2.5222160816192627\n","step = 4604   loss = 2.5221855640411377\n","step = 4605   loss = 2.5221548080444336\n","step = 4606   loss = 2.5221240520477295\n","step = 4607   loss = 2.5220937728881836\n","step = 4608   loss = 2.5220632553100586\n","step = 4609   loss = 2.5220329761505127\n","step = 4610   loss = 2.522002696990967\n","step = 4611   loss = 2.52197265625\n","step = 4612   loss = 2.5219428539276123\n","step = 4613   loss = 2.5219130516052246\n","step = 4614   loss = 2.5218827724456787\n","step = 4615   loss = 2.521853446960449\n","step = 4616   loss = 2.5218238830566406\n","step = 4617   loss = 2.521794319152832\n","step = 4618   loss = 2.5217647552490234\n","step = 4619   loss = 2.521735906600952\n","step = 4620   loss = 2.5217065811157227\n","step = 4621   loss = 2.5216774940490723\n","step = 4622   loss = 2.521648645401001\n","step = 4623   loss = 2.521620035171509\n","step = 4624   loss = 2.5215911865234375\n","step = 4625   loss = 2.521562337875366\n","step = 4626   loss = 2.521533727645874\n","step = 4627   loss = 2.521505355834961\n","step = 4628   loss = 2.521476984024048\n","step = 4629   loss = 2.521448850631714\n","step = 4630   loss = 2.521420478820801\n","step = 4631   loss = 2.521392583847046\n","step = 4632   loss = 2.521364688873291\n","step = 4633   loss = 2.521336555480957\n","step = 4634   loss = 2.5213091373443604\n","step = 4635   loss = 2.5212812423706055\n","step = 4636   loss = 2.5212535858154297\n","step = 4637   loss = 2.521226406097412\n","step = 4638   loss = 2.5211989879608154\n","step = 4639   loss = 2.521171808242798\n","step = 4640   loss = 2.5211446285247803\n","step = 4641   loss = 2.5211174488067627\n","step = 4642   loss = 2.521090507507324\n","step = 4643   loss = 2.5210635662078857\n","step = 4644   loss = 2.5210368633270264\n","step = 4645   loss = 2.521010160446167\n","step = 4646   loss = 2.5209836959838867\n","step = 4647   loss = 2.5209569931030273\n","step = 4648   loss = 2.520930528640747\n","step = 4649   loss = 2.520904302597046\n","step = 4650   loss = 2.5208780765533447\n","step = 4651   loss = 2.5208520889282227\n","step = 4652   loss = 2.5208256244659424\n","step = 4653   loss = 2.5207998752593994\n","step = 4654   loss = 2.5207741260528564\n","step = 4655   loss = 2.5207483768463135\n","step = 4656   loss = 2.5207223892211914\n","step = 4657   loss = 2.5206971168518066\n","step = 4658   loss = 2.5206716060638428\n","step = 4659   loss = 2.520646333694458\n","step = 4660   loss = 2.520620822906494\n","step = 4661   loss = 2.5205957889556885\n","step = 4662   loss = 2.5205702781677246\n","step = 4663   loss = 2.520545482635498\n","step = 4664   loss = 2.5205202102661133\n","step = 4665   loss = 2.5204954147338867\n","step = 4666   loss = 2.5204708576202393\n","step = 4667   loss = 2.5204460620880127\n","step = 4668   loss = 2.5204217433929443\n","step = 4669   loss = 2.520397186279297\n","step = 4670   loss = 2.5203726291656494\n","step = 4671   loss = 2.52034854888916\n","step = 4672   loss = 2.5203239917755127\n","step = 4673   loss = 2.5202996730804443\n","step = 4674   loss = 2.520275831222534\n","step = 4675   loss = 2.520251512527466\n","step = 4676   loss = 2.5202279090881348\n","step = 4677   loss = 2.5202040672302246\n","step = 4678   loss = 2.5201802253723145\n","step = 4679   loss = 2.5201566219329834\n","step = 4680   loss = 2.5201332569122314\n","step = 4681   loss = 2.5201098918914795\n","step = 4682   loss = 2.5200862884521484\n","step = 4683   loss = 2.5200629234313965\n","step = 4684   loss = 2.5200395584106445\n","step = 4685   loss = 2.5200164318084717\n","step = 4686   loss = 2.519993782043457\n","step = 4687   loss = 2.519970417022705\n","step = 4688   loss = 2.5199477672576904\n","step = 4689   loss = 2.5199248790740967\n","step = 4690   loss = 2.519902229309082\n","step = 4691   loss = 2.5198793411254883\n","step = 4692   loss = 2.519857168197632\n","step = 4693   loss = 2.519834518432617\n","step = 4694   loss = 2.5198121070861816\n","step = 4695   loss = 2.519789934158325\n","step = 4696   loss = 2.5197675228118896\n","step = 4697   loss = 2.519745349884033\n","step = 4698   loss = 2.519723415374756\n","step = 4699   loss = 2.5197014808654785\n","step = 4700   loss = 2.519679307937622\n","step = 4701   loss = 2.519657850265503\n","step = 4702   loss = 2.5196359157562256\n","step = 4703   loss = 2.5196139812469482\n","step = 4704   loss = 2.519592523574829\n","step = 4705   loss = 2.51957106590271\n","step = 4706   loss = 2.519549608230591\n","step = 4707   loss = 2.519528388977051\n","step = 4708   loss = 2.5195071697235107\n","step = 4709   loss = 2.5194857120513916\n","step = 4710   loss = 2.5194647312164307\n","step = 4711   loss = 2.5194437503814697\n","step = 4712   loss = 2.5194225311279297\n","step = 4713   loss = 2.519402265548706\n","step = 4714   loss = 2.519381046295166\n","step = 4715   loss = 2.519360303878784\n","step = 4716   loss = 2.5193395614624023\n","step = 4717   loss = 2.5193188190460205\n","step = 4718   loss = 2.519298791885376\n","step = 4719   loss = 2.519278049468994\n","step = 4720   loss = 2.5192577838897705\n","step = 4721   loss = 2.5192372798919678\n","step = 4722   loss = 2.5192172527313232\n","step = 4723   loss = 2.5191969871520996\n","step = 4724   loss = 2.519177198410034\n","step = 4725   loss = 2.5191571712493896\n","step = 4726   loss = 2.519137144088745\n","step = 4727   loss = 2.5191173553466797\n","step = 4728   loss = 2.5190975666046143\n","step = 4729   loss = 2.519078016281128\n","step = 4730   loss = 2.5190584659576416\n","step = 4731   loss = 2.519038677215576\n","step = 4732   loss = 2.51901912689209\n","step = 4733   loss = 2.5190000534057617\n","step = 4734   loss = 2.5189805030822754\n","step = 4735   loss = 2.5189614295959473\n","step = 4736   loss = 2.51894211769104\n","step = 4737   loss = 2.518922805786133\n","step = 4738   loss = 2.518904209136963\n","step = 4739   loss = 2.5188851356506348\n","step = 4740   loss = 2.5188660621643066\n","step = 4741   loss = 2.5188469886779785\n","step = 4742   loss = 2.5188286304473877\n","step = 4743   loss = 2.5188095569610596\n","step = 4744   loss = 2.518791437149048\n","step = 4745   loss = 2.518772602081299\n","step = 4746   loss = 2.518754005432129\n","step = 4747   loss = 2.518735885620117\n","step = 4748   loss = 2.5187175273895264\n","step = 4749   loss = 2.5186991691589355\n","step = 4750   loss = 2.5186805725097656\n","step = 4751   loss = 2.518662929534912\n","step = 4752   loss = 2.5186448097229004\n","step = 4753   loss = 2.5186266899108887\n","step = 4754   loss = 2.518608808517456\n","step = 4755   loss = 2.5185911655426025\n","step = 4756   loss = 2.518573522567749\n","step = 4757   loss = 2.5185556411743164\n","step = 4758   loss = 2.518537759780884\n","step = 4759   loss = 2.518519878387451\n","step = 4760   loss = 2.518502712249756\n","step = 4761   loss = 2.5184850692749023\n","step = 4762   loss = 2.518467664718628\n","step = 4763   loss = 2.5184507369995117\n","step = 4764   loss = 2.5184333324432373\n","step = 4765   loss = 2.518416166305542\n","step = 4766   loss = 2.5183990001678467\n","step = 4767   loss = 2.5183815956115723\n","step = 4768   loss = 2.518364906311035\n","step = 4769   loss = 2.51834774017334\n","step = 4770   loss = 2.5183308124542236\n","step = 4771   loss = 2.5183141231536865\n","step = 4772   loss = 2.5182971954345703\n","step = 4773   loss = 2.518280267715454\n","step = 4774   loss = 2.518264055252075\n","step = 4775   loss = 2.518247604370117\n","step = 4776   loss = 2.518231153488159\n","step = 4777   loss = 2.518214702606201\n","step = 4778   loss = 2.518198251724243\n","step = 4779   loss = 2.5181820392608643\n","step = 4780   loss = 2.5181655883789062\n","step = 4781   loss = 2.5181496143341064\n","step = 4782   loss = 2.5181334018707275\n","step = 4783   loss = 2.5181171894073486\n","step = 4784   loss = 2.518101453781128\n","step = 4785   loss = 2.518085479736328\n","step = 4786   loss = 2.5180695056915283\n","step = 4787   loss = 2.5180537700653076\n","step = 4788   loss = 2.518038034439087\n","step = 4789   loss = 2.518022298812866\n","step = 4790   loss = 2.5180065631866455\n","step = 4791   loss = 2.517990827560425\n","step = 4792   loss = 2.5179755687713623\n","step = 4793   loss = 2.5179603099823\n","step = 4794   loss = 2.517944812774658\n","step = 4795   loss = 2.5179293155670166\n","step = 4796   loss = 2.517914056777954\n","step = 4797   loss = 2.5178987979888916\n","step = 4798   loss = 2.5178840160369873\n","step = 4799   loss = 2.5178682804107666\n","step = 4800   loss = 2.5178537368774414\n","step = 4801   loss = 2.517838478088379\n","step = 4802   loss = 2.5178236961364746\n","step = 4803   loss = 2.5178089141845703\n","step = 4804   loss = 2.517794132232666\n","step = 4805   loss = 2.5177793502807617\n","step = 4806   loss = 2.5177645683288574\n","step = 4807   loss = 2.517749547958374\n","step = 4808   loss = 2.517735481262207\n","step = 4809   loss = 2.5177206993103027\n","step = 4810   loss = 2.5177063941955566\n","step = 4811   loss = 2.5176918506622314\n","step = 4812   loss = 2.5176775455474854\n","step = 4813   loss = 2.5176632404327393\n","step = 4814   loss = 2.5176491737365723\n","step = 4815   loss = 2.517634630203247\n","step = 4816   loss = 2.517620801925659\n","step = 4817   loss = 2.517606735229492\n","step = 4818   loss = 2.517592430114746\n","step = 4819   loss = 2.517578601837158\n","step = 4820   loss = 2.5175647735595703\n","step = 4821   loss = 2.5175509452819824\n","step = 4822   loss = 2.5175371170043945\n","step = 4823   loss = 2.5175232887268066\n","step = 4824   loss = 2.517509698867798\n","step = 4825   loss = 2.517495632171631\n","step = 4826   loss = 2.5174825191497803\n","step = 4827   loss = 2.5174689292907715\n","step = 4828   loss = 2.517455577850342\n","step = 4829   loss = 2.517441749572754\n","step = 4830   loss = 2.5174286365509033\n","step = 4831   loss = 2.5174150466918945\n","step = 4832   loss = 2.517401933670044\n","step = 4833   loss = 2.5173888206481934\n","step = 4834   loss = 2.5173752307891846\n","step = 4835   loss = 2.517362356185913\n","step = 4836   loss = 2.5173497200012207\n","step = 4837   loss = 2.517336368560791\n","step = 4838   loss = 2.5173234939575195\n","step = 4839   loss = 2.517310619354248\n","step = 4840   loss = 2.5172977447509766\n","step = 4841   loss = 2.517284631729126\n","step = 4842   loss = 2.5172722339630127\n","step = 4843   loss = 2.5172595977783203\n","step = 4844   loss = 2.517246723175049\n","step = 4845   loss = 2.5172338485717773\n","step = 4846   loss = 2.517221450805664\n","step = 4847   loss = 2.517209053039551\n","step = 4848   loss = 2.5171966552734375\n","step = 4849   loss = 2.517184257507324\n","step = 4850   loss = 2.51717209815979\n","step = 4851   loss = 2.5171597003936768\n","step = 4852   loss = 2.5171473026275635\n","step = 4853   loss = 2.51713490486145\n","step = 4854   loss = 2.517122983932495\n","step = 4855   loss = 2.517110824584961\n","step = 4856   loss = 2.5170986652374268\n","step = 4857   loss = 2.5170865058898926\n","step = 4858   loss = 2.5170750617980957\n","step = 4859   loss = 2.5170629024505615\n","step = 4860   loss = 2.5170507431030273\n","step = 4861   loss = 2.5170390605926514\n","step = 4862   loss = 2.5170273780822754\n","step = 4863   loss = 2.5170154571533203\n","step = 4864   loss = 2.5170037746429443\n","step = 4865   loss = 2.5169920921325684\n","step = 4866   loss = 2.5169806480407715\n","step = 4867   loss = 2.5169692039489746\n","step = 4868   loss = 2.5169575214385986\n","step = 4869   loss = 2.5169458389282227\n","step = 4870   loss = 2.516934633255005\n","step = 4871   loss = 2.516923427581787\n","step = 4872   loss = 2.5169119834899902\n","step = 4873   loss = 2.5169005393981934\n","step = 4874   loss = 2.5168893337249756\n","step = 4875   loss = 2.516878604888916\n","step = 4876   loss = 2.516867160797119\n","step = 4877   loss = 2.5168561935424805\n","step = 4878   loss = 2.516845226287842\n","step = 4879   loss = 2.516834020614624\n","step = 4880   loss = 2.5168230533599854\n","step = 4881   loss = 2.5168120861053467\n","step = 4882   loss = 2.516801357269287\n","step = 4883   loss = 2.5167906284332275\n","step = 4884   loss = 2.516779661178589\n","step = 4885   loss = 2.5167689323425293\n","step = 4886   loss = 2.516758441925049\n","step = 4887   loss = 2.51674747467041\n","step = 4888   loss = 2.5167369842529297\n","step = 4889   loss = 2.516726493835449\n","step = 4890   loss = 2.5167160034179688\n","step = 4891   loss = 2.5167055130004883\n","step = 4892   loss = 2.516695022583008\n","step = 4893   loss = 2.5166845321655273\n","step = 4894   loss = 2.516674280166626\n","step = 4895   loss = 2.5166640281677246\n","step = 4896   loss = 2.5166537761688232\n","step = 4897   loss = 2.516643762588501\n","step = 4898   loss = 2.5166332721710205\n","step = 4899   loss = 2.5166232585906982\n","step = 4900   loss = 2.516613006591797\n","step = 4901   loss = 2.5166032314300537\n","step = 4902   loss = 2.5165932178497314\n","step = 4903   loss = 2.51658296585083\n","step = 4904   loss = 2.516573190689087\n","step = 4905   loss = 2.5165631771087646\n","step = 4906   loss = 2.5165534019470215\n","step = 4907   loss = 2.5165436267852783\n","step = 4908   loss = 2.516533613204956\n","step = 4909   loss = 2.516523838043213\n","step = 4910   loss = 2.516514301300049\n","step = 4911   loss = 2.5165047645568848\n","step = 4912   loss = 2.5164949893951416\n","step = 4913   loss = 2.5164856910705566\n","step = 4914   loss = 2.5164759159088135\n","step = 4915   loss = 2.5164666175842285\n","step = 4916   loss = 2.5164570808410645\n","step = 4917   loss = 2.5164475440979004\n","step = 4918   loss = 2.5164380073547363\n","step = 4919   loss = 2.5164289474487305\n","step = 4920   loss = 2.5164194107055664\n","step = 4921   loss = 2.5164103507995605\n","step = 4922   loss = 2.5164010524749756\n","step = 4923   loss = 2.5163919925689697\n","step = 4924   loss = 2.5163826942443848\n","step = 4925   loss = 2.516373634338379\n","step = 4926   loss = 2.516364574432373\n","step = 4927   loss = 2.516355514526367\n","step = 4928   loss = 2.5163462162017822\n","step = 4929   loss = 2.5163376331329346\n","step = 4930   loss = 2.516328811645508\n","step = 4931   loss = 2.516319990158081\n","step = 4932   loss = 2.516310691833496\n","step = 4933   loss = 2.5163021087646484\n","step = 4934   loss = 2.5162932872772217\n","step = 4935   loss = 2.516284704208374\n","step = 4936   loss = 2.5162758827209473\n","step = 4937   loss = 2.516267776489258\n","step = 4938   loss = 2.516258716583252\n","step = 4939   loss = 2.5162501335144043\n","step = 4940   loss = 2.5162415504455566\n","step = 4941   loss = 2.516232967376709\n","step = 4942   loss = 2.5162246227264404\n","step = 4943   loss = 2.5162160396575928\n","step = 4944   loss = 2.516207695007324\n","step = 4945   loss = 2.5161993503570557\n","step = 4946   loss = 2.516191005706787\n","step = 4947   loss = 2.5161824226379395\n","step = 4948   loss = 2.516174077987671\n","step = 4949   loss = 2.5161659717559814\n","step = 4950   loss = 2.516157865524292\n","step = 4951   loss = 2.5161495208740234\n","step = 4952   loss = 2.516141653060913\n","step = 4953   loss = 2.5161333084106445\n","step = 4954   loss = 2.516125440597534\n","step = 4955   loss = 2.5161173343658447\n","step = 4956   loss = 2.5161092281341553\n","step = 4957   loss = 2.516101598739624\n","step = 4958   loss = 2.5160934925079346\n","step = 4959   loss = 2.516085386276245\n","step = 4960   loss = 2.516077995300293\n","step = 4961   loss = 2.5160698890686035\n","step = 4962   loss = 2.516062021255493\n","step = 4963   loss = 2.516054391860962\n","step = 4964   loss = 2.5160465240478516\n","step = 4965   loss = 2.5160388946533203\n","step = 4966   loss = 2.516031265258789\n","step = 4967   loss = 2.516023635864258\n","step = 4968   loss = 2.5160160064697266\n","step = 4969   loss = 2.5160083770751953\n","step = 4970   loss = 2.516000986099243\n","step = 4971   loss = 2.515993595123291\n","step = 4972   loss = 2.5159857273101807\n","step = 4973   loss = 2.5159785747528076\n","step = 4974   loss = 2.5159711837768555\n","step = 4975   loss = 2.5159637928009033\n","step = 4976   loss = 2.5159566402435303\n","step = 4977   loss = 2.515949010848999\n","step = 4978   loss = 2.515941858291626\n","step = 4979   loss = 2.515934467315674\n","step = 4980   loss = 2.515927314758301\n","step = 4981   loss = 2.5159201622009277\n","step = 4982   loss = 2.5159130096435547\n","step = 4983   loss = 2.5159060955047607\n","step = 4984   loss = 2.5158987045288086\n","step = 4985   loss = 2.5158917903900146\n","step = 4986   loss = 2.5158846378326416\n","step = 4987   loss = 2.5158774852752686\n","step = 4988   loss = 2.5158705711364746\n","step = 4989   loss = 2.5158638954162598\n","step = 4990   loss = 2.515856981277466\n","step = 4991   loss = 2.5158498287200928\n","step = 4992   loss = 2.515843152999878\n","step = 4993   loss = 2.515836238861084\n","step = 4994   loss = 2.515829563140869\n","step = 4995   loss = 2.515822649002075\n","step = 4996   loss = 2.5158159732818604\n","step = 4997   loss = 2.5158092975616455\n","step = 4998   loss = 2.5158023834228516\n","step = 4999   loss = 2.515795946121216\n"]}]},{"cell_type":"code","source":["x_new = torch.linspace(-3, 3, 100).reshape(100, 1)\n","with torch.no_grad():\n","  y_new= model(x_new)\n","plt.plot(x_train, y_train, '.')\n","plt.plot(x_new, y_new, '-') #y_new.detach() 넘파이로 출력가능하게 해줌"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"0da8tjWFp8k-","executionInfo":{"status":"ok","timestamp":1691454379821,"user_tz":-540,"elapsed":34,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"56cf4273-c68c-471d-ba41-d9c0ab6a9126"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7ddc7c99c790>]"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["pred = model(x_valid)\n","mse = loss_fn(pred, y_train) # loss_fn= torch.mean((pred - y_train)**2)\n","rmse = mse *0.5\n","mse, rmse"],"metadata":{"id":"4kE_ktVlA7O3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691454379821,"user_tz":-540,"elapsed":12,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"df8ff971-40a5-4bfd-8776-0a2a419bd22d"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(7.6407, grad_fn=<MseLossBackward0>),\n"," tensor(3.8203, grad_fn=<MulBackward0>))"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["relu = torch.nn.ReLU()\n","relu(torch.tensor([-3, 0, 10]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HfTAVUN_On0","executionInfo":{"status":"ok","timestamp":1691454379822,"user_tz":-540,"elapsed":12,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"a9c3a6c3-24fb-41b0-84d1-afdb84e58d9f"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0,  0, 10])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["'''def model(x):\n","  z = lin1(x)\n","  z = relu1(z)\n","  z = lin2(z)\n","  return z'''\n","''' model= torch.nn.Sequential(\n","  torch.nn.Linear(1,5)\n","  torch.nn.ReLU(),\n","  torch.nn.Linear(5,5)\n","  torch.nn.ReLu(),\n","  torch.nn.Linear(5,1)\n",")'''\n","lin1= torch.nn.Linear(1, 5) #(1(입력값),1(출력값)) 한번에 함수를 만듬\n","relu1 =torch.nn.ReLU()\n","lin2 = torch.nn.Linear(5,1)\n","\n","model = torch.nn.Sequential(\n","    lin1, relu1, lin2\n",")\n","opt = torch.optim.Adam(model.parameters()) # opt = torch.optim.Adam(model.parameters())\n","loss_fn = torch.nn.MSELoss() # loss_fn= torch.mean((pred - y_train)**2)\n","pred= model(x_train)\n","loss_fn(pred, y_train)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCCCleOc_Kyt","executionInfo":{"status":"ok","timestamp":1691454379822,"user_tz":-540,"elapsed":11,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"e0b889dd-5910-4ac1-b3a6-04fd08a4b23f"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(15.2728, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["a = np.array([10,20,30,40,50,60])\n","indices = np.random.permutation(6) # 랜덤하게 인덱스값 추출\n","a[indices[:3]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6raT6p0s6u9","executionInfo":{"status":"ok","timestamp":1691457143320,"user_tz":-540,"elapsed":264,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"642a52f2-11af-4549-ed4d-7eb7da6d80ec"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([20, 40, 30])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["train_len = len(x_train)\n","valid_len= len(x_valid)\n","total_sample_count=0\n","for epoch in range(100):\n","  step=1\n","  loss_list = []\n","  indices = torch.randperm(train_len)\n","  for i in range(0, train_len, 32):\n","    sample_i= indices[i:i+32]\n","    x_sample = x_train[sample_i]\n","    y_sample = y_train[sample_i]\n","    # 1. feed-forward\n","    pred = model(x_sample)\n","    # 2. loss\n","    loss = loss_fn(pred, y_sample) # loss_fn= torch.mean((pred - y_train)**2)\n","    # 3. grad\n","    opt.zero_grad()  #a.grad = None #b.grad = None 초기화\n","    loss.backward()\n","    # 4. update\n","    opt.step()\n","    step += 1\n","    loss_list.append(loss.item())\n","    total_sample_count += len(x_sample)\n","    print(f' \\repoch = {epoch}  step = {step}   loss = {np.mean(loss_list)}', end='')\n","  print()\n","\n","  step=1\n","  loss_list = []\n","  indices = torch.randperm(train_len)\n","  for i in range(0, valid_len, 32):\n","    sample_i= indices[i:i+32]\n","    x_sample = x_valid[sample_i]\n","    y_sample = y_valid[sample_i]\n","    # 1. feed-forward\n","    with torch.no_grad():\n","      pred = model(x_sample)\n","    # 2. loss\n","    loss = loss_fn(pred, y_sample) # loss_fn= torch.mean((pred - y_train)**2)\n","\n","    step += 1\n","    loss_list.append(loss.item()  * len(x_sample))\n","    total_sample_count += len(x_sample)\n","    print(f' \\r{\" \"*50}step = {step}   loss = {np.mean(loss_list)}', end='')\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DoPaUzRYAXYM","executionInfo":{"status":"ok","timestamp":1691459518021,"user_tz":-540,"elapsed":1058,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"e2b91eee-8df6-457f-c420-9c8826783ed9"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":[" \repoch = 0  step = 2   loss = 0.7931922078132629 \repoch = 0  step = 3   loss = 0.7889967858791351 \repoch = 0  step = 4   loss = 1.0423901279767354 \repoch = 0  step = 5   loss = 0.9016138091683388\n"," \r                                                  step = 2   loss = 35.28886413574219 \r                                                  step = 3   loss = 39.41861343383789 \r                                                  step = 4   loss = 38.94949849446615 \r                                                  step = 5   loss = 29.533486545085907\n"," \repoch = 1  step = 2   loss = 0.7051043510437012 \repoch = 1  step = 3   loss = 0.9824928045272827 \repoch = 1  step = 4   loss = 0.9903268416722616 \repoch = 1  step = 5   loss = 1.1739535331726074\n"," \r                                                  step = 2   loss = 40.092159271240234 \r                                                  step = 3   loss = 39.58065414428711 \r                                                  step = 4   loss = 36.171339670817055 \r                                                  step = 5   loss = 29.567626953125\n"," \repoch = 2  step = 2   loss = 0.758765459060669 \repoch = 2  step = 3   loss = 1.0956873297691345 \repoch = 2  step = 4   loss = 1.0477019151051838 \repoch = 2  step = 5   loss = 0.8717049360275269\n"," \r                                                  step = 2   loss = 52.98530197143555 \r                                                  step = 3   loss = 44.243141174316406 \r                                                  step = 4   loss = 38.723300298055015 \r                                                  step = 5   loss = 29.579298973083496\n"," \repoch = 3  step = 2   loss = 1.2272611856460571 \repoch = 3  step = 3   loss = 1.1217105388641357 \repoch = 3  step = 4   loss = 1.0454533497492473 \repoch = 3  step = 5   loss = 0.8835552483797073\n"," \r                                                  step = 2   loss = 41.52008056640625 \r                                                  step = 3   loss = 43.0077018737793 \r                                                  step = 4   loss = 37.855713526407875 \r                                                  step = 5   loss = 29.595698475837708\n"," \repoch = 4  step = 2   loss = 1.114851951599121 \repoch = 4  step = 3   loss = 1.0579621195793152 \repoch = 4  step = 4   loss = 1.0189828077952068 \repoch = 4  step = 5   loss = 1.0230973362922668\n"," \r                                                  step = 2   loss = 33.69101333618164 \r                                                  step = 3   loss = 37.25894355773926 \r                                                  step = 4   loss = 37.91744613647461 \r                                                  step = 5   loss = 29.599895358085632\n"," \repoch = 5  step = 2   loss = 0.7944641709327698 \repoch = 5  step = 3   loss = 0.889756590127945 \repoch = 5  step = 4   loss = 1.052055259545644 \repoch = 5  step = 5   loss = 0.8483437523245811\n"," \r                                                  step = 2   loss = 27.268543243408203 \r                                                  step = 3   loss = 31.825664520263672 \r                                                  step = 4   loss = 38.03753153483073 \r                                                  step = 5   loss = 29.625534415245056\n"," \repoch = 6  step = 2   loss = 0.6947497129440308 \repoch = 6  step = 3   loss = 0.9002264738082886 \repoch = 6  step = 4   loss = 1.0378690958023071 \repoch = 6  step = 5   loss = 0.9232739508152008\n"," \r                                                  step = 2   loss = 43.83102798461914 \r                                                  step = 3   loss = 39.57877540588379 \r                                                  step = 4   loss = 39.34662755330404 \r                                                  step = 5   loss = 29.62222658097744\n"," \repoch = 7  step = 2   loss = 1.3160793781280518 \repoch = 7  step = 3   loss = 1.080722451210022 \repoch = 7  step = 4   loss = 1.0411040385564168 \repoch = 7  step = 5   loss = 0.9035761952400208\n"," \r                                                  step = 2   loss = 27.802560806274414 \r                                                  step = 3   loss = 38.000481605529785 \r                                                  step = 4   loss = 37.62297757466634 \r                                                  step = 5   loss = 29.603654146194458\n"," \repoch = 8  step = 2   loss = 1.4973573684692383 \repoch = 8  step = 3   loss = 1.183551162481308 \repoch = 8  step = 4   loss = 0.9875789682070414 \repoch = 8  step = 5   loss = 1.1842585057020187\n"," \r                                                  step = 2   loss = 35.54783248901367 \r                                                  step = 3   loss = 34.80198669433594 \r                                                  step = 4   loss = 39.097357432047524 \r                                                  step = 5   loss = 29.644638538360596\n"," \repoch = 9  step = 2   loss = 0.947711706161499 \repoch = 9  step = 3   loss = 0.9096980690956116 \repoch = 9  step = 4   loss = 0.9915849765141805 \repoch = 9  step = 5   loss = 1.164854258298874\n"," \r                                                  step = 2   loss = 24.804523468017578 \r                                                  step = 3   loss = 32.2327995300293 \r                                                  step = 4   loss = 37.268245697021484 \r                                                  step = 5   loss = 29.705094575881958\n"," \repoch = 10  step = 2   loss = 1.25690495967865 \repoch = 10  step = 3   loss = 1.1411777138710022 \repoch = 10  step = 4   loss = 1.0426273345947266 \repoch = 10  step = 5   loss = 0.8980078175663948\n"," \r                                                  step = 2   loss = 38.0140266418457 \r                                                  step = 3   loss = 36.792457580566406 \r                                                  step = 4   loss = 39.226922353108726 \r                                                  step = 5   loss = 29.728256702423096\n"," \repoch = 11  step = 2   loss = 0.636852502822876 \repoch = 11  step = 3   loss = 0.823025643825531 \repoch = 11  step = 4   loss = 0.9386125802993774 \repoch = 11  step = 5   loss = 1.4439953863620758\n"," \r                                                  step = 2   loss = 45.06050109863281 \r                                                  step = 3   loss = 39.07493209838867 \r                                                  step = 4   loss = 33.15673700968424 \r                                                  step = 5   loss = 29.776476860046387\n"," \repoch = 12  step = 2   loss = 0.8511754274368286 \repoch = 12  step = 3   loss = 0.8677938580513 \repoch = 12  step = 4   loss = 0.9892927010854086 \repoch = 12  step = 5   loss = 1.184209167957306\n"," \r                                                  step = 2   loss = 34.038448333740234 \r                                                  step = 3   loss = 36.04970932006836 \r                                                  step = 4   loss = 37.59651565551758 \r                                                  step = 5   loss = 29.955098867416382\n"," \repoch = 13  step = 2   loss = 1.3005365133285522 \repoch = 13  step = 3   loss = 1.1239793300628662 \repoch = 13  step = 4   loss = 1.0035029848416646 \repoch = 13  step = 5   loss = 1.1155385822057724\n"," \r                                                  step = 2   loss = 34.87736511230469 \r                                                  step = 3   loss = 39.072486877441406 \r                                                  step = 4   loss = 37.841064453125 \r                                                  step = 5   loss = 30.138874053955078\n"," \repoch = 14  step = 2   loss = 0.7826929688453674 \repoch = 14  step = 3   loss = 0.9965794384479523 \repoch = 14  step = 4   loss = 1.0372161666552226 \repoch = 14  step = 5   loss = 0.9408790469169617\n"," \r                                                  step = 2   loss = 32.014625549316406 \r                                                  step = 3   loss = 38.02913856506348 \r                                                  step = 4   loss = 37.46354548136393 \r                                                  step = 5   loss = 30.248712301254272\n"," \repoch = 15  step = 2   loss = 0.8782888650894165 \repoch = 15  step = 3   loss = 1.1035221219062805 \repoch = 15  step = 4   loss = 1.0505868593851726 \repoch = 15  step = 5   loss = 0.8826338276267052\n"," \r                                                  step = 2   loss = 44.32868194580078 \r                                                  step = 3   loss = 45.415977478027344 \r                                                  step = 4   loss = 39.82452710469564 \r                                                  step = 5   loss = 30.334371626377106\n"," \repoch = 16  step = 2   loss = 1.1537110805511475 \repoch = 16  step = 3   loss = 0.99202561378479 \repoch = 16  step = 4   loss = 1.0223918755849202 \repoch = 16  step = 5   loss = 1.0302308797836304\n"," \r                                                  step = 2   loss = 37.49304962158203 \r                                                  step = 3   loss = 39.16153335571289 \r                                                  step = 4   loss = 38.29577128092448 \r                                                  step = 5   loss = 30.312289595603943\n"," \repoch = 17  step = 2   loss = 1.1254935264587402 \repoch = 17  step = 3   loss = 1.101627230644226 \repoch = 17  step = 4   loss = 1.0276364882787068 \repoch = 17  step = 5   loss = 1.0065792798995972\n"," \r                                                  step = 2   loss = 47.6126823425293 \r                                                  step = 3   loss = 38.069902420043945 \r                                                  step = 4   loss = 39.085679372151695 \r                                                  step = 5   loss = 30.197361648082733\n"," \repoch = 18  step = 2   loss = 0.9375502467155457 \repoch = 18  step = 3   loss = 0.9612241089344025 \repoch = 18  step = 4   loss = 0.9760627150535583 \repoch = 18  step = 5   loss = 1.2645666748285294\n"," \r                                                  step = 2   loss = 39.69055938720703 \r                                                  step = 3   loss = 37.95961380004883 \r                                                  step = 4   loss = 39.1430918375651 \r                                                  step = 5   loss = 30.157804250717163\n"," \repoch = 19  step = 2   loss = 1.1186490058898926 \repoch = 19  step = 3   loss = 1.0844688415527344 \repoch = 19  step = 4   loss = 0.9913888374964396 \repoch = 19  step = 5   loss = 1.1816365718841553\n"," \r                                                  step = 2   loss = 37.150978088378906 \r                                                  step = 3   loss = 44.20985794067383 \r                                                  step = 4   loss = 39.31489054361979 \r                                                  step = 5   loss = 30.074023723602295\n"," \repoch = 20  step = 2   loss = 1.1409088373184204 \repoch = 20  step = 3   loss = 0.9197186827659607 \repoch = 20  step = 4   loss = 1.0326581398646038 \repoch = 20  step = 5   loss = 0.9620244354009628\n"," \r                                                  step = 2   loss = 45.013084411621094 \r                                                  step = 3   loss = 41.38857078552246 \r                                                  step = 4   loss = 39.80144500732422 \r                                                  step = 5   loss = 29.931543610990047\n"," \repoch = 21  step = 2   loss = 1.0636593103408813 \repoch = 21  step = 3   loss = 0.9839302003383636 \repoch = 21  step = 4   loss = 1.057940701643626 \repoch = 21  step = 5   loss = 0.8273969069123268\n"," \r                                                  step = 2   loss = 34.698612213134766 \r                                                  step = 3   loss = 38.08428382873535 \r                                                  step = 4   loss = 37.95490264892578 \r                                                  step = 5   loss = 29.7994225025177\n"," \repoch = 22  step = 2   loss = 0.9992610812187195 \repoch = 22  step = 3   loss = 0.9867779016494751 \repoch = 22  step = 4   loss = 0.9813991983731588 \repoch = 22  step = 5   loss = 1.2222530096769333\n"," \r                                                  step = 2   loss = 31.921558380126953 \r                                                  step = 3   loss = 37.54570770263672 \r                                                  step = 4   loss = 38.542198181152344 \r                                                  step = 5   loss = 29.721539795398712\n"," \repoch = 23  step = 2   loss = 0.8564983606338501 \repoch = 23  step = 3   loss = 0.8575167655944824 \repoch = 23  step = 4   loss = 1.0332658688227336 \repoch = 23  step = 5   loss = 0.9489718079566956\n"," \r                                                  step = 2   loss = 46.42350769042969 \r                                                  step = 3   loss = 41.42855453491211 \r                                                  step = 4   loss = 39.39564768473307 \r                                                  step = 5   loss = 29.647470459342003\n"," \repoch = 24  step = 2   loss = 0.984704315662384 \repoch = 24  step = 3   loss = 1.0541339218616486 \repoch = 24  step = 4   loss = 1.0304998358090718 \repoch = 24  step = 5   loss = 0.9606013000011444\n","                                                  step = 5   loss = 29.571420669555664\n","epoch = 25  step = 5   loss = 0.8792520388960838\n","                                                  step = 5   loss = 29.468088150024414\n","epoch = 26  step = 5   loss = 1.088333174586296\n","                                                  step = 5   loss = 29.386325240135193\n","epoch = 27  step = 5   loss = 1.0018257945775986\n","                                                  step = 5   loss = 29.303340047597885\n","epoch = 28  step = 5   loss = 1.147946685552597\n","                                                  step = 5   loss = 29.279166102409363\n","epoch = 29  step = 5   loss = 1.0020041912794113\n","                                                  step = 5   loss = 29.308971524238586\n","epoch = 30  step = 5   loss = 0.949087917804718\n","                                                  step = 5   loss = 29.345466375350952\n","epoch = 31  step = 5   loss = 0.8265052400529385\n","                                                  step = 5   loss = 29.359639406204224\n","epoch = 32  step = 5   loss = 0.9020571112632751\n","                                                  step = 5   loss = 29.36539763212204\n","epoch = 33  step = 5   loss = 1.3261117935180664\n","                                                  step = 5   loss = 29.361574053764343\n","epoch = 34  step = 5   loss = 0.8436252363026142\n","                                                  step = 5   loss = 29.387731790542603\n","epoch = 35  step = 5   loss = 0.9332809299230576\n","                                                  step = 5   loss = 29.39512300491333\n","epoch = 36  step = 5   loss = 0.8300888314843178\n","                                                  step = 5   loss = 29.391874611377716\n","epoch = 37  step = 5   loss = 1.068028599023819\n","                                                  step = 5   loss = 29.38556718826294\n","epoch = 38  step = 5   loss = 0.9346690326929092\n","                                                  step = 5   loss = 29.346182823181152\n","epoch = 39  step = 5   loss = 0.9275207221508026\n","                                                  step = 5   loss = 29.312350898981094\n","epoch = 40  step = 5   loss = 1.0318190157413483\n","                                                  step = 5   loss = 29.272884249687195\n","epoch = 41  step = 5   loss = 1.0983733534812927\n","                                                  step = 5   loss = 29.214821577072144\n","epoch = 42  step = 5   loss = 0.8929886072874069\n","                                                  step = 5   loss = 29.166573524475098\n","epoch = 43  step = 5   loss = 0.9833394438028336\n","                                                  step = 5   loss = 29.16091239452362\n","epoch = 44  step = 5   loss = 1.1672588735818863\n","                                                  step = 5   loss = 29.15857896208763\n","epoch = 45  step = 5   loss = 0.9469321370124817\n","                                                  step = 5   loss = 29.15400457382202\n","epoch = 46  step = 5   loss = 1.1119332611560822\n","                                                  step = 5   loss = 29.175660133361816\n","epoch = 47  step = 5   loss = 1.153549388051033\n","                                                  step = 5   loss = 29.186008483171463\n","epoch = 48  step = 5   loss = 1.0321817696094513\n","                                                  step = 5   loss = 29.18946933746338\n","epoch = 49  step = 5   loss = 0.9265025407075882\n","                                                  step = 5   loss = 29.1664936542511\n","epoch = 50  step = 5   loss = 1.0551836788654327\n","                                                  step = 5   loss = 29.19105899333954\n","epoch = 51  step = 5   loss = 0.8682698011398315\n","                                                  step = 5   loss = 29.221687361598015\n","epoch = 52  step = 5   loss = 0.8502712659537792\n","                                                  step = 5   loss = 29.23577630519867\n","epoch = 53  step = 5   loss = 0.8227439410984516\n","                                                  step = 5   loss = 29.256415963172913\n","epoch = 54  step = 5   loss = 1.0167782008647919\n","                                                  step = 5   loss = 29.27417629957199\n","epoch = 55  step = 5   loss = 1.2526137083768845\n","                                                  step = 5   loss = 29.290172815322876\n","epoch = 56  step = 5   loss = 0.8798413872718811\n","                                                  step = 5   loss = 29.31571936607361\n","epoch = 57  step = 5   loss = 1.2378426045179367\n","                                                  step = 5   loss = 29.303889989852905\n","epoch = 58  step = 5   loss = 0.8176357485353947\n","                                                  step = 5   loss = 29.22712767124176\n","epoch = 59  step = 5   loss = 0.9279027283191681\n","                                                  step = 5   loss = 29.19387423992157\n","epoch = 60  step = 5   loss = 0.969426617026329\n","                                                  step = 5   loss = 29.201387330889702\n","epoch = 61  step = 5   loss = 1.3063743561506271\n","                                                  step = 5   loss = 29.251087963581085\n","epoch = 62  step = 5   loss = 0.8268227130174637\n","                                                  step = 5   loss = 29.32349257171154\n","epoch = 63  step = 5   loss = 1.1232806593179703\n","                                                  step = 5   loss = 29.40109470486641\n","epoch = 64  step = 5   loss = 1.0358771234750748\n","                                                  step = 5   loss = 29.466814637184143\n","epoch = 65  step = 5   loss = 1.1553716659545898\n","                                                  step = 5   loss = 29.50403928756714\n","epoch = 66  step = 5   loss = 1.1752419024705887\n","                                                  step = 5   loss = 29.467461585998535\n","epoch = 67  step = 5   loss = 0.871088445186615\n","                                                  step = 5   loss = 29.41763174533844\n","epoch = 68  step = 5   loss = 0.8311064317822456\n","                                                  step = 5   loss = 29.371895670890808\n","epoch = 69  step = 5   loss = 1.36199489235878\n","                                                  step = 5   loss = 29.329026877880096\n","epoch = 70  step = 5   loss = 0.8541493862867355\n","                                                  step = 5   loss = 29.28239941596985\n","epoch = 71  step = 5   loss = 0.9093054085969925\n","                                                  step = 5   loss = 29.266923621296883\n","epoch = 72  step = 5   loss = 1.0843695402145386\n","                                                  step = 5   loss = 29.273223876953125\n","epoch = 73  step = 5   loss = 0.9809141904115677\n","                                                  step = 5   loss = 29.31537628173828\n","epoch = 74  step = 5   loss = 1.131822183728218\n","                                                  step = 5   loss = 29.310232281684875\n","epoch = 75  step = 5   loss = 1.2476826012134552\n","                                                  step = 5   loss = 29.27849519252777\n","epoch = 76  step = 5   loss = 1.4860632717609406\n","                                                  step = 5   loss = 29.272993624210358\n","epoch = 77  step = 5   loss = 1.3887808173894882\n","                                                  step = 5   loss = 29.30920910835266\n","epoch = 78  step = 5   loss = 1.2766223102807999\n","                                                  step = 5   loss = 29.421517550945282\n","epoch = 79  step = 5   loss = 1.1307746767997742\n","                                                  step = 5   loss = 29.516431033611298\n","epoch = 80  step = 5   loss = 0.8923968970775604\n","                                                  step = 5   loss = 29.56972861289978\n","epoch = 81  step = 5   loss = 0.9842486828565598\n","                                                  step = 5   loss = 29.5784969329834\n","epoch = 82  step = 5   loss = 1.4538771659135818\n","                                                  step = 5   loss = 29.515627026557922\n","epoch = 83  step = 5   loss = 0.9452631175518036\n","                                                  step = 5   loss = 29.45849061012268\n","epoch = 84  step = 5   loss = 0.9750164151191711\n","                                                  step = 5   loss = 29.38071918487549\n","epoch = 85  step = 5   loss = 0.9688681364059448\n","                                                  step = 5   loss = 29.31527429819107\n","epoch = 86  step = 5   loss = 0.8870209380984306\n","                                                  step = 5   loss = 29.32421064376831\n","epoch = 87  step = 5   loss = 1.001622512936592\n","                                                  step = 5   loss = 29.351131439208984\n","epoch = 88  step = 5   loss = 1.485427662730217\n","                                                  step = 5   loss = 29.450761079788208\n","epoch = 89  step = 5   loss = 0.9044339135289192\n","                                                  step = 5   loss = 29.57866257429123\n","epoch = 90  step = 5   loss = 1.2324313819408417\n","                                                  step = 5   loss = 29.63665646314621\n","epoch = 91  step = 5   loss = 1.1243753880262375\n","                                                  step = 5   loss = 29.632191836833954\n","epoch = 92  step = 5   loss = 0.8292561359703541\n","                                                  step = 5   loss = 29.70661985874176\n","epoch = 93  step = 5   loss = 0.9395358115434647\n","                                                  step = 5   loss = 29.742867350578308\n","epoch = 94  step = 5   loss = 1.3090742975473404\n","                                                  step = 5   loss = 29.76748490333557\n","epoch = 95  step = 5   loss = 0.8902056366205215\n","                                                  step = 5   loss = 29.825082540512085\n","epoch = 96  step = 5   loss = 0.8411429151892662\n","                                                  step = 5   loss = 29.84242057800293\n","epoch = 97  step = 5   loss = 1.2378965020179749\n","                                                  step = 5   loss = 29.833181977272034\n","epoch = 98  step = 5   loss = 1.0281886756420135\n","                                                  step = 5   loss = 29.76003360748291\n","epoch = 99  step = 5   loss = 1.0028506517410278\n","                                                  step = 5   loss = 29.69107711315155\n"]}]},{"cell_type":"code","source":["x_new = torch.linspace(-3, 3, 100).reshape(100, 1)\n","with torch.no_grad():\n","  y_new= model(x_new)\n","plt.plot(x_train, y_train, '.')\n","plt.plot(x_new, y_new, '-')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"id":"vugpbsU_BVsm","executionInfo":{"status":"ok","timestamp":1691454464964,"user_tz":-540,"elapsed":327,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"989d24fd-643c-41dd-b4e3-b84397a29d85"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7ddc7c88e4a0>]"]},"metadata":{},"execution_count":24},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["pred = model(x_valid)\n","mse = loss_fn(pred, y_train) # loss_fn= torch.mean((pred - y_train)**2)\n","rmse = mse *0.5\n","mse, rmse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAa4rNh4ttDI","executionInfo":{"status":"ok","timestamp":1691454385426,"user_tz":-540,"elapsed":12,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"24014a47-aac0-4c21-f910-33c9981dd556"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(9.1723, grad_fn=<MseLossBackward0>),\n"," tensor(4.5862, grad_fn=<MulBackward0>))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["##Linear 구체적 설명"],"metadata":{"id":"6_Z0xXra1cnJ"}},{"cell_type":"code","source":["lin = torch.nn.Linear(3,2) # 파라미터와 바이어스를 한꺼번에 만들어줌\n","dummy = torch.randn(5,3)\n"],"metadata":{"id":"msu31nnGttAS","executionInfo":{"status":"ok","timestamp":1691454385427,"user_tz":-540,"elapsed":11,"user":{"displayName":"서동관","userId":"12646868893277677565"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["lin(dummy), lin.bias"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq3ehoeEts93","executionInfo":{"status":"ok","timestamp":1691454385427,"user_tz":-540,"elapsed":10,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"2fcb6d82-2b15-46a6-d02a-d0a6575821e7"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[ 0.1942,  0.0424],\n","         [ 0.2317,  0.3680],\n","         [ 0.3256, -0.4818],\n","         [ 0.6237, -0.0908],\n","         [ 0.5377,  0.2103]], grad_fn=<AddmmBackward0>),\n"," Parameter containing:\n"," tensor([0.4958, 0.2132], requires_grad=True))"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["dummy @ lin.weight.T + lin.bias"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acnraNtCts7F","executionInfo":{"status":"ok","timestamp":1691454385427,"user_tz":-540,"elapsed":9,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"6bd98de4-1835-4bea-bc83-d48c8d5514e1"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.1942,  0.0424],\n","        [ 0.2317,  0.3680],\n","        [ 0.3256, -0.4818],\n","        [ 0.6237, -0.0908],\n","        [ 0.5377,  0.2103]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["a = torch.randn(3,2, requires_grad=True)\n","b = torch.randn(2, requires_grad= True)\n","\n","def my_lin(x):  # model = torch.nn.Linear(3(입력값),2(출력값)) 한번에 함수를 만듬\n","  return x @ a + b\n","\n","my_lin(dummy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzJk3zumts0p","executionInfo":{"status":"ok","timestamp":1691454385427,"user_tz":-540,"elapsed":8,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"c16750cc-c858-4ec2-8c82-88d257339463"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.2224, -2.2772],\n","        [ 2.1293, -2.2272],\n","        [-0.9221, -0.4076],\n","        [ 0.1484, -0.6409],\n","        [-0.7798, -0.3102]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"3_aPKiNr2160","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691455974970,"user_tz":-540,"elapsed":259,"user":{"displayName":"서동관","userId":"12646868893277677565"}},"outputId":"d82b9c7e-3dfc-474d-bbe0-d46568c3ecb3"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([30, 40, 50])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[],"metadata":{"id":"K9dTeBsyoeqf"},"execution_count":null,"outputs":[]}]}